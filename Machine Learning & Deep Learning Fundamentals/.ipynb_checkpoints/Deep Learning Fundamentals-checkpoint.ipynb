{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Fundamentals\n",
    "\n",
    "[Playlist link](https://www.youtube.com/watch?v=OT1jslLoCyA&list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU&index=2)\n",
    "\n",
    "### What is Deep Learning\n",
    "\n",
    "Deep learning is a sub-field of machine learning that uses algorithms inspired by the structure and function of the brain's neural networks.\n",
    "\n",
    "With deep learning, we're still talking about algorithms that learn from data just like we discussed in the last post on machine learning. However, now the algorithms or models that do this learning are based loosely on the structure and function of the brain's neural networks.\n",
    "\n",
    "### Artificial Neural Networks\n",
    "\n",
    "An artificial neural network is a computing system that is comprised of a collection of connected units called neurons that are organized into what we call layers.\n",
    "\n",
    "The connected neural units form the so-called network. Each connection between neurons transmits a signal from one neuron to the other. The receiving neuron processes the signal and signals to downstream neurons connected to it within the network. Note that neurons are also commonly referred to as nodes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The neural networks that we use in deep learning aren't actual biological neural networks though. They simply share some characteristics with biological neural networks and for this reason, we call them artificial neural networks (ANNs).\n",
    "\n",
    "\n",
    "![](http://deeplizard.com/images/neural%20network%203%20layers.png)\n",
    "\n",
    "\n",
    "### ANN - Architecture\n",
    "\n",
    "Nodes are organized into what we call layers. At the highest level, there are three types of layers in every ANN:\n",
    "\n",
    "- Input layer\n",
    "- Hidden layers\n",
    "- Output layer\n",
    "\n",
    "Different layers perform different kinds of transformations on their inputs. Data flows through the network starting at the input layer and moving through the hidden layers until the output layer is reached. This is known as a forward pass through the network. Layers positioned between the input and output layers are known as hidden layers.\n",
    "\n",
    "\n",
    "Let’s consider the number of nodes contained in each type of layer:\n",
    "\n",
    "- Input layer - One node for each component of the input data.\n",
    "- Hidden layers - Arbitrarily chosen number of nodes for each hidden layer.\n",
    "- Output layer - One node for each of the possible desired outputs.\n",
    "\n",
    "![](http://deeplizard.com/images/neural%20network%202%203%202.png)\n",
    "\n",
    "This ANN has three layers total. The layer on the left is the input layer. The layer on the right is the output layer, and the layer in the middle is the hidden layer. Remember that each layer is comprised of neurons or nodes. Here, the nodes are depicted with the circles, so let’s consider how many nodes are in each layer of this network.\n",
    "\n",
    "Number of nodes in each layer:\n",
    "\n",
    "- Input layer (left): 2 nodes\n",
    "- Hidden layer (middle): 3 nodes\n",
    "- Output layer (right): 2 nodes\n",
    "\n",
    "\n",
    "Since this network has two nodes in the input layer, this tells us that each input to this network must have two dimensions, like for example height and weight.\n",
    "\n",
    "Since this network has two nodes in the output layer, this tells us that there are two possible outputs for every input that is passed forward (left to right) through the network. For example, overweight or underweight could be the two output classes. Note that the output classes are also known as the prediction classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Sequential Model\n",
    "\n",
    "In Keras, we can build what is called a sequential model. **Keras defines a sequential model as a sequential stack of linear layers. This is what we might expect as we have just learned that neurons are organized into layers.**\n",
    "\n",
    "This sequential model is Keras’ implementation of an artificial neural network. Let’s see now how a very simple sequential model is built using Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is an instance of a Sequential obj\n",
    "\n",
    "Dense is an obj for layers\n",
    "\n",
    "Dense is just one type of layer and there are many diff types of layers\n",
    "\n",
    "Looking at the arrows in our image (in the above section) coming from the hidden layer to the output layer, we can see that each node in the hidden layer is connected to all nodes in the output layer. This is how we know that the **output layer** in the image is a dense layer. This same logic applies to the hidden layer.\n",
    "\n",
    "\n",
    "\n",
    "Dense is the most basic type of layer and it connects each ip to each op within the layer\n",
    "\n",
    "First param: no of neurons/nodes in the layer\n",
    "\n",
    "The input shape parameter input_shape=(2,) tells us how many neurons our input layer has, so in our case, we have two.\n",
    "\n",
    "activation: activation function is a non-linear function that typically follows a dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(3, input_shape=(2,), activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "]\n",
    "\n",
    "model = Sequential(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers in a NN\n",
    "\n",
    "Few examples of layers in a NN are:\n",
    "\n",
    "- Dense (or fully connected) layers\n",
    "- Convolutional layers\n",
    "- Pooling layers\n",
    "- Recurrent layers\n",
    "- Normalization layers\n",
    "\n",
    "Different layers perform different transformations on their inputs, and some layers are better suited for some tasks than others. For example, a convolutional layer is usually used in models that are doing work with image data. Recurrent layers are used in models that are doing work with time series data, and fully connected layers, as the name suggests, fully connects each input to each output within its layer.\n",
    "\n",
    "Let’s consider the following example ANN:\n",
    "\n",
    "![](http://deeplizard.com/images/deep%20neural%20network%20with%204%20layers.png)\n",
    "\n",
    "We can see that the first layer, the input layer, consists of eight nodes. Each of the eight nodes in this layer represents an individual feature from a given sample in our dataset.\n",
    "\n",
    "This tells us that a single sample from our dataset consists of eight dimensions. When we choose a sample from our dataset and pass this sample to the model, each of the eight values contained in the sample will be provided to a corresponding node in the input layer.\n",
    "\n",
    "We can see that each of the eight input nodes are connected to every node in the next layer.\n",
    "\n",
    "Each connection between the first and second layers transfers the output from the previous node to the input of the receiving node (left to right). The two layers in the middle that have six nodes each are hidden layers simply because they are positioned between the input and output layers.\n",
    "\n",
    "#### Layer weights\n",
    "\n",
    "Each connection between two nodes has an associated weight, which is just a number.\n",
    "\n",
    "Each weight represents the strength of the connection between the two nodes. When the network receives an input at a given node in the input layer, this input is passed to the next node via a connection, and the input will be multiplied by the weight assigned to that connection.\n",
    "\n",
    "For each node in the second layer, a weighted sum is then computed with each of the incoming connections. This sum is then passed to an activation function, which performs some type of transformation on the given sum. For example, an activation function may transform the sum to be a number between zero and one. The actual transformation will vary depending on which activation function is used.\n",
    "\n",
    "`node output = activation(weighted sum of inputs)`\n",
    "\n",
    "#### Forward pass through a neural network\n",
    "\n",
    "\n",
    "Once we obtain the output for a given node, the obtained output is the value that is passed as input to the nodes in the next layer.\n",
    "\n",
    "This process continues until the output layer is reached. The number of nodes in the output layer depends on the number of possible output or prediction classes we have. In our example, we have four possible prediction classes.\n",
    "\n",
    "Suppose our model was tasked with classifying four types of animals. Each node in the output layer would represent one of four possibilities. For example, we could have cat, dog, llama or lizard. The categories or classes depend on how many classes are in our dataset.\n",
    "\n",
    "For a given sample from the dataset, the entire process from input layer to output layer is called a forward pass through the network.\n",
    "\n",
    "#### Finding the optimal weights\n",
    "\n",
    "As the model learns, the weights at all connections are updated and optimized so that the input data point maps to the correct output prediction class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network in code with Keras\n",
    "\n",
    "In our previous discussion, we saw how to use Keras to build a sequential model. Now, let’s do this for our example network.\n",
    "\n",
    "Will start out by defining an array of Dense objects, our layers. This array will then be passed to the constructor of the sequential model.\n",
    "\n",
    "Remember our network looks like this:\n",
    "\n",
    "![](http://deeplizard.com/images/deep%20neural%20network%20with%204%20layers.png)\n",
    "\n",
    "Given this, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # first hidden layer: needs to have input shape specified\n",
    "    Dense(6, input_shape=(8,), activation='relu'),\n",
    "    Dense(6, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "]\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first Dense object specified in the array is not the input layer. The first Dense object is the first hidden layer. The input layer is specified as a parameter to the first Dense object’s constructor.\n",
    "\n",
    "Our input shape is eight. This is why our input shape is specified as input_shape=(8,). Our first hidden layer has six nodes as does our second hidden layer, and our output layer has four nodes.\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "In an artificial neural network, an activation function is a function that maps a node's inputs to its corresponding output.\n",
    "\n",
    "`node output = activation(weighted sum of inputs)`\n",
    "\n",
    "The activation function does some type of operation to transform the sum to a number that is often times between some lower limit and some upper limit. This transformation is often a non-linear transformation. \n",
    "\n",
    "\n",
    "#### Sigmoid activation function\n",
    "\n",
    "Sigmoid takes in an input and does the following:\n",
    "\n",
    "- For negative inputs, sigmoid will transform the input to a number close to zero.\n",
    "- For positive inputs, sigmoid will transform the input into a number close to one.\n",
    "- For inputs close to zero, sigmoid will transform the input into some number between zero and one.\n",
    "\n",
    "![](http://deeplizard.com/images/sigmoid%20function%20graph%20curve.svg)\n",
    "\n",
    "So, for sigmoid, zero is the lower limit, and one is the upper limit.\n",
    "\n",
    "Alright, we now understand mathematically what one of these activation functions does, but what’s the intuition?\n",
    "\n",
    "#### Activation function intuition\n",
    "\n",
    "Well, an activation function is biologically inspired by activity in our brains where different neurons fire (or are activated) by different stimuli.\n",
    "\n",
    "For example, if you smell something pleasant, like freshly baked cookies, certain neurons in your brain will fire and become activated. If you smell something unpleasant, like spoiled milk, this will cause other neurons in your brain to fire.\n",
    "\n",
    "Deep within the folds of our brains, certain neurons are either firing or they’re not. This can be represented by a zero for not firing or a one for firing.\n",
    "\n",
    "With the Sigmoid activation function in an artificial neural network, we have seen that the neuron can be between zero and one, and the closer to one, the more activated that neuron is while the closer to zero the less activated that neuron is.\n",
    "\n",
    "\n",
    "#### Relu activation function\n",
    "\n",
    "Now, it’s not always the case that our activation function is going to do a transformation on an input to be between zero and one.\n",
    "\n",
    "In fact, one of the most widely used activation functions today called ReLU doesn’t do this. ReLU, which is short for rectified linear unit, transforms the input to the maximum of either zero or the input itself.\n",
    "\n",
    "`ReLU(x) = max(0, x)`\n",
    "\n",
    "So if the input is less than or equal to zero, then relu will output zero. If the input is greater than zero, relu will then just output the given input.\n",
    "\n",
    "The idea here is, the more positive the neuron is, the more activated it is. Now, we’ve only talked about two activation functions here, Sigmoid and relu, but there are other types of activation functions that do different types of transformations to their inputs.\n",
    "\n",
    "### Why do we use activation functions?\n",
    "\n",
    "\n",
    "To understand why we use activation functions, we need to first understand linear functions.\n",
    "\n",
    "Suppose that f is a function on a set X. \n",
    "Suppose that a and b are in X. \n",
    "Suppose that x is a real number.\n",
    "\n",
    "The function f is said to be a linear function if and only if:\n",
    "\n",
    "`f(a+b) = f(a) + f(b)` and `f(xa) = xf(a)`\n",
    "\n",
    "An important feature of linear functions is that the composition of two linear functions is also a linear function. This means that, even in very deep neural networks, if we only had linear transformations of our data values during a forward pass, the learned mapping in our network from input to output would also be linear.\n",
    "\n",
    "Typically, the types of mappings that we are aiming to learn with our deep neural networks are more complex than simple linear mappings.\n",
    "\n",
    "This is where activation functions come in. Most activation functions are non-linear, and they are chosen in this way on purpose. Having non-linear activation functions allows our neural networks to compute arbitrarily complex functions.\n",
    "\n",
    "#### Activation functions in code with Keras\n",
    "\n",
    "Let’s take a look at how to specify an activation function in a Keras Sequential model.\n",
    "\n",
    "There are two basic ways to achieve this. First, we’ll import our classes.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(5, input_shape=(3,), activation='relu')\n",
    "])\n",
    "```\n",
    "\n",
    "In this case, we have a Dense layer and we are specifying relu as our activation function activation='relu'.\n",
    "\n",
    "The second way is to add the layers and activation functions to our model after the model has been instantiated like so:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape=(3,)))\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "Remember that:\n",
    "\n",
    "`node output = activation(weighted sum of inputs)`\n",
    "\n",
    "For our example, this means that each output from the nodes in our Dense layer will be equal to the relu result of the weighted sums like\n",
    "\n",
    "`node output = relu(weighted sum of inputs)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
