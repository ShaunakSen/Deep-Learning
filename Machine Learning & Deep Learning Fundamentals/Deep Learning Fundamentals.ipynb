{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Fundamentals\n",
    "\n",
    "[Playlist link](https://www.youtube.com/watch?v=OT1jslLoCyA&list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU&index=2)\n",
    "\n",
    "### What is Deep Learning\n",
    "\n",
    "Deep learning is a sub-field of machine learning that uses algorithms inspired by the structure and function of the brain's neural networks.\n",
    "\n",
    "With deep learning, we're still talking about algorithms that learn from data just like we discussed in the last post on machine learning. However, now the algorithms or models that do this learning are based loosely on the structure and function of the brain's neural networks.\n",
    "\n",
    "### Artificial Neural Networks\n",
    "\n",
    "An artificial neural network is a computing system that is comprised of a collection of connected units called neurons that are organized into what we call layers.\n",
    "\n",
    "The connected neural units form the so-called network. Each connection between neurons transmits a signal from one neuron to the other. The receiving neuron processes the signal and signals to downstream neurons connected to it within the network. Note that neurons are also commonly referred to as nodes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The neural networks that we use in deep learning aren't actual biological neural networks though. They simply share some characteristics with biological neural networks and for this reason, we call them artificial neural networks (ANNs).\n",
    "\n",
    "\n",
    "![](http://deeplizard.com/images/neural%20network%203%20layers.png)\n",
    "\n",
    "\n",
    "### ANN - Architecture\n",
    "\n",
    "Nodes are organized into what we call layers. At the highest level, there are three types of layers in every ANN:\n",
    "\n",
    "- Input layer\n",
    "- Hidden layers\n",
    "- Output layer\n",
    "\n",
    "Different layers perform different kinds of transformations on their inputs. Data flows through the network starting at the input layer and moving through the hidden layers until the output layer is reached. This is known as a forward pass through the network. Layers positioned between the input and output layers are known as hidden layers.\n",
    "\n",
    "\n",
    "Let’s consider the number of nodes contained in each type of layer:\n",
    "\n",
    "- Input layer - One node for each component of the input data.\n",
    "- Hidden layers - Arbitrarily chosen number of nodes for each hidden layer.\n",
    "- Output layer - One node for each of the possible desired outputs.\n",
    "\n",
    "![](http://deeplizard.com/images/neural%20network%202%203%202.png)\n",
    "\n",
    "This ANN has three layers total. The layer on the left is the input layer. The layer on the right is the output layer, and the layer in the middle is the hidden layer. Remember that each layer is comprised of neurons or nodes. Here, the nodes are depicted with the circles, so let’s consider how many nodes are in each layer of this network.\n",
    "\n",
    "Number of nodes in each layer:\n",
    "\n",
    "- Input layer (left): 2 nodes\n",
    "- Hidden layer (middle): 3 nodes\n",
    "- Output layer (right): 2 nodes\n",
    "\n",
    "\n",
    "Since this network has two nodes in the input layer, this tells us that each input to this network must have two dimensions, like for example height and weight.\n",
    "\n",
    "Since this network has two nodes in the output layer, this tells us that there are two possible outputs for every input that is passed forward (left to right) through the network. For example, overweight or underweight could be the two output classes. Note that the output classes are also known as the prediction classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Sequential Model\n",
    "\n",
    "In Keras, we can build what is called a sequential model. **Keras defines a sequential model as a sequential stack of linear layers. This is what we might expect as we have just learned that neurons are organized into layers.**\n",
    "\n",
    "This sequential model is Keras’ implementation of an artificial neural network. Let’s see now how a very simple sequential model is built using Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is an instance of a Sequential obj\n",
    "\n",
    "Dense is an obj for layers\n",
    "\n",
    "Dense is just one type of layer and there are many diff types of layers\n",
    "\n",
    "Looking at the arrows in our image (in the above section) coming from the hidden layer to the output layer, we can see that each node in the hidden layer is connected to all nodes in the output layer. This is how we know that the **output layer** in the image is a dense layer. This same logic applies to the hidden layer.\n",
    "\n",
    "\n",
    "\n",
    "Dense is the most basic type of layer and it connects each ip to each op within the layer\n",
    "\n",
    "First param: no of neurons/nodes in the layer\n",
    "\n",
    "The input shape parameter input_shape=(2,) tells us how many neurons our input layer has, so in our case, we have two.\n",
    "\n",
    "activation: activation function is a non-linear function that typically follows a dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(3, input_shape=(2,), activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "]\n",
    "\n",
    "model = Sequential(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers in a NN\n",
    "\n",
    "Few examples of layers in a NN are:\n",
    "\n",
    "- Dense (or fully connected) layers\n",
    "- Convolutional layers\n",
    "- Pooling layers\n",
    "- Recurrent layers\n",
    "- Normalization layers\n",
    "\n",
    "Different layers perform different transformations on their inputs, and some layers are better suited for some tasks than others. For example, a convolutional layer is usually used in models that are doing work with image data. Recurrent layers are used in models that are doing work with time series data, and fully connected layers, as the name suggests, fully connects each input to each output within its layer.\n",
    "\n",
    "Let’s consider the following example ANN:\n",
    "\n",
    "![](http://deeplizard.com/images/deep%20neural%20network%20with%204%20layers.png)\n",
    "\n",
    "We can see that the first layer, the input layer, consists of eight nodes. Each of the eight nodes in this layer represents an individual feature from a given sample in our dataset.\n",
    "\n",
    "This tells us that a single sample from our dataset consists of eight dimensions. When we choose a sample from our dataset and pass this sample to the model, each of the eight values contained in the sample will be provided to a corresponding node in the input layer.\n",
    "\n",
    "We can see that each of the eight input nodes are connected to every node in the next layer.\n",
    "\n",
    "Each connection between the first and second layers transfers the output from the previous node to the input of the receiving node (left to right). The two layers in the middle that have six nodes each are hidden layers simply because they are positioned between the input and output layers.\n",
    "\n",
    "#### Layer weights\n",
    "\n",
    "Each connection between two nodes has an associated weight, which is just a number.\n",
    "\n",
    "Each weight represents the strength of the connection between the two nodes. When the network receives an input at a given node in the input layer, this input is passed to the next node via a connection, and the input will be multiplied by the weight assigned to that connection.\n",
    "\n",
    "For each node in the second layer, a weighted sum is then computed with each of the incoming connections. This sum is then passed to an activation function, which performs some type of transformation on the given sum. For example, an activation function may transform the sum to be a number between zero and one. The actual transformation will vary depending on which activation function is used.\n",
    "\n",
    "`node output = activation(weighted sum of inputs)`\n",
    "\n",
    "#### Forward pass through a neural network\n",
    "\n",
    "\n",
    "Once we obtain the output for a given node, the obtained output is the value that is passed as input to the nodes in the next layer.\n",
    "\n",
    "This process continues until the output layer is reached. The number of nodes in the output layer depends on the number of possible output or prediction classes we have. In our example, we have four possible prediction classes.\n",
    "\n",
    "Suppose our model was tasked with classifying four types of animals. Each node in the output layer would represent one of four possibilities. For example, we could have cat, dog, llama or lizard. The categories or classes depend on how many classes are in our dataset.\n",
    "\n",
    "For a given sample from the dataset, the entire process from input layer to output layer is called a forward pass through the network.\n",
    "\n",
    "#### Finding the optimal weights\n",
    "\n",
    "As the model learns, the weights at all connections are updated and optimized so that the input data point maps to the correct output prediction class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network in code with Keras\n",
    "\n",
    "In our previous discussion, we saw how to use Keras to build a sequential model. Now, let’s do this for our example network.\n",
    "\n",
    "Will start out by defining an array of Dense objects, our layers. This array will then be passed to the constructor of the sequential model.\n",
    "\n",
    "Remember our network looks like this:\n",
    "\n",
    "![](http://deeplizard.com/images/deep%20neural%20network%20with%204%20layers.png)\n",
    "\n",
    "Given this, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    # first hidden layer: needs to have input shape specified\n",
    "    Dense(6, input_shape=(8,), activation='relu'),\n",
    "    Dense(6, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "]\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first Dense object specified in the array is not the input layer. The first Dense object is the first hidden layer. The input layer is specified as a parameter to the first Dense object’s constructor.\n",
    "\n",
    "Our input shape is eight. This is why our input shape is specified as input_shape=(8,). Our first hidden layer has six nodes as does our second hidden layer, and our output layer has four nodes.\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "In an artificial neural network, an activation function is a function that maps a node's inputs to its corresponding output.\n",
    "\n",
    "`node output = activation(weighted sum of inputs)`\n",
    "\n",
    "The activation function does some type of operation to transform the sum to a number that is often times between some lower limit and some upper limit. This transformation is often a non-linear transformation. \n",
    "\n",
    "\n",
    "#### Sigmoid activation function\n",
    "\n",
    "Sigmoid takes in an input and does the following:\n",
    "\n",
    "- For negative inputs, sigmoid will transform the input to a number close to zero.\n",
    "- For positive inputs, sigmoid will transform the input into a number close to one.\n",
    "- For inputs close to zero, sigmoid will transform the input into some number between zero and one.\n",
    "\n",
    "![](http://deeplizard.com/images/sigmoid%20function%20graph%20curve.svg)\n",
    "\n",
    "So, for sigmoid, zero is the lower limit, and one is the upper limit.\n",
    "\n",
    "Alright, we now understand mathematically what one of these activation functions does, but what’s the intuition?\n",
    "\n",
    "#### Activation function intuition\n",
    "\n",
    "Well, an activation function is biologically inspired by activity in our brains where different neurons fire (or are activated) by different stimuli.\n",
    "\n",
    "For example, if you smell something pleasant, like freshly baked cookies, certain neurons in your brain will fire and become activated. If you smell something unpleasant, like spoiled milk, this will cause other neurons in your brain to fire.\n",
    "\n",
    "Deep within the folds of our brains, certain neurons are either firing or they’re not. This can be represented by a zero for not firing or a one for firing.\n",
    "\n",
    "With the Sigmoid activation function in an artificial neural network, we have seen that the neuron can be between zero and one, and the closer to one, the more activated that neuron is while the closer to zero the less activated that neuron is.\n",
    "\n",
    "\n",
    "#### Relu activation function\n",
    "\n",
    "Now, it’s not always the case that our activation function is going to do a transformation on an input to be between zero and one.\n",
    "\n",
    "In fact, one of the most widely used activation functions today called ReLU doesn’t do this. ReLU, which is short for rectified linear unit, transforms the input to the maximum of either zero or the input itself.\n",
    "\n",
    "`ReLU(x) = max(0, x)`\n",
    "\n",
    "So if the input is less than or equal to zero, then relu will output zero. If the input is greater than zero, relu will then just output the given input.\n",
    "\n",
    "The idea here is, the more positive the neuron is, the more activated it is. Now, we’ve only talked about two activation functions here, Sigmoid and relu, but there are other types of activation functions that do different types of transformations to their inputs.\n",
    "\n",
    "### Why do we use activation functions?\n",
    "\n",
    "\n",
    "To understand why we use activation functions, we need to first understand linear functions.\n",
    "\n",
    "Suppose that f is a function on a set X. \n",
    "Suppose that a and b are in X. \n",
    "Suppose that x is a real number.\n",
    "\n",
    "The function f is said to be a linear function if and only if:\n",
    "\n",
    "`f(a+b) = f(a) + f(b)` and `f(xa) = xf(a)`\n",
    "\n",
    "An important feature of linear functions is that the composition of two linear functions is also a linear function. This means that, even in very deep neural networks, if we only had linear transformations of our data values during a forward pass, the learned mapping in our network from input to output would also be linear.\n",
    "\n",
    "Typically, the types of mappings that we are aiming to learn with our deep neural networks are more complex than simple linear mappings.\n",
    "\n",
    "This is where activation functions come in. Most activation functions are non-linear, and they are chosen in this way on purpose. Having non-linear activation functions allows our neural networks to compute arbitrarily complex functions.\n",
    "\n",
    "#### Activation functions in code with Keras\n",
    "\n",
    "Let’s take a look at how to specify an activation function in a Keras Sequential model.\n",
    "\n",
    "There are two basic ways to achieve this. First, we’ll import our classes.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(5, input_shape=(3,), activation='relu')\n",
    "])\n",
    "```\n",
    "\n",
    "In this case, we have a Dense layer and we are specifying relu as our activation function activation='relu'.\n",
    "\n",
    "The second way is to add the layers and activation functions to our model after the model has been instantiated like so:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape=(3,)))\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "Remember that:\n",
    "\n",
    "`node output = activation(weighted sum of inputs)`\n",
    "\n",
    "For our example, this means that each output from the nodes in our Dense layer will be equal to the relu result of the weighted sums like\n",
    "\n",
    "`node output = relu(weighted sum of inputs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an ANN\n",
    "\n",
    "When we train a model, we’re basically trying to solve an optimization problem. We’re trying to optimize the weights within the model. Our task is to find the weights that most accurately map our input data to the correct output class. This mapping is what the network must learn.\n",
    "\n",
    "#### Optimization algorithm\n",
    "\n",
    "The weights are optimized using what we call an optimization algorithm. The optimization process depends on the chosen optimization algorithm. We also use the term optimizer to refer to the chosen algorithm. The most widely known optimizer is called stochastic gradient descent, or more simply, SGD.\n",
    "\n",
    "When we have any optimization problem, we must have an optimization objective, so now let’s consider what SGD’s objective is in optimizing the model’s weights.\n",
    "\n",
    "The objective of SGD is to minimize some given function that we call a loss function. So, SGD updates the model's weights in such a way as to make this loss function as close to its minimum value as possible.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "One common loss function is mean squared error (MSE), but there are several loss functions that we could use in its place. As deep learning practitioners, it's our job to decide which loss function to use.\n",
    "\n",
    "Alright, but what is the actual loss we’re talking about? Well, during training, we supply our model with data and the corresponding labels to that data.\n",
    "\n",
    "For example, suppose we have a model that we want to train to classify whether images are either images of cats or images of dogs. We will supply our model with images of cats and dogs along with the labels for these images that state whether each image is of a cat or of a dog.\n",
    "\n",
    "Suppose we give one image of a cat to our model. Once the forward pass is complete and the cat image data has flowed through the network, the model is going to provide an output at the end. This will consist of what the model thinks the image is, either a cat or a dog.\n",
    "\n",
    "In a literal sense, the output will consist of probabilities for cat or dog. For example, it may assign a 75% probability to the image being a cat, and a 25% probability to it being a dog. In this case, the model is assigning a higher likelihood to the image being of a cat than of a dog.\n",
    "\n",
    "- 75% chance it's a cat\n",
    "- 25% chance it's a dog\n",
    "\n",
    "If we stop and think about it for a moment, this is very similar to how humans make decisions. Everything is a prediction!\n",
    "\n",
    "The loss is the error or difference between what the network is predicting for the image versus the true label of the image, and SGD will to try to minimize this error to make our model as accurate as possible in its predictions.\n",
    "\n",
    "After passing all of our data through our model, we’re going to continue passing the same data over and over again. This process of repeatedly sending the same data through the network is considered training. During this training process is when the model will actually learn. More about learning in the next post. So, through this process that’s occurring with SGD iteratively, the model is able to learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in artificial neural networks - More details\n",
    "\n",
    "In a previous post, we learned about the training process and saw that each data point used for training is passed through the network. This pass through the network from input to output is called a forward pass, and the resulting output depends on the weights at each connection inside the network.\n",
    "\n",
    "Once all of the data points in our dataset have been passed through the network, we say that an epoch is complete.\n",
    "\n",
    "**An epoch refers to a single pass of the entire dataset to the network during training.**\n",
    "\n",
    "Note that many epochs occur throughout the training process as the model learns.\n",
    "\n",
    "#### What does it mean to learn?\n",
    "\n",
    "Well, remember, when the model is initialized, the network weights are set to arbitrary values. We have also seen that, at the end of the network, the model will provide the output for a given input.\n",
    "\n",
    "Once the output is obtained, the loss (or the error) can be computed for that specific output by looking at what the model predicted versus the true label.\n",
    "\n",
    "After the loss is calculated, the gradient of this loss function is computed with respect to each of the weights within the network. Note, gradient is just a word for the derivative of a function of several variables.\n",
    "\n",
    "Continuing with this explanation, let’s focus in on only one of the weights in the model.\n",
    "\n",
    "At this point, we’ve calculated the loss of a single output, and we calculate the gradient of that loss with respect to our single chosen weight. This calculation is done using a technique called backpropagation\n",
    "\n",
    "Once we have the value for the gradient of the loss function, we can use this value to update the model’s weight. The gradient tells us which direction will move the loss towards the minimum, and our task is to move in a direction that lowers the loss and steps closer to this minimum value.\n",
    "\n",
    "We then multiply the gradient value by something called a learning rate. A learning rate is a small number usually ranging between 0.01 and 0.0001, but the actual value can vary.\n",
    "\n",
    "**The learning rate tells us how large of a step we should take in the direction of the minimum.**\n",
    "\n",
    "Alright, so we multiply the gradient with the learning rate, and we subtract this product from the weight, which will give us the new updated value for this weight.\n",
    "\n",
    "`new weight = old weight - (learning rate * gradient)`\n",
    "\n",
    "In this discussion, we just focused on one single weight to explain the concept, but this same process is going to happen with each of the weights in the model each time data passes through it.\n",
    "\n",
    "The only difference is that when the gradient of the loss function is computed, the value for the gradient is going to be different for each weight because the gradient is being calculated with respect to each weight.\n",
    "\n",
    "So now imagine all these weights being iteratively updated with each epoch. The weights are going to be incrementally getting closer and closer to their optimized values while SGD works to minimize the loss function.\n",
    "\n",
    "This updating of the weights is essentially what we mean when we say that the model is learning. It’s learning what values to assign to each weight based on how those incremental changes are affecting the loss function. As the weights change, the network is getting smarter in terms of accurately mapping inputs to the correct output.\n",
    "\n",
    "After each epoch basically the loss should decrease and the accuracy should increase\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data to be trained using our NN\n",
    "\n",
    "[link](https://www.youtube.com/watch?v=UkzhouEk6uY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For keras the samples need to be in form of a np array or a list of np arrays\n",
    "The labels need to be in form of a np array\n",
    "\n",
    "We will generate some numeric data and do some preprocessing on it st keras can understand the data and train our \n",
    "NN on it\n",
    "\n",
    "Example data:\n",
    "\n",
    "- An experimental drug was tested on idvs from ages 13 - 100\n",
    "- The trial had 2100 participants. Half were under 65 and half over 65\n",
    "- 95% of patients 65 or older experienced side effects\n",
    "- 95% of patients under 65 experienced no side effects\n",
    "\n",
    "We want our NN to predict if an indv will have side effects or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    random_younger = randint(13, 64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65, 100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "for i in range(50):\n",
    "    random_younger = randint(13, 64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    random_older = randint(65, 100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples) == len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our raw data in the formalt keras wants\n",
    "\n",
    "The NN might not learn v well from nos ranging from 13 - 100\n",
    "\n",
    "So we scale our data in range 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31],\n",
       "       [77],\n",
       "       [49],\n",
       "       ...,\n",
       "       [91],\n",
       "       [26],\n",
       "       [82]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.reshape(2100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaunak/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scalar = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_train_samples = scalar.fit_transform(train_samples.reshape(len(train_samples), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20689655],\n",
       "       [0.73563218],\n",
       "       [0.4137931 ],\n",
       "       ...,\n",
       "       [0.89655172],\n",
       "       [0.14942529],\n",
       "       [0.79310345]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is perfect for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training in code with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, input_shape = (1,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our model, we must compile it like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the compile() function, we are passing the optimizer, the loss function, and the metrics that we would like to see. Notice that the optimizer we have specified is called Adam. Adam is just a variant of SGD. Inside the Adam constructor is where we specify the learning rate, and in this case Adam(lr=.0001), we have chosen 0.0001.\n",
    "\n",
    "Finally, we fit our model to the data. Fitting the model to the data means to train the model on the data. We do this with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "Epoch 1/20 0s - loss: 0.6400 - acc: 0.5576\n",
    "Epoch 2/20 0s - loss: 0.6061 - acc: 0.6310\n",
    "Epoch 3/20 0s - loss: 0.5748 - acc: 0.7010\n",
    "Epoch 4/20 0s - loss: 0.5401 - acc: 0.7633\n",
    "Epoch 5/20 0s - loss: 0.5050 - acc: 0.7990\n",
    "Epoch 6/20 0s - loss: 0.4702 - acc: 0.8300\n",
    "Epoch 7/20 0s - loss: 0.4366 - acc: 0.8495\n",
    "Epoch 8/20 0s - loss: 0.4066 - acc: 0.8767\n",
    "Epoch 9/20 0s - loss: 0.3808 - acc: 0.8814\n",
    "Epoch 10/20 0s - loss: 0.3596 - acc: 0.8962\n",
    "Epoch 11/20 0s - loss: 0.3420 - acc: 0.9043\n",
    "Epoch 12/20 0s - loss: 0.3282 - acc: 0.9090\n",
    "Epoch 13/20 0s - loss: 0.3170 - acc: 0.9129\n",
    "Epoch 14/20 0s - loss: 0.3081 - acc: 0.9210\n",
    "Epoch 15/20 0s - loss: 0.3014 - acc: 0.9190\n",
    "Epoch 16/20 0s - loss: 0.2959 - acc: 0.9205\n",
    "Epoch 17/20 0s - loss: 0.2916 - acc: 0.9238\n",
    "Epoch 18/20 0s - loss: 0.2879 - acc: 0.9267\n",
    "Epoch 19/20 0s - loss: 0.2848 - acc: 0.9252\n",
    "Epoch 20/20 0s - loss: 0.2824 - acc: 0.9286\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_train_samples is a numpy array consisting of the training samples.\n",
    "\n",
    "train_labels is a numpy array consisting of the corresponding labels for the training samples.\n",
    "\n",
    "batch_size=10 specifies how many training samples should be sent to the model at once.\n",
    "\n",
    "epochs=20 means that the complete training set (all of the samples) will be passed to the model a total of 20 times.\n",
    "\n",
    "shuffle=True indicates that the data should first be shuffled before being passed to the model.\n",
    "\n",
    "verbose=2 indicates how much logging we will see as the model trains.\n",
    "\n",
    "The output gives us the following values for each epoch:\n",
    "\n",
    "- Epoch number\n",
    "- Duration in seconds\n",
    "- Loss\n",
    "- Accuracy\n",
    "\n",
    "\n",
    "What you will notice is that the loss is going down and the accuracy is going up as the epochs progress.\n",
    "\n",
    "\n",
    "### Loss functions in neural networks\n",
    "\n",
    "The loss function is what SGD is attempting to minimize by iteratively updating the weights in the network.\n",
    "\n",
    "At the end of each epoch during the training process, the loss will be calculated using the network’s output predictions and the true labels for the respective input.\n",
    "\n",
    "Suppose our model is classifying images of cats and dogs, and assume that the label for cat is 0 and the label for dog is 1.\n",
    "\n",
    "- cat: 0\n",
    "- dog: 1\n",
    "\n",
    "Now suppose we pass an image of a cat to the model, and the provided output is 0.25. In this case, the difference between the model’s prediction and the true label is 0.25 - 0.00 = 0.25. This difference is also called the error.\n",
    "\n",
    "`error = 0.25 - 0.00 = 0.25`\n",
    "\n",
    "This process is performed for every output. For each epoch, the error is accumulated across all the individual outputs.\n",
    "\n",
    "Let’s look at a loss function that is commonly used in practice called the mean squared error (MSE).\n",
    "\n",
    "#### MSE\n",
    "\n",
    "For a single sample, with MSE, we first calculate the difference (the error) between the provided output prediction and the label. We then square this error. For a single input, this is all we do.\n",
    "\n",
    "`MSE(input) = (output - label)^2`\n",
    "\n",
    "If we passed multiple samples to the model at once (a batch of samples), then we would take the mean of the squared errors over all of these samples.\n",
    "\n",
    "This was just illustrating the math behind how one loss function, MSE, works. There are several different loss functions that we could work with though.\n",
    "\n",
    "The general idea that we just showed for calculating the error of individual samples will hold true for all of the different types of loss functions. The implementation of what we actually do with each of the errors will be dependent upon the algorithm of the given loss function we’re using. For example, we averaged the squared errors to calculate MSE, but other loss functions will use other algorithms to determine the value of the loss.\n",
    "\n",
    "If we passed our entire training set to the model at once (batch_size=1), then the process we just went over for calculating the loss will occur at the end of each epoch during training.\n",
    "\n",
    "If we split our training set into batches, and passed batches one at a time to our model, then the loss would be calculated on each batch. With either method, since the loss depends on the weights, we expect to see the value of the loss change each time the weights are updated. Given that the objective of SGD is to minimize the loss, we want to see our loss decrease as we run more epochs.\n",
    "\n",
    "The currently available loss functions for Keras are as follows:\n",
    "\n",
    "- mean_squared_error\n",
    "- mean_absolute_error\n",
    "- mean_absolute_percentage_error\n",
    "- mean_squared_logarithmic_error\n",
    "- squared_hinge\n",
    "- hinge\n",
    "- categorical_hinge\n",
    "- logcosh\n",
    "- categorical_crossentropy\n",
    "- sparse_categorical_crossentropy\n",
    "- binary_crossentropy\n",
    "- kullback_leibler_divergence\n",
    "- poisson\n",
    "- cosine_proximity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
