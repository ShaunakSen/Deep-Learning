{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization in NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/%5CGradient%20Descent%5COptimization_in_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-NLwbrZE_F3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimization Algorithms in Neural Networks\n",
        "\n",
        "### Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent\n",
        "\n",
        "[article link](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)\n",
        "\n",
        "Optimization algorithms helps us to minimize (or maximize) an Objective function (another name for Error function) E(x) which is simply a mathematical function dependent on the Model’s internal learnable parameters which are used in computing the target values(Y) from the set of predictors(X) used in the model.\n",
        "\n",
        "The internal parameters of a Model play a very important role in efficiently and effectively training a Model and produce accurate results. This is why we use various Optimization strategies and algorithms to update and calculate appropriate and optimum values of such model’s parameters which influence our Model’s learning process and the output of a Model.\n",
        "\n",
        "### First Order Optimization Algorithms \n",
        "\n",
        "These algorithms minimize or maximize a Loss function E(x) using its Gradient values with respect to the parameters. Most widely used First order optimization algorithm is Gradient Descent.The First order derivative tells us whether the function is decreasing or increasing at a particular point. First order Derivative basically give us a line which is Tangential to a point on its Error Surface.\n",
        "\n",
        "A Gradient is simply a vector which is a multi-variable generalization of a derivative(dy/dx) which is the instantaneous rate of change of y with respect to x. The difference is that to calculate a derivative of a function which is dependent on more than one variable or multiple variables, a Gradient takes its place. And a gradient is calculated using Partial Derivatives . Also another major difference between the Gradient and a derivative is that a Gradient of a function produces a Vector Field.\n",
        "\n",
        "A Gradient is represented by a Jacobian Matrix — which is simply a Matrix consisting of first order partial Derivatives(Gradients).\n",
        "\n",
        "Hence summing up, a derivative is simply defined for a function dependent on single variables , whereas a Gradient is defined for function dependent on multiple variables.\n",
        "\n",
        "### Second Order Optimization Algorithms \n",
        "\n",
        "Second-order methods use the second order derivative which is also called Hessian to minimize or maximize the Loss function.The Hessian is a Matrix of Second Order Partial Derivatives. Since the second derivative is costly to compute, the second order is not used much .The second order derivative tells us whether the first derivative is increasing or decreasing which hints at the function’s curvature.Second Order Derivative provide us with a quadratic surface which touches the curvature of the Error Surface.\n",
        "\n",
        "Although the Second Order Derivative may be a bit costly to find and calculate, but the advantage of a Second order Optimization Technique is that is does not neglect or ignore the curvature of Surface.Secondly, in terms of Step-wise Performance they are better.\n",
        "\n",
        "### So which Order Optimization Strategy to use ?\n",
        "\n",
        "\n",
        "\n",
        "1.   Now The First Order Optimization techniques are easy to compute and less time consuming , converging pretty fast on large data sets.\n",
        "\n",
        "\n",
        "2.  Second Order Techniques are faster only when the Second Order Derivative is known otherwise, these methods are always slower and costly to compute in terms of both time and memory.\n",
        "\n",
        "Although ,sometimes Newton’s Second Order Optimization technique can sometimes Outperform First Order  Gradient Descent Techniques because Second Order Techniques will not get stuck around paths of slow        convergence around saddle points whereas Gradient Descent sometimes gets stuck and does not converges.\n",
        "\n",
        "\n",
        "Best way to know which one converges fast is to try it out yourself.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0uDmlcPYAuP6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "\n",
        "**θ=θ−η⋅∇J(θ)** — is the formula of the parameter updates, where ‘η’ is the learning rate ,’∇J(θ)’ is the Gradient of Loss function-J(θ) w.r.t parameters-‘θ’.\n",
        "\n",
        "It is the most popular Optimization algorithms used in optimizing a Neural Network. Now gradient descent is majorly used to do Weights updates in a Neural Network Model , i.e update and tune the Model’s parameters in a direction so that we can minimize the Loss function. Now we all know a Neural Network trains via a famous technique called Backpropagation , in which we first propagate forward calculating the dot product of Inputs signals and their corresponding Weights and then apply a activation function to those sum of products, which transforms the input signal to an output signal and also is important to model complex Non-linear functions and introduces Non-linearities to the Model which enables the Model to learn almost any arbitrary functional mappings.\n",
        "\n",
        "After this we propagate backwards in the Network carrying Error terms and updating Weights values using Gradient Descent, in which we calculate the gradient of Error(E) function with respect to the Weights (W) or the parameters , and update the parameters (here Weights) in the opposite direction of the Gradient of the Loss function w.r.t to the Model’s parameters.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*iR7vgbLQ6f70cHHIsSYN2g.png)\n",
        "\n",
        "The image on above shows the process of Weight updates in the opposite direction of the Gradient Vector of Error w.r.t to the Weights of the Network. The U-Shaped curve is the Gradient(slope). As one can notice if the Weight(W) values are too small or too large then we have large Errors , so want to update and optimize the weights such that it is neither too small nor too large , so we descent downwards opposite to the Gradients until we find a local minima.\n",
        "\n",
        "The traditional Batch Gradient Descent will calculate the gradient of the whole Data set but will perform only one update , hence it can be very slow and hard to control for datasets which are very very large and don’t fit in the Memory. How big or small of an update to do is determined by the Learning Rate -η , and it is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.Another thing while using Standard batch Gradient descent is that it computes redundant updates for large data sets.\n",
        "\n",
        "The above problems of Standard Gradient Descent are rectified in Stochastic Gradient Descent.\n",
        "\n",
        "### Stochastic gradient descent\n",
        "\n",
        "Stochastic Gradient Descent(SGD) on the other hand performs a parameter update for each training example .It is usually much faster technique.It performs one update at a time.\n",
        "\n",
        "**θ=θ−η⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples**\n",
        "\n",
        "Now due to these frequent updates ,parameters updates have high variance and causes the Loss function to fluctuate to different intensities. This is actually a good thing because it helps us discover new and possibly better local minima , whereas Standard Gradient Descent will only converge to the minimum of the basin as mentioned above.\n",
        "\n",
        "But the problem with SGD is that due to the frequent updates and fluctuations it ultimately complicates the convergence to the exact minimum and will keep overshooting due to the frequent fluctuations .\n",
        "\n",
        "Although, it has been shown that as we slowly decrease the learning rate-η, SGD shows the same convergence pattern as Standard gradient descent.\n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*BS5UuWEE_qXzoWBDQumgDA.png)\n",
        "\n",
        "The problems of high variance parameter updates and unstable convergence can be rectified in another variant called Mini-Batch Gradient Descent."
      ]
    },
    {
      "metadata": {
        "id": "sFhpn-T5Eexy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mini Batch Gradient Descent\n",
        "\n",
        "An improvement to avoid all the problems and demerits of SGD and standard Gradient Descent would be to use Mini Batch Gradient Descent as it takes the best of both techniques and performs an update for every batch with n training examples in each batch.\n",
        "\n",
        "The advantages of using Mini Batch Gradient Descent are —\n",
        "\n",
        "\n",
        "1. It Reduces the variance in the parameter updates , which can ultimately lead us to a much better and stable convergence.\n",
        "2. Can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient.\n",
        "3. Commonly Mini-batch sizes Range from 50 to 256, but can vary as per the application and problem being solved.\n",
        "4. Mini-batch gradient descent is typically the algorithm of choice when training a neural network nowadays\n",
        "\n",
        "\n",
        "P.S —Actually the term SGD is used also when mini-batch gradient descent is used\n",
        "\n",
        "### Challenges faced while using Gradient Descent and its variants —\n",
        "\n",
        "1. Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence i.e will result in small baby steps towards finding optimal parameter values which minimize loss and finding that valley which directly affects the overall training time which gets too large. While a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n",
        "\n",
        "2. Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n",
        "\n",
        "3. Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous sub-optimal local minima. Actually, Difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n",
        "\n",
        "[Understand saddle points here](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions-videos/v/saddle-points)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Y5gIweEfH6hP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will discuss about the various algorithms which are used to further optimize Gradient Descent.\n",
        "\n",
        "### Momentum\n",
        "\n",
        "\n",
        "[article link](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
        "\n",
        "[video link](https://www.youtube.com/watch?v=k8fTYJPd3_I)\n",
        "\n",
        "Exponentially weighed averages deal with sequences of numbers. Suppose, we have some sequence S which is noisy. For this example I plotted cosine function and added some Gaussian noise. It looks like this:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*U5g-MNIKrZjVnI12ePtbLw.png)\n",
        "\n",
        "Note, that even though these dots seem very close to each over, none of them share x coordinate. It is a unique number for each point. That’s the number the defines the index of each point in our sequence S.\n",
        "\n",
        "What we want to do with this data is, instead of using it, we want some kind of ‘moving’ average which would ‘denoise’ the data and bring it closer to the original function. Exponentially weighed averages can give us a pictures which looks like this:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*fhHakQ1nWN7HK1KBNdarqw.png)\n",
        "\n",
        "As you can see, that’s a pretty good result. Instead of having data with a lot of noise, we got much smoother line, which is closer to the original function than data we had. Exponentially weighed averages define a new sequence V with the following equation:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*KQC1UiYUxdzA5IsSEg4Gow.png)\n",
        "\n",
        "That sequence V is the one plotted yellow above. Beta is another hyper-parameter which takes values from 0 to one. I used beta = 0.9 above. It is a good value and most often used in SGD with momentum. Intuitively, you can think of beta as follows. We’re approximately averaging over last 1 / (1- beta) points of sequence. Let’s see how the choice of beta affects our new sequence V.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*buj-RJg3wW6RSclnpczkzA.png)\n",
        "\n",
        "As you can see, with smaller numbers of beta, the new sequence turns out to be fluctuating a lot, because we’re averaging over smaller number of examples and therefore are ‘closer’ to the noisy data. With bigger values of beta, like beta=0.98, we get much smother curve, but it’s a little bit shifted to the right, because we average over larger number of example(around 50 for beta=0.98). Beta = 0.9 provides a good balance between these two extremes.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*cDsuuj5wMAQ2AJrJ1AcXrA.png)\n",
        "\n",
        "From this equation we see, that the value of Tth number of the new sequence is dependent on all the previous values 1..t of the original sequence S. All of the values from S are assigned some weight. \n",
        "\n",
        "Because beta is less than 1, it becomes even smaller when we take beta to the power of some positive number. So the older values of S get much smaller weight and therefore contribute less for overall value of the current point of V. At some point the weight is going to be so small that we can almost say that we ‘forget’ that value because its contribution becomes too small to notice. A good value to use for this approximation is when weight becomes smaller than 1 / e. Bigger values of beta require larger values of the power to be smaller than 1 / e. This is why with bigger values of beta we’re averaging over bigger numbers of points. The following graph shows how fast weights get smaller with older values of S compared to threshold = 1 / e, where we mostly ‘forget’ values that are older.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*ptkdnVKPzIXRbrdsKXY4PQ.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "XjpIZF7pVcKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent With Momentum\n",
        "\n",
        "GD takes a lot of steps and oscillates and converges towards the minimum as shown in the contour plot below\n",
        "These up and down oscillations slow down GD and prevent us to use a much larger LR. If we use a large LR we might overshoot and diverge as shown in the purple line\n",
        "\n",
        "![](diag1.png)\n",
        "\n",
        "This forces us to use a small LR\n",
        "\n",
        "In vertical axis, we want our learning to be slower as we don not want these oscillations\n",
        "\n",
        "But in the hor axis, we want it to be fast to aggressively move towards minimum\n",
        "\n",
        "**This can be done with GD with momentum**\n",
        "\n",
        "On each iteration t:\n",
        "\n",
        "- We compute the derivatives dw and db on current mini batch (works for batch GD as well)\n",
        "\n",
        "- $V_{dw} = \\beta V_{dw} + (1-\\beta)dw$: This is similar to what we saw above:\n",
        "\n",
        "\n",
        "  ![](https://raw.githubusercontent.com/ShaunakSen/Deep-Learning/master/Gradient%20Descent/img/diag1.png)\n",
        "  \n",
        "  So basically we are calculating the **moving averages for the derivatives of w (dw)**\n",
        "  \n",
        "- Similarly $V_{db} = \\beta V_{db} + (1-\\beta)db$\n",
        "\n",
        "- We update our wts :\n",
        "\n",
        "  $w = w - \\alpha V_{dw}$\n",
        "  \n",
        "  $b = b - \\alpha V_{db}$\n",
        "  \n",
        "\n",
        "This smoothes out the steps of GD. For eg say the last few derivatives of dw or db were as shown:\n",
        "\n",
        "![](https://raw.githubusercontent.com/ShaunakSen/Deep-Learning/master/Gradient%20Descent/img/diag2.png)\n",
        " \n",
        "\n",
        "If we average out these gradients,we find oscillations in vertical direction will average to close to zero. Remember this is good because in the vertical direction we want to slow things down. \n",
        "In the horizontal direction, the avg will still be pretty big\n",
        "\n",
        "\n",
        "This is why with this algorithm, with a few iterations, we will find GD with momentum takes steps which have much smaller oscillations in the vertical direction but which are more directed towards horizontal direction\n",
        "\n",
        "This allows our algo to take a more **directed path**\n",
        "\n",
        " ![](https://raw.githubusercontent.com/ShaunakSen/Deep-Learning/master/Gradient%20Descent/img/diag3.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "S_srlP2FcY0k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementation details\n",
        "\n",
        "Now we have 2 hyperparams: alpha (LR) and beta (which controls the exponentially wt avg)\n",
        "\n",
        "Most common value of beta is 0.9 (avg over last 10 iteration gradients)\n",
        "\n",
        "![](https://raw.githubusercontent.com/ShaunakSen/Deep-Learning/master/Gradient%20Descent/img/diag4.png)"
      ]
    }
  ]
}