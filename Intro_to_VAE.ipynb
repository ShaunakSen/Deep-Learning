{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Intro_to_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ5f4ELVhFqh",
        "colab_type": "text"
      },
      "source": [
        "## Variational Autoencoders Explained\n",
        "\n",
        "[tutorial link](http://kvfrans.com/variational-autoencoders-explained/)\n",
        "\n",
        "There were a couple of downsides to using a plain GAN\n",
        "\n",
        "First, the images are generated off some arbitrary noise. If you wanted to generate a picture with specific features, there's no way of determining which initial noise values would produce that picture, other than searching over the entire distribution.\n",
        "\n",
        "Second, a generative adversarial model only discriminates between \"real\" and \"fake\" images. There's no constraints that an image of a cat has to look like a cat. This leads to results where there's no actual object in a generated image, but the style just looks like picture.\n",
        "\n",
        "In this post, I'll go over the variational autoencoder, a type of network that solves these two problems.\n",
        "\n",
        "### What is a variational autoencoder?\n",
        "\n",
        "To get an understanding of a VAE, we'll first start from a simple network and add parts step by step.\n",
        "\n",
        "An common way of describing a neural network is an approximation of some function we wish to model. However, they can also be thought of as a data structure that holds information.\n",
        "\n",
        "Let's say we had a network comprised of a few deconvolution layers. We set the input to always be a vector of ones. Then, we can train the network to reduce the mean squared error between itself and one target image. The \"data\" for that image is now contained within the network's parameters.\n",
        "\n",
        "![](http://kvfrans.com/content/images/2016/08/dat.jpg)\n",
        "\n",
        "Now, let's try it on multiple images. Instead of a vector of ones, we'll use a one-hot vector for the input. [1, 0, 0, 0] could mean a cat image, while [0, 1, 0, 0] could mean a dog. This works, but we can only store up to 4 images. Using a longer vector means adding in more and more parameters so the network can memorize the different images.\n",
        "\n",
        "To fix this, we use a vector of real numbers instead of a one-hot vector. We can think of this as a code for an image, which is where the terms encode/decode come from. For example, [3.3, 4.5, 2.1, 9.8] could represent the cat image, while [3.4, 2.1, 6.7, 4.2] could represent the dog. This initial vector is known as our latent variables.\n",
        "\n",
        "Choosing the latent variables randomly, like I did above, is obviously a bad idea. In an autoencoder, we add in another component that takes in the original images and encodes them into vectors for us. The deconvolutional layers then \"decode\" the vectors back to the original images.\n",
        "\n",
        "![](http://kvfrans.com/content/images/2016/08/autoenc.jpg)\n",
        "\n",
        "We've finally reached a stage where our model has some hint of a practical use. We can train our network on as many images as we want. If we save the encoded vector of an image, we can reconstruct it later by passing it into the decoder portion. What we have is the standard autoencoder.\n",
        "\n",
        "However, we're trying to build a generative model here, not just a fuzzy data structure that can \"memorize\" images. We can't generate anything yet, since we don't know how to create latent vectors other than encoding them from images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUsbe6CqnIDw",
        "colab_type": "text"
      },
      "source": [
        "There's a simple solution here. We add a constraint on the encoding network, that forces it to generate latent vectors that roughly follow a unit gaussian distribution. It is this constraint that separates a variational autoencoder from a standard one.\n",
        "\n",
        "Generating new images is now easy: all we need to do is sample a latent vector from the unit gaussian and pass it into the decoder.\n",
        "\n",
        "In practice, there's a tradeoff between how accurate our network can be and how close its latent variables can match the unit gaussian distribution.\n",
        "\n",
        "We let the network decide this itself. For our loss term, we sum up two separate losses: the generative loss, which is a mean squared error that measures how accurately the network reconstructed the images, and a latent loss, which is the KL divergence that measures how closely the latent variables match a unit gaussian.\n",
        "\n",
        "```\n",
        "generation_loss = mean(square(generated_image - real_image))  \n",
        "latent_loss = KL-Divergence(latent_variable, unit_gaussian)  \n",
        "loss = generation_loss + latent_loss \n",
        "```\n",
        "\n",
        "In order to optimize the KL divergence, we need to apply a simple reparameterization trick: instead of the encoder generating a vector of real values, it will generate a vector of means and a vector of standard deviations.\n",
        "\n",
        "![](http://kvfrans.com/content/images/2016/08/vae.jpg)\n",
        "\n",
        "This lets us calculate KL divergence as follows:\n",
        "\n",
        "```\n",
        "# z_mean and z_stddev are two vectors generated by encoder network\n",
        "latent_loss = 0.5 * tf.reduce_sum(tf.square(z_mean) + tf.square(z_stddev) - tf.log(tf.square(z_stddev)) - 1,1)  \n",
        "```\n",
        "\n",
        "When we're calculating loss for the decoder network, we can just sample from the standard deviations and add the mean, and use that as our latent vector:\n",
        "\n",
        "```\n",
        "samples = tf.random_normal([batchsize,n_z],0,1,dtype=tf.float32)  \n",
        "sampled_z = z_mean + (z_stddev * samples)  \n",
        "```\n",
        "\n",
        "In addition to allowing us to generate random latent variables, this constraint also improves the generalization of our network.\n",
        "\n",
        "To visualize this, we can think of the latent variable as a transfer of data.\n",
        "\n",
        "Let's say you were given a bunch of pairs of real numbers between [0, 10], along with a name. For example, 5.43 means apple, and 5.44 means banana. When someone gives you the number 5.43, you know for sure they are talking about an apple. We can essentially encode infinite information this way, since there's no limit on how many different real numbers we can have between [0, 10].\n",
        "\n",
        "However, what if there was a gaussian noise of one added every time someone tried to tell you a number? Now when you receive the number 5.43, the original number could have been anywhere around [4.4 ~ 6.4], so the other person could just as well have meant banana (5.44).\n",
        "\n",
        "The greater standard deviation on the noise added, the less information we can pass using that one variable.\n",
        "\n",
        "Now we can apply this same logic to the latent variable passed between the encoder and decoder. The more efficiently we can encode the original image, the higher we can raise the standard deviation on our gaussian until it reaches one.\n",
        "\n",
        "This constraint forces the encoder to be very efficient, creating information-rich latent variables. This improves generalization, so latent variables that we either randomly generated, or we got from encoding non-training images, will produce a nicer result when decoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X37SHai1rxkJ",
        "colab_type": "text"
      },
      "source": [
        "## Variational Autoencoder in PyTorch\n",
        "\n",
        "[tutorial link](https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/)\n",
        "\n",
        "The general idea of the autoencoder (AE) is to squeeze information through a narrow bottleneck between the mirrored encoder (input) and decoder (output) parts of a neural network. (see the diagram below)\n",
        "\n",
        "Because the network achitecture and loss function are setup so that the output tries to emulate the input, the network has to learn how to encode input data on the very limited space represented by the bottleneck.\n",
        "\n",
        "Variational Autoencoders, or VAEs, are an extension of AEs that additionally force the network to ensure that samples are normally distributed over the space represented by the bottleneck.\n",
        "\n",
        "They do this by having the encoder output two n-dimensional (where n is the number of dimensions in the latent space) vectors representing the mean and the standard devation. These Gaussians are sampled, and the samples are sent through the decoder. This is the reparameterization step, also see my comments in the reparameterize() function.\n",
        "\n",
        "The loss function has a term for input-output similarity, and, importantly, it has a second term that uses the Kullback–Leibler divergence to test how close the learned Gaussians are to unit Gaussians.\n",
        "\n",
        "The loss function has a term for input-output similarity, and, importantly, it has a second term that uses the Kullback–Leibler divergence to test how close the learned Gaussians are to unit Gaussians.\n",
        "\n",
        "In other words, this extension to AEs enables us to derive Gaussian distributed latent spaces from arbitrary data. Given for example a large set of shapes, the latest space would be a high-dimensional space where each shape is represented by a single point, and the points would be normally distributed over all dimensions. With this one can represent existing shapes, but one can also synthesise completely new and plausible shapes by sampling points in latent space.\n",
        "\n",
        "### Results using MNIST\n",
        "\n",
        "\n",
        "Below you see 64 random samples of a two-dimensional latent space of MNIST digits that I made with the example below, with ZDIMS=2.\n",
        "\n",
        "![](https://vxlabs.com/wp-content/uploads/2017/12/pytorch-vae-sample-z2-epoch10.png?w=660&ssl=1)\n",
        "\n",
        "Next is the reconstruction of 8 random unseen test digits via a more reasonable 20-dimensional latent space. Keep in mind that the VAE has learned a 20-dimensional normal distribution for any input digit, from which samples are drawn that reconstruct via the decoder to output that appear similar to the input.\n",
        "\n",
        "![](https://vxlabs.com/wp-content/uploads/2017/12/pytorch-vae-reconstruction-z10-epoch10.png?w=660&ssl=1)\n",
        "\n",
        "### A diagram of a simple VAE\n",
        "\n",
        "An example VAE, incidentally also the one implemented in the PyTorch code below, looks like this:\n",
        "\n",
        "![](https://vxlabs.com/wp-content/uploads/2017/12/pytorch-vae-arch-2.png?resize=660%2C317&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "novjebtjg3dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU2tu7Z3uZhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# changed configuration to this instead of argparse for easier interaction\n",
        "CUDA = True\n",
        "SEED = 1\n",
        "BATCH_SIZE = 128\n",
        "LOG_INTERVAL = 10\n",
        "EPOCHS = 10\n",
        "# connections through the autoencoder bottleneck\n",
        "# in the pytorch VAE example, this is 20\n",
        "ZDIMS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rFMnKwHvFwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "726792ea-8437-485e-c09a-dda97543dab4"
      },
      "source": [
        "torch.manual_seed(SEED)\n",
        "if CUDA:\n",
        "    print (\"yes\") \n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# DataLoader instances will load tensors directly into GPU memory\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQQ3J2qYvQ1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download or load downloaded MNIST dataset\n",
        "# shuffle data at every epoch\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
        "                                            batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "# each training example comes in batch size of 128, has 1 ip color channel and is 28x28. So [128,1,28,28]\n",
        "\n",
        "# Same for test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAWrt0C3xf9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "93b4727a-8724-4576-8658-833f8ca201a5"
      },
      "source": [
        "train_loader.dataset.train_data[0].shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBBsYP24yxEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    #-----------ENCODER-----------------\n",
        "\n",
        "    # 28 x 28 pixels = 784 input pixels, 400 outputs\n",
        "\n",
        "    self.fc1 = nn.Linear(784, 400)\n",
        "    # rectified linear unit layer from 400 to 400\n",
        "    # max(0, x)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc21 = nn.Linear(400, ZDIMS) # mu layer\n",
        "    self.fc22 = nn.Linear(400, ZDIMS) # logvariance layer\n",
        "    # this last layer bottlenecks through ZDIMS connections\n",
        "\n",
        "    #-----------DECODER-----------------\n",
        "\n",
        "\n",
        "    # from bottleneck to hidden 400\n",
        "\n",
        "    self.fc3 = nn.Linear(ZDIMS, 400)\n",
        "\n",
        "    # from hidden 400 to 784 outputs\n",
        "    self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAe_urQM0RFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(self, x: Variable) -> (Variable, Variable):\n",
        "  \"\"\"\n",
        "  Input vector x -> fully connected 1 -> ReLU -> (fully connected\n",
        "  21, fully connected 22)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : [128, 784] matrix; 128 digits of 28x28 pixels each\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  (mu, logvar) : ZDIMS (here 20) mean units one for each latent dimension, ZDIMS\n",
        "      variance units one for each latent dimension\n",
        "\n",
        "  mu : [128, ZDIMS] mean matrix\n",
        "  logvar : [128, ZDIMS] variance matrix\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # h1 is [128, 400]\n",
        "  h1 = self.relu(self.fc1(x))  # type: Variable\n",
        "\n",
        "  return self.fc21(h1), self.fc22(h1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "819n-dW4OuR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
        "  \n",
        "  \"\"\"\n",
        "  THE REPARAMETERIZATION IDEA:\n",
        "  \n",
        "  For each training sample (we get 128 images batched at a time)\n",
        "  \n",
        "  - take the current learned mu, stddev for each of the ZDIMS\n",
        "    dimensions and draw a random sample from that distribution\n",
        "  \n",
        "  - the whole network is trained so that these randomly drawn\n",
        "    samples decode to output that looks like the input\n",
        "    \n",
        "  - which will mean that the std, mu will be learned\n",
        "    *distributions* that correctly encode the inputs\n",
        "    \n",
        "  - due to the additional KLD term (see loss_function() below)\n",
        "    the distribution will tend to unit Gaussians\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  mu : [128, ZDIMS] mean matrix\n",
        "  logvar : [128, ZDIMS] variance matrix\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "\n",
        "  During training random sample from the learned ZDIMS-dimensional\n",
        "  normal distribution; during inference its mean.\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  if self.training:\n",
        "    # convert log variance to exp\n",
        "    # multiply log variance with 0.5, then in-place exponent\n",
        "    # yielding the standard deviation\n",
        "    \n",
        "    std = logvar.mul(0.5).exp_() # type: Variable\n",
        "    # - std.data is the [128,ZDIMS] tensor that is wrapped by std\n",
        "    \n",
        "    # - so eps is [128,ZDIMS] with all elements drawn from a mean 0\n",
        "    #   and stddev 1 normal distribution that is 128 samples\n",
        "    #   of random ZDIMS-float vectors\n",
        "    \n",
        "    eps = Variable(std.data.new(std.size()).normal_())\n",
        "    \n",
        "    # - sample from a normal distribution with standard\n",
        "    #   deviation = std and mean = mu by multiplying mean 0\n",
        "    #   stddev 1 sample with desired std and mu, see\n",
        "    #   https://stats.stackexchange.com/a/16338\n",
        "    # - so we have 128 sets (the batch) of random ZDIMS-float\n",
        "    #   vectors sampled from normal distribution with learned\n",
        "    #   std and mu for the current input\n",
        "    \n",
        "    return eps.mul(std).add_(mu)\n",
        "  \n",
        "  else:\n",
        "    # During inference, we simply spit out the mean of the\n",
        "    # learned distribution for the current input.  We could\n",
        "    # use a random sample from the distribution, but mu of\n",
        "    # course has the highest probability.\n",
        "    return mu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo-SPclXYffa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(self, z:Variable) -> Variable:\n",
        "  #[128, 20] -> [128, 400] -> [128, 784]\n",
        "  \n",
        "  h3 = self.relu(self.fc3(z))\n",
        "  return self.sigmoid(self.fc4(h3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQNktWh_oa_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(self, x:Variable) -> (Variable, Variable, Variable):\n",
        "  mu, logvar = self.encode(x.view(-1, 784))\n",
        "  z = self.reparameterize(mu, logvar)\n",
        "  return self.decode(z), mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5ZzXIDhpBT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init the model\n",
        "\n",
        "model = VAE()\n",
        "\n",
        "if CUDA:\n",
        "  model.cuda()\n",
        "  \n",
        "# define ADAM as optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "  \n",
        "def loss_function(recon_x, x, mu, logvar) -> Variable:\n",
        "  # how well do input x and output recon_x agree?\n",
        "  \n",
        "  BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
        "  \n",
        "  # KLD is Kullback–Leibler divergence -- how much does one learned\n",
        "  # distribution deviate from another, in this specific case the\n",
        "  # learned distribution from the unit Gaussian\n",
        "\n",
        "  # see Appendix B from VAE paper:\n",
        "  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "  # https://arxiv.org/abs/1312.6114\n",
        "  # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "  # note the negative D_{KL} in appendix B of the paper\n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "  # Normalise by same number of elements as in reconstruction\n",
        "  KLD /= BATCH_SIZE * 784\n",
        "\n",
        "  # BCE tries to make our reconstruction as accurate as possible\n",
        "  # KLD tries to push the distributions as close as possible to unit Gaussian\n",
        "  return BCE + KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWWFcehmv06I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60eeeb41-b785-47f2-fbfa-722895ec580c"
      },
      "source": [
        "len(train_loader.dataset)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LAKw0y9sMK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  # toggle model to train mode\n",
        "  \n",
        "  model.train()\n",
        "  \n",
        "  train_loss = 0\n",
        "  \n",
        "  # in the case of MNIST, len(train_loader.dataset) is 60000\n",
        "  # each `data` is of BATCH_SIZE samples and has shape [128, 1, 28, 28]\n",
        "  \n",
        "  for batch_idx, (data, _) in enumerate(train_loader):\n",
        "    data = Variable(data)\n",
        "    if CUDA:\n",
        "      data = data.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # push whole batch of data through VAE.forward() to get recon_loss\n",
        "    \n",
        "    recon_batch, mu, logvar = model(data) # equivalent to calling forward(data)\n",
        "    \n",
        "    # calculate scalar loss\n",
        "    \n",
        "    loss = loss_function(recon_batch, data, mu, logvar)\n",
        "    \n",
        "    # calculate the gradient of the loss w.r.t. the graph leaves\n",
        "    # i.e. input variables -- by the power of pytorch!\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    train_loss += loss.data[0]\n",
        "    \n",
        "    # perform the optimization step\n",
        "    optimizer.step()\n",
        "    \n",
        "    if batch_idx % LOG_INTERVAL == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader),\n",
        "          loss.data[0] / len(data)))\n",
        "      \n",
        "  print(\"======> Epoch: {} Average loss: {:.4f}\".format(epoch, train_loss/len(train_loader.dataset)))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}