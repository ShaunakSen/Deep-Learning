{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJb49pMMKdCuk++DihGwRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/ML_algos_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF9WDuWyNFIc"
      },
      "source": [
        "# ML algos basics from scratch\n",
        "\n",
        "> Credits: Python engineer channel: https://www.youtube.com/playlist?list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E\n",
        "\n",
        "---\n",
        "\n",
        "## KNN\n",
        "\n",
        "It is basically based on majority voting. Say we have k = 3 and we are trying to detect a binary class label\n",
        "\n",
        "For a new point we look at the 3 closest training pts and pick the majority\n",
        "\n",
        "### Distance metrics\n",
        "\n",
        "Read here: https://machinelearningmastery.com/distance-measures-for-machine-learning/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhwKG-bRP9MW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "cmap = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSJqpkKtVepG"
      },
      "source": [
        "def euclidean_distance(v1, v2):\n",
        "    return np.sqrt(np.sum((v1 - v2)**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5H1vMi2jxz"
      },
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X are the training samples\n",
        "        y are the training labels\n",
        "        \"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Given\n",
        "        \"\"\"\n",
        "\n",
        "        pred_labels = [self._predict(x) for x in X]\n",
        "        return np.array(pred_labels)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        ## 1. compute distances\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        distances = np.array(distances)\n",
        "        ## 2. get k neares sample labels\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        ## 3. majority voting\n",
        "        most_common = Counter(k_nearest_labels).most_common(1) ### returns [(most_common_item, count), (2nd most common, count), ...]\n",
        "        return most_common[0][0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_7vSDtYP6Yu"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf2hiFpDQiC7",
        "outputId": "83c854f8-9e46-4bbe-ed58-0e786dbaa37c"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 4), (150,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTgrZKbDQkC0",
        "outputId": "25a65b60-eb84-4bce-8f8c-a3b1fc255951"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n",
        "\n",
        "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 4) (30, 4) (120,) (30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "maxx-8uzUN4L",
        "outputId": "9c47488f-d585-4aa9-8c97-677a36809588"
      },
      "source": [
        "plt.scatter(x=X[:, 0], y=X[:, 3], c=y, cmap=cmap)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYKyeNLYUcsD"
      },
      "source": [
        "clf = KNN(k=3)\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uji80EhlXeOc"
      },
      "source": [
        "pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWF66_KXXhcO",
        "outputId": "30ab5067-c1ca-44df-eae2-4bbe098fd2a0"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0,\n",
              "       1, 2, 1, 1, 1, 2, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbIaRxPBXizE"
      },
      "source": [
        "acc = np.sum(pred == y_test)/y_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIYRt8QuXpfU",
        "outputId": "e07804ad-c114-4132-fc4f-59eac846acf9"
      },
      "source": [
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbykTMQWNb8f"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "1. Assumptions of LR\n",
        "2. Lasso and ridge - read here: https://colab.research.google.com/drive/1mDSuiBb2LvtpMorp1nQuUg6GNmPGzZaC\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN1mOadt1SlU"
      },
      "source": [
        "![](https://i.imgur.com/TDkVmuG.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcYDJXdqXqoF"
      },
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self, lr=0.001, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.theta = None\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        \"\"\"\n",
        "        Loss : (y - X. theta)^T . (y - X.theta) where y_pred = X.theta\n",
        "        \"\"\"\n",
        "        return np.dot((y-y_pred).T, (y-y_pred))[0][0]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        X: training data : N x d\n",
        "        y: true labels: N x 1\n",
        "        \"\"\"\n",
        "\n",
        "        # 1 : append a column of 1 to X to turn this into a design matrix\n",
        "        n_samples = X.shape[0]\n",
        "        n_features = X.shape[1]\n",
        "        all_ones = np.ones((n_samples, 1))\n",
        "\n",
        "        X = np.concatenate((all_ones, X), axis=1)\n",
        "\n",
        "        # 2: initialize theta as a vector of shape (d+1) x 1\n",
        "\n",
        "        theta = np.zeros((n_features+1, 1))\n",
        "        y_pred = np.dot(X, theta)\n",
        "        loss_value = self.loss(y, y_pred)\n",
        "        n_ = 0 ### counter tracking the number of iterations\n",
        "\n",
        "        while loss_value > 1 and n_ < self.n_iters:\n",
        "\n",
        "            y_pred = np.dot(X, theta)\n",
        "\n",
        "            gradient = -X.T.dot(y - y_pred)\n",
        "\n",
        "            theta = theta - self.lr * gradient\n",
        "\n",
        "            loss_value = self.loss(y, y_pred)\n",
        "            if n_%10 == 0:\n",
        "                print (n_, loss_value)\n",
        "            n_ += 1\n",
        "\n",
        "        ### assign the trained wts\n",
        "        self.theta = theta\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Given a set of samples predict the values using trained wts\n",
        "        \"\"\"\n",
        "        # 1 : append a column of 1 to X to turn this into a design matrix\n",
        "        n_samples = X.shape[0]\n",
        "        n_features = X.shape[1]\n",
        "        all_ones = np.ones((n_samples, 1))\n",
        "\n",
        "        X = np.concatenate((all_ones, X), axis=1)\n",
        "        print (X.shape, self.theta.shape)\n",
        "        if self.theta is None:\n",
        "            return 'Please run .fit(X,y) before running prediction'\n",
        "\n",
        "        return np.dot(X, self.theta).flatten()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFkqde3zeox1"
      },
      "source": [
        "X = np.random.randint(low=0, high=9, size=(10, 4))\n",
        "y = np.random.randint(low=0, high=9, size=(10, 1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbj9V4WDezMy"
      },
      "source": [
        "linReg = LinearRegression(0.0001)\n",
        "linReg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx0tiYhFe1sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8f43ac-19fb-4720-8e64-f73fd1c71b43"
      },
      "source": [
        "y_pred = linReg.predict(X)\n",
        "\n",
        "print (X.shape, y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5) (5, 1)\n",
            "(10, 4) (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWvdqMAf36xn"
      },
      "source": [
        "# Load the diabetes dataset\n",
        "diabetes_data = datasets.load_diabetes()\n",
        "\n",
        "feature_names = diabetes_data['feature_names']\n",
        "X, y = diabetes_data['data'], diabetes_data['target']\n",
        "\n",
        "X = pd.DataFrame(data=X, columns=feature_names)\n",
        "y = pd.DataFrame(data=y, columns=['Target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "6-3bObNt4i3T",
        "outputId": "fea185eb-8153-4382-f615-644346cfe546"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.059744</td>\n",
              "      <td>-0.005697</td>\n",
              "      <td>-0.002566</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.007207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>-0.067642</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.079165</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.018118</td>\n",
              "      <td>0.044485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>0.017282</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.013840</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.011080</td>\n",
              "      <td>-0.046879</td>\n",
              "      <td>0.015491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>0.044528</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.073030</td>\n",
              "      <td>-0.081414</td>\n",
              "      <td>0.083740</td>\n",
              "      <td>0.027809</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.003064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          age       sex       bmi  ...        s4        s5        s6\n",
              "0    0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646\n",
              "1   -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204\n",
              "2    0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930\n",
              "3   -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362\n",
              "4    0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "437  0.041708  0.050680  0.019662  ... -0.002592  0.031193  0.007207\n",
              "438 -0.005515  0.050680 -0.015906  ...  0.034309 -0.018118  0.044485\n",
              "439  0.041708  0.050680 -0.015906  ... -0.011080 -0.046879  0.015491\n",
              "440 -0.045472 -0.044642  0.039062  ...  0.026560  0.044528 -0.025930\n",
              "441 -0.045472 -0.044642 -0.073030  ... -0.039493 -0.004220  0.003064\n",
              "\n",
              "[442 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Us0vJg94joS"
      },
      "source": [
        "linReg = LinearRegression(0.002, 10000)\n",
        "linReg.fit(X.values, y.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMa4eBz_LnXm"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "> Notes on the explanation by Andrew Ng: https://www.youtube.com/watch?v=-la3q9d7AKQ\n",
        "\n",
        "Say we have a dataset of tumor size and whether the tumor is malignant (1) or not (0)\n",
        "\n",
        "We can simply fit a st line through it using LinReg\n",
        "\n",
        "and then we set a threshold on the y axis value\n",
        "\n",
        "![](https://i.imgur.com/km28jFh.png)\n",
        "\n",
        "But is this a good approach\n",
        "\n",
        "Say we have an example way out to the right, now intuitively this should not change the decision boundary, but when we fit LinReg, the line will look different and the decision boundary if we chose the same threshold of 0.5 will change\n",
        "\n",
        "![](https://i.imgur.com/tfU8Z2O.png)\n",
        "\n",
        "So by adding a new example the decision boundary shifted and caused us to get a worse hypothesis\n",
        "\n",
        "More on the problems of using regression for classification tasks: https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression\n",
        "\n",
        "Also in LinReg the hypothesis can o/p values lesser than 0 or more than 1, which is counter-intuitive for classification\n",
        "\n",
        "---\n",
        "\n",
        "Logisic Regression model\n",
        "\n",
        "We simply apply the sigmoid function to the hypothesis to transform it to a range bw 0->1\n",
        "\n",
        "![](https://i.imgur.com/H7nBYXX.png)\n",
        "\n",
        "#### Interpretation of hypothesis op\n",
        "\n",
        "![](https://i.imgur.com/kGj8FQN.png)\n",
        "\n",
        "![](https://i.imgur.com/4B5thwA.png)\n",
        "\n",
        "![](https://i.imgur.com/RIeVlWu.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2wBiG1Ob5nn"
      },
      "source": [
        "#### Decision boundary\n",
        "\n",
        "We want to analyze further when the hypothesis predicts a -ve or a +ve class\n",
        "\n",
        "Say we set a threshold of 0.5, s.t whenever $h_{\\theta}(x) \\geq 0.5$ we predict `y = 1` and whenever $h_{\\theta}(x) <  0.5$ we predict `y=0`\n",
        "\n",
        "From the sigmoid curve we see that g(z) >= 0.5 whenever z > 0\n",
        "\n",
        "Similarly as we have assumed for LogReg model that $z = \\theta^Tx $, we can say that $h_{\\theta}(x) = g(\\theta^Tx) \\geq 0.5$ when $\\theta^Tx \\geq 0$, so we predict y=1 whenever $\\theta^Tx > 0$\n",
        "\n",
        "![](https://i.imgur.com/mvToRcK.png)\n",
        "\n",
        "Similarly we predict y=0 whenever z < 0, i.e when $\\theta^Tx < 0$\n",
        "\n",
        "We can use this iinfo to better understand the decision boundary of LogReg\n",
        "\n",
        "Say we have:\n",
        "\n",
        "![](https://i.imgur.com/f4KffDj.jpeg)\n",
        "\n",
        "Now we have not discussed how to find these theta values but assume for now that they take the values (-3,1,1)\n",
        "\n",
        "![](https://i.imgur.com/DbNdzaD.png)\n",
        "\n",
        "Expanding on this, we can draw the db:\n",
        "\n",
        "![](https://i.imgur.com/o1mSM0F.png)\n",
        "\n",
        "Similarly for the region `x1+x2<3` we would predict y=0\n",
        "\n",
        "The line separating the 2 classes is called the __decision boundary__\n",
        "\n",
        "Also note that the db is a function of the theta values - we do not need the training set for plotting the db. of course the training set defines the values of theta, but the db itself can be determined once we have the theta values\n",
        "\n",
        "#### Non linear desision boundaries\n",
        "\n",
        "We can add higher order poly terms to LogReg to build complex dbs\n",
        "\n",
        "![](https://i.imgur.com/B9QqZuZ.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCU2le3jGrXJ"
      },
      "source": [
        "#### Cost Function - Problems with the MSE cost function in LogReg\n",
        "\n",
        "Let us understand how the theta values are actually computed\n",
        "\n",
        "Before that let us define the problem:\n",
        "\n",
        "![](https://i.imgur.com/96B9YOK.png)\n",
        "\n",
        "Note here there are m training data points and n features for each example\n",
        "\n",
        "For a single instance:\n",
        "\n",
        "$\\hat{y} = h_{\\theta}(x) =\\mathbf{sigmoid}(\\theta^T_{(1\\times n+1)}\\cdot \\mathbf{x}_{(1\\times n+1)}) \\rightarrow \\mathit{scalar}$\n",
        "\n",
        "#### Design matrix for LogReg\n",
        "\n",
        "In LogReg we can build the design matrix by considering the each instance along the column, and appending a row of 1s at the start\n",
        "\n",
        "`X -> (n+1) x m`\n",
        "\n",
        "![](https://i.imgur.com/uNpljpb.jpeg)\n",
        "\n",
        "Here we get the entire vector $h_{\\theta}(X)$ as avector of shape 1 x m -> the hypothesis for each of the m training instances\n",
        "\n",
        "Eah of these elemenets is a scalar\n",
        "\n",
        "\n",
        "\n",
        "LinReg cost function:\n",
        "\n",
        "![](https://i.imgur.com/o2g4xqU.png)\n",
        "\n",
        "As we have m examples, we sum over m values and divide by m\n",
        "This is the MSE loss in LinReg\n",
        "\n",
        "But in LogReg $h_{\\theta}(x)$ has a non-linearity, so it will be non-convex (having multiple local minima), so hard to optimize via Gradient Descent\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/CssA936.jpeg)\n",
        "\n",
        "![](https://i.imgur.com/ZTEAUG4.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFYxjROPwIQA"
      },
      "source": [
        "### Cost function used in LogReg\n",
        "\n",
        "![](https://i.imgur.com/Oc4sBpL.png)\n",
        "\n",
        "![](https://i.imgur.com/rtX9xwx.png)\n",
        "\n",
        "Again here as y = 0 and we make the hypothesis of near 1 i.e P(y=1| x;theta)-> 1 then the cost is very high\n",
        "\n",
        "If we make the hypothesis of near 0 i.e P(y=1| x;theta)-> 0 then the cost is very low (almost zero)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMcjPb2bypAq"
      },
      "source": [
        "### Simplified Cost Function and Gradient Descent\n",
        "\n",
        "What we have done till now:\n",
        "\n",
        "![](https://i.imgur.com/PXGCTbG.png)\n",
        "\n",
        "![](https://i.imgur.com/NppBbhq.png)\n",
        "\n",
        "To conclude the cost function for LogReg:\n",
        "\n",
        "![](https://i.imgur.com/HzXeAzM.png)\n",
        "\n",
        "Why this particular cost function?\n",
        "\n",
        "- This can be derived from a statistical approach using MLE as well\n",
        "- It is convex, so convenient to optimize using GD\n",
        "\n",
        "![](https://i.imgur.com/1e87ZCq.png)\n",
        "\n",
        "The only task left is how to minimize this cost function\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxLMXzh52H_T"
      },
      "source": [
        "### Minimizing the Cost Function\n",
        "\n",
        "![](https://i.imgur.com/CAs44n5.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL8dEbHviMaA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTprwS2JiMWB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJaGlI4liMNZ"
      },
      "source": [
        "## K-means clustering\n",
        "\n",
        "> https://www.youtube.com/watch?v=0MQEt10e4NM\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "\n",
        "There are certain pts which are the centers of the clusters adn each pt in the cluster is near to one of the centers\n",
        "\n",
        "Also in k-means we assume `K` clusters with centers u1,u2...uk\n",
        "\n",
        "![](https://i.imgur.com/7DMhA1t.png)\n",
        "\n",
        "\n",
        "### Mathematical formulation of Loss\n",
        "\n",
        "For all points i in the data assigned to uj (cluster j) we can find the sum of distances to uj\n",
        "\n",
        "We do this for all the clusters\n",
        "\n",
        "Basically for each cluster j = 1 -> K, we take the points inside that cluser j and compute the distance from the center. This can be written as:\n",
        "\n",
        "![](https://i.imgur.com/flZo9Rh.png)\n",
        "\n",
        "We want to minimize this L\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "K means iteratively tries to mimize L\n",
        "\n",
        "1. Init the centers randomly (u1, u2, ... uk)\n",
        "2. Now we have the centers, chose optimal `a` for fixed `u`\n",
        "    - We just assign each pt to the nearest center\n",
        "    ![](https://i.imgur.com/BZIG6Qg.png)\n",
        "3. Chose optimal `u` for fixed `a`\n",
        "    -\n",
        "4. Repeat 2 and 3 until convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple NN from scratch\n",
        "\n",
        "> https://www.youtube.com/watch?v=tsqwvgHgwzs\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mAtgpwfIkOgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "We want to build a simple NN with\n",
        "- one i/p layer (2 nodes)\n",
        "- one o/p layer (3 nodes) - 3 classes\n",
        "\n",
        "![](https://imgur.com/ZeAYPz0.png)\n",
        "\n",
        "![](https://imgur.com/w8BTe4N.png)\n",
        "\n",
        "Note here we take one sample at a time for __X__\n",
        "\n",
        "### Loss function\n",
        "\n",
        "![](https://imgur.com/o2DxUzv.png)"
      ],
      "metadata": {
        "id": "h2rh3dVzoOkP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlDKC-fF4rj5"
      },
      "source": [
        "f"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}