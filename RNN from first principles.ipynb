{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Understanding RNNs from first principles\n\n> Notes on the blog post by Terence Parr: https://explained.ai/rnn/index.html\n\n---",
   "metadata": {
    "tags": [],
    "cell_id": "00000-299abc3e-dcdb-47d6-a8d8-de7345475589",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## The goal: meaningful vectors representing words\n\nTo understand what's going on inside an RNN, let's re-invent the order-sensitive encodings generated by an RNN using a simple classification problem. Imagine that we have three words for cat in three different languages. Given a word, we'd like to classify it as English, French, or German:\n\n![](https://i.imgur.com/QiYhC19.png)\n\nIn order to numericalize data before training a model, we can encode the targets as classes 0, 1, and 2, which works great. Unfortunately, we can't convert the cat words to unique integers because we get nonsense like 0->0, as shown on the right above.\n\nInstead of a single number per word, we need to come up with a vector of numbers to represent each word. We don't need to know what the vector elements are per se, just that they somehow meaningfully represent a word in some high-dimensional space:\n\n![](https://explained.ai/rnn/images/vector-to-num.svg)\n\nAs an analogy, consider the common tactic of breaking apart a single date feature (often represented as the number of seconds since 1970) into a vector of its constituent components like hour, minute, day, month, year.\n\nOnce we have these meaningful feature vectors (for cat, chat, and katze), we can use them to train a random forest or any other classifier. So this article is all about how we find suitable vectors. To do that, let's baby step through some possible approaches to arrive at the RNN solution.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00001-4f1bbe74-5ccf-494b-bc87-da722edad8bd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Encoding words as integers\n\nThe first thing we have to do is split apart the words into a sequence of characters and encode those characters using a character vocabulary. Computing the vocabulary is straightforward. Any unique integers will work for the vocabulary, but the implementation is simpler if we use consecutive integers starting from zero:\n\n![](https://i.imgur.com/uhnu92r.png)",
   "metadata": {
    "tags": [],
    "cell_id": "00002-c4740132-63dd-4464-a2af-e265686c17c4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-32ed2a8a-83c5-4d21-ba03-1435b9a33020",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "448c953c",
    "execution_start": 1624461831768,
    "execution_millis": 74467677,
    "deepnote_cell_type": "code"
   },
   "source": "vocab = {c:i for i,c in enumerate(\"acehktz\")}\nprint (vocab)",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'a': 0, 'c': 1, 'e': 2, 'h': 3, 'k': 4, 't': 5, 'z': 6}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Unfortunately, the feature vectors for different translations of the word \"cat\" have different lengths (three, four, and five). A simple way to convert these variable length character sequences into a single feature is to add up the vocabulary character encodings, which we can do with a trivial loop:\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00004-58512c6d-9179-49d8-81e5-9e55db4a0228",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-5967d2bf-bc7a-49a0-bea5-ebe2afe43e25",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5007c99d",
    "execution_start": 1624461831822,
    "execution_millis": 74225156,
    "deepnote_cell_type": "code"
   },
   "source": "for word in [\"cat\", \"chat\", \"katze\"]:\n    x = [vocab[letter] for letter in word]\n    h = 0\n    for t in range(len(x)):\n        h+=x[t]\n\n    print (f'{word} -> {h}')",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "cat -> 6\nchat -> 9\nkatze -> 17\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "This is not a great solution: First, because it's unclear that 6, 9, and 17 meaningfully distinguish between the three translations of cat. More importantly, though, this encoding is order independent. For example, a-c-t has the same encoding as c-a-t:\n\nA simple way to make the encoding order dependent is to multiply previous h values by, say, 2. This performs a scale and add operation very much like what we'd find in a hash function. Scalar 2 is not some magic numberâ€”I just chose it randomly, but it's a value we could learn if we created and optimized a loss function.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00006-f1a0fae4-4317-4914-a549-db5fa5c6aedf",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-be43adfb-c41d-4ddd-83e1-cc75d0f090e2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ba904fa6",
    "execution_start": 1624462288642,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "for word in [\"cat\", \"act\"]:\n    x = [vocab[letter] for letter in word]\n    h = 0\n    print (x)\n    for t in range(len(x)):\n        print (f'h_t-1: {h}, x_t: {x[t]}')\n        h = 2*h + x[t]\n        print (f'h_t = 2.h_t-1 + x_t = {h}')\n    \n    print (f'{word} -> {h}')",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "[1, 0, 5]\nh_t-1: 0, x_t: 1\nh_t = 2.h_t-1 + x_t = 1\nh_t-1: 1, x_t: 0\nh_t = 2.h_t-1 + x_t = 2\nh_t-1: 2, x_t: 5\nh_t = 2.h_t-1 + x_t = 9\ncat -> 9\n[0, 1, 5]\nh_t-1: 0, x_t: 0\nh_t = 2.h_t-1 + x_t = 0\nh_t-1: 0, x_t: 1\nh_t = 2.h_t-1 + x_t = 1\nh_t-1: 1, x_t: 5\nh_t = 2.h_t-1 + x_t = 7\nact -> 7\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "That inner loop is equivalent to the following recurrence relation:\n\n![](https://explained.ai/rnn/images/blkeqn-3048294F838BEC17B89CE0457E8359E8.svg)\n\nThe recurrence just says that the value of h at iteration t is twice its previous value plus the encoding of the t_th  character; t moves from 1 to m for m characters in the word. For x = cat we get three values of h beyond our initial value:\n\n![](https://explained.ai/rnn/images/blkeqn-33E117DC61B77D31604D90FAB16799E3.svg)\n\n\n<img src = \"https://i.imgur.com/njh0cOD.jpeg\" width=\"700\" height=\"600\"/>\n\nThe key take away here is that, despite having a constant multiplier of 2, each character encoding is multiplied by a different number, depending on its position in the sequence: x_t is multiplied by 2^(m-t) where m is the no of letters. This is why cat and act get different encodings. We've solved the order dependency issue, but it's unlikely that a single integer will ever contain enough information to meaningfully represent a natural language word.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00008-68916965-ec06-4588-87b7-7c53225c220b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Aggregating character vectors to encode words",
   "metadata": {
    "tags": [],
    "cell_id": "00009-13568037-60e2-4890-9470-edc5c8e22561",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-8ea96f92-7b88-434f-b72e-0959a7cf6ff1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1624461831823,
    "execution_millis": 7,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-af6a11f5-5c25-45e4-a863-9308eeadf3d1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1624461831833,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=306a1d85-aa04-4db6-b565-92149392d58c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "d628ee36-1bd0-42fd-a792-26018c12ad66",
  "deepnote_execution_queue": []
 }
}