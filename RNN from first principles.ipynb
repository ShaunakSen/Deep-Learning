{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Understanding RNNs from first principles\n\n> Notes on the blog post by Terence Parr: https://explained.ai/rnn/index.html\n\n---",
   "metadata": {
    "tags": [],
    "cell_id": "00000-299abc3e-dcdb-47d6-a8d8-de7345475589",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## The goal: meaningful vectors representing words\n\nTo understand what's going on inside an RNN, let's re-invent the order-sensitive encodings generated by an RNN using a simple classification problem. Imagine that we have three words for cat in three different languages. Given a word, we'd like to classify it as English, French, or German:\n\n![](https://i.imgur.com/QiYhC19.png)\n\nIn order to numericalize data before training a model, we can encode the targets as classes 0, 1, and 2, which works great. Unfortunately, we can't convert the cat words to unique integers because we get nonsense like 0->0, as shown on the right above.\n\nInstead of a single number per word, we need to come up with a vector of numbers to represent each word. We don't need to know what the vector elements are per se, just that they somehow meaningfully represent a word in some high-dimensional space:\n\n![](https://explained.ai/rnn/images/vector-to-num.svg)\n\nAs an analogy, consider the common tactic of breaking apart a single date feature (often represented as the number of seconds since 1970) into a vector of its constituent components like hour, minute, day, month, year.\n\nOnce we have these meaningful feature vectors (for cat, chat, and katze), we can use them to train a random forest or any other classifier. So this article is all about how we find suitable vectors. To do that, let's baby step through some possible approaches to arrive at the RNN solution.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00001-4f1bbe74-5ccf-494b-bc87-da722edad8bd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Encoding words as integers\n\nThe first thing we have to do is split apart the words into a sequence of characters and encode those characters using a character vocabulary. Computing the vocabulary is straightforward. Any unique integers will work for the vocabulary, but the implementation is simpler if we use consecutive integers starting from zero:\n\n![](https://i.imgur.com/uhnu92r.png)",
   "metadata": {
    "tags": [],
    "cell_id": "00002-c4740132-63dd-4464-a2af-e265686c17c4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-32ed2a8a-83c5-4d21-ba03-1435b9a33020",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "448c953c",
    "execution_start": 1624549949898,
    "execution_millis": 88118127,
    "deepnote_cell_type": "code"
   },
   "source": "vocab = {c:i for i,c in enumerate(\"acehktz\")}\nprint (vocab)",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'a': 0, 'c': 1, 'e': 2, 'h': 3, 'k': 4, 't': 5, 'z': 6}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Unfortunately, the feature vectors for different translations of the word \"cat\" have different lengths (three, four, and five). A simple way to convert these variable length character sequences into a single feature is to add up the vocabulary character encodings, which we can do with a trivial loop:\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00004-58512c6d-9179-49d8-81e5-9e55db4a0228",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-5967d2bf-bc7a-49a0-bea5-ebe2afe43e25",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5007c99d",
    "execution_start": 1624549949918,
    "execution_millis": 88118086,
    "deepnote_cell_type": "code"
   },
   "source": "for word in [\"cat\", \"chat\", \"katze\"]:\n    x = [vocab[letter] for letter in word]\n    h = 0\n    for t in range(len(x)):\n        h+=x[t]\n\n    print (f'{word} -> {h}')",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "cat -> 6\nchat -> 9\nkatze -> 17\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "This is not a great solution: First, because it's unclear that 6, 9, and 17 meaningfully distinguish between the three translations of cat. More importantly, though, this encoding is order independent. For example, a-c-t has the same encoding as c-a-t:\n\nA simple way to make the encoding order dependent is to multiply previous h values by, say, 2. This performs a scale and add operation very much like what we'd find in a hash function. Scalar 2 is not some magic numberâ€”I just chose it randomly, but it's a value we could learn if we created and optimized a loss function.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00006-f1a0fae4-4317-4914-a549-db5fa5c6aedf",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-be43adfb-c41d-4ddd-83e1-cc75d0f090e2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ba904fa6",
    "execution_start": 1624549949970,
    "execution_millis": 8,
    "deepnote_cell_type": "code"
   },
   "source": "for word in [\"cat\", \"act\"]:\n    x = [vocab[letter] for letter in word]\n    h = 0\n    print (x)\n    for t in range(len(x)):\n        print (f'h_t-1: {h}, x_t: {x[t]}')\n        h = 2*h + x[t]\n        print (f'h_t = 2.h_t-1 + x_t = {h}')\n    \n    print (f'{word} -> {h}')",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "[1, 0, 5]\nh_t-1: 0, x_t: 1\nh_t = 2.h_t-1 + x_t = 1\nh_t-1: 1, x_t: 0\nh_t = 2.h_t-1 + x_t = 2\nh_t-1: 2, x_t: 5\nh_t = 2.h_t-1 + x_t = 9\ncat -> 9\n[0, 1, 5]\nh_t-1: 0, x_t: 0\nh_t = 2.h_t-1 + x_t = 0\nh_t-1: 0, x_t: 1\nh_t = 2.h_t-1 + x_t = 1\nh_t-1: 1, x_t: 5\nh_t = 2.h_t-1 + x_t = 7\nact -> 7\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "That inner loop is equivalent to the following recurrence relation:\n\n![](https://explained.ai/rnn/images/blkeqn-3048294F838BEC17B89CE0457E8359E8.svg)\n\nThe recurrence just says that the value of h at iteration t is twice its previous value plus the encoding of the t_th  character; t moves from 1 to m for m characters in the word. For x = cat we get three values of h beyond our initial value:\n\n![](https://explained.ai/rnn/images/blkeqn-33E117DC61B77D31604D90FAB16799E3.svg)\n\n\n<img src = \"https://i.imgur.com/njh0cOD.jpeg\" width=\"700\" height=\"600\"/>\n\nThe key take away here is that, despite having a constant multiplier of 2, each character encoding is multiplied by a different number, depending on its position in the sequence: x_t is multiplied by 2^(m-t) where m is the no of letters. This is why cat and act get different encodings. We've solved the order dependency issue, but still we are representing every word by a single integer, it's unlikely that a single integer will ever contain enough information to meaningfully represent a natural language word.\n\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00008-68916965-ec06-4588-87b7-7c53225c220b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Aggregating character vectors to encode words\n\nAs we discussed at the beginning, our goal is really to represent entire words by vectors in some higher dimensional space, not integers, which are 1D vectors. We don't have a proper encoding vector for a word, but we can easily get the one-hot vectors for individual characters. So, we just have to find a way to convert these character one-hot vectors to a suitable encoding for an entire word. The simplest way to combine the character vectors is to merge them or, equivalently, add them together into a bag of words (BOW), or bag of characters in this case:\n\nSo if the encoding for c -> [0 1 0 0 0 0 0] a -> [1 0 0 0 0 0 0 ] t -> [0 0 0 0 0 1 0]\nthe encoding for cat -> [0 1 0 0 0 0 0] + [1 0 0 0 0 0 0 ] + [0 0 0 0 0 1 0] -> [1 1 0 0 0 1 0]\n\n![](https://explained.ai/rnn/images/cat-onehot.svg)",
   "metadata": {
    "tags": [],
    "cell_id": "00009-13568037-60e2-4890-9470-edc5c8e22561",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "import torch",
   "metadata": {
    "tags": [],
    "cell_id": "00010-4d357932-fc8d-4dbc-b949-bf9a84818bb9",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dd32fe6c",
    "execution_start": 1624550427403,
    "execution_millis": 1901,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "vocab",
   "metadata": {
    "tags": [],
    "cell_id": "00011-1b665eb3-d642-491d-a680-587d70a45b0b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fe2ed1ce",
    "execution_start": 1624550779297,
    "execution_millis": 15,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 11,
     "data": {
      "text/plain": "{'a': 0, 'c': 1, 'e': 2, 'h': 3, 'k': 4, 't': 5, 'z': 6}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "def onehot(character):\n    one_hot_vector = torch.zeros(len(vocab), 1)\n    one_hot_vector[vocab[character]] = 1\n    return one_hot_vector\n\nonehot('t')",
   "metadata": {
    "tags": [],
    "cell_id": "00011-c80aa9ac-81ba-480d-8cbe-398004a0e0c4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e01ea4c1",
    "execution_start": 1624550767718,
    "execution_millis": 4,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 10,
     "data": {
      "text/plain": "tensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.]])"
     },
     "metadata": {}
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-8ea96f92-7b88-434f-b72e-0959a7cf6ff1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "70f294c9",
    "execution_start": 1624550716786,
    "execution_millis": 11,
    "deepnote_cell_type": "code"
   },
   "source": "### BOW implementation in code\nfor x in [\"cat\", \"chat\", \"katze\"]:\n    h = torch.zeros(len(vocab), 1)\n    for t in range(len(x)):\n        h = h + onehot(x[t])\n\n    print (f'{x} -> {h}')",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "cat -> tensor([[1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.]])\nchat -> tensor([[1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.]])\nkatze -> tensor([[1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.]])\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "But, BOW vectors do not encode information about the order of the characters and the summation of a bunch of one-hot vectors isn't much more meaningful than the integer representation.\n\nTo combine the character vectors in an order-dependent way, we can simply apply the same trick we did before to scale the previous value of h. The only difference between this and the previous version is that we use character vectors not character values and we initialize h to the zero vector rather than 0:\n\n![](https://explained.ai/rnn/images/cat-onehot-ordered.svg)",
   "metadata": {
    "tags": [],
    "cell_id": "00014-f82046b7-bf76-400d-ba83-31bd09e51ba5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "for x in [\"cat\", \"chat\", \"katze\"]:\n    h = torch.zeros(len(vocab), 1)\n    for t in range(len(x)):\n        h = 2 * h + onehot(x[t])\n\n    print (f'{x} -> {h}')",
   "metadata": {
    "tags": [],
    "cell_id": "00014-9034cdc5-82df-46ff-9a3d-45a86de309bd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f8c26f8c",
    "execution_start": 1624551731839,
    "execution_millis": 11,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cat -> tensor([[2.],\n        [4.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.]])\nchat -> tensor([[2.],\n        [8.],\n        [0.],\n        [4.],\n        [0.],\n        [1.],\n        [0.]])\nkatze -> tensor([[ 8.],\n        [ 0.],\n        [ 1.],\n        [ 0.],\n        [16.],\n        [ 4.],\n        [ 2.]])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "Turning the crank three times yields h3 as our encoding for cat. Again, multiplying by 2 is nonsense, but the bigger issue is that multiplying by any single scalar is unlikely create a word encoding meaningful enough to distinguish these words properly.\n\nTo make a more sophisticated model, we need to multiply the h vector by a matrix W (@ is the matrix multiply operator), which represents many more model parameters than a lonely scalar value. There are multiple useful interpretations of matrix-vector multiplication, but it makes the most sense in this case to think of matrix-vector multiplication as transforming a vector into a new space (possibly with different dimensionality). Let's start with the identity matrix as the transformation matrix to see what it looks like:\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00016-79b2404e-ef6a-466b-9303-3c1c40789377",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "nfeatures = len(vocab)\nW = torch.eye(nfeatures, nfeatures) ### identity matrix of shape nfeatures, nfeatures\nW",
   "metadata": {
    "tags": [],
    "cell_id": "00016-5ed55632-0477-4628-8930-151b493092bb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "692aa355",
    "execution_start": 1624552053833,
    "execution_millis": 7,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 15,
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 1.]])"
     },
     "metadata": {}
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "for x in [\"cat\", \"chat\", \"katze\"]:\n    h = torch.zeros(nfeatures, 1)\n    for t in range(len(x)):\n        h = W@h + onehot(x[t])  ### W: nxn h: nx1 onehot: nx1\n\n    print (f'{x} -> {h}')",
   "metadata": {
    "tags": [],
    "cell_id": "00018-2b0a4c5e-78fb-4fd8-900c-6b70e11777d8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "60b6a8b4",
    "execution_start": 1624552143730,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "cat -> tensor([[1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.]])\nchat -> tensor([[1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.]])\nkatze -> tensor([[1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.]])\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "Because multiplying a vector by the identity matrix leaves the vector as-is, the update equation reduces to `h = h + onehot(c)`, which leads to a BOW word encoding because it just sums up the character one-hot vectors.\n\nSo basically we need a smart way of constructing matrix W st h captures useful info about each word",
   "metadata": {
    "tags": [],
    "cell_id": "00019-acff7f1a-f1be-474f-b590-9188b72390cb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00019-67736383-fc6b-4b8e-99ad-f06cf821c868",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=306a1d85-aa04-4db6-b565-92149392d58c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "d628ee36-1bd0-42fd-a792-26018c12ad66",
  "deepnote_execution_queue": []
 }
}