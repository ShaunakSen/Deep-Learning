{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Fundamentals-DEEP LEARNING WITH PYTORCH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/DL_Fundamentals_DEEP_LEARNING_WITH_PYTORCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epEE41dhK9vM",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n",
        "\n",
        "[link](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)\n",
        "\n",
        "Deep learning consists of composing linearities with non-linearities in clever ways. The introduction of non-linearities allows for powerful models. In this section, we will play with these core components, make up an objective function, and see how the model is trained.\n",
        "\n",
        "One of the core workhorses of deep learning is the affine map, which is a function f(x) where\n",
        "\n",
        "f(x)=Ax+b\n",
        "for a matrix A and vectors x,b. The parameters to be learned here are A and b. Often, b is refered to as the bias term.\n",
        "\n",
        "PyTorch and most other deep learning frameworks do things a little differently than traditional linear algebra. It maps the rows of the input instead of the columns. That is, the i’th row of the output below is the mapping of the i’th row of the input under A, plus the bias term. Look at the example below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRY62LuNK3DX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76fada60-df44-496a-b9d8-7327f92daf85"
      },
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fec9afd1630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIHKk_9rLKhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "edfc21f7-8b1d-4d79-94a9-af59b170297a"
      },
      "source": [
        "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
        "\n",
        "# basically lin is nothing but a matrix of wts which will be updated by grad descent\n",
        "\n",
        "print (lin.weight.shape)\n",
        "\n",
        "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
        "\n",
        "data = torch.randn(2, 5)\n",
        "\n",
        "# op: 2x5 5x3 -> 2x3\n",
        "\n",
        "print(lin(data))\n",
        "\n",
        "print (lin(data).shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n",
            "tensor([[-0.0120,  0.3745, -0.3695],\n",
            "        [ 0.0722,  0.7715, -0.4374]], grad_fn=<AddmmBackward>)\n",
            "torch.Size([2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mb4x7RINNFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9b35d1a5-80c8-4cfa-c9c0-ea85446eae69"
      },
      "source": [
        "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
        "# Note that non-linearites typically don't have parameters like affine maps do.\n",
        "# That is, they don't have weights that are updated during training.\n",
        "data = torch.randn(2, 2)\n",
        "print(data)\n",
        "print(F.relu(data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2912, -0.8317],\n",
            "        [-0.5525,  0.6355]])\n",
            "tensor([[0.2912, 0.0000],\n",
            "        [0.0000, 0.6355]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qZBdhY9NIFQ",
        "colab_type": "text"
      },
      "source": [
        "The function Softmax(x) is also just a non-linearity, but it is special in that it usually is the last operation done in a network. This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. Let x be a vector of real numbers (positive, negative, whatever, there are no constraints). Then the i’th component of Softmax(x) is\n",
        "\n",
        "exp(xi)∑jexp(xj)\n",
        "\n",
        "It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is 1.\n",
        "\n",
        "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm1lf7rsNXL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2d724c13-69de-4a96-e99f-e60eb1e471b9"
      },
      "source": [
        "# Softmax is also in torch.nn.functional\n",
        "data = torch.randn(5)\n",
        "print(data)\n",
        "\n",
        "print(F.softmax(data, dim=0))\n",
        "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
        "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-2.5667, -1.4303,  0.5009,  0.5438, -0.4057])\n",
            "tensor([0.0176, 0.0549, 0.3789, 0.3955, 0.1530])\n",
            "tensor(1.)\n",
            "tensor([-4.0381, -2.9017, -0.9705, -0.9276, -1.8771])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzQxQwPrOZbi",
        "colab_type": "text"
      },
      "source": [
        "### Creating Network Components in PyTorch\n",
        "\n"
      ]
    }
  ]
}