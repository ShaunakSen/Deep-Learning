{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Fundamentals-DEEP LEARNING WITH PYTORCH.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/DL_Fundamentals_DEEP_LEARNING_WITH_PYTORCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epEE41dhK9vM",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n",
        "\n",
        "[link](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)\n",
        "\n",
        "Deep learning consists of composing linearities with non-linearities in clever ways. The introduction of non-linearities allows for powerful models. In this section, we will play with these core components, make up an objective function, and see how the model is trained.\n",
        "\n",
        "One of the core workhorses of deep learning is the affine map, which is a function f(x) where\n",
        "\n",
        "f(x)=Ax+b\n",
        "for a matrix A and vectors x,b. The parameters to be learned here are A and b. Often, b is refered to as the bias term.\n",
        "\n",
        "PyTorch and most other deep learning frameworks do things a little differently than traditional linear algebra. It maps the rows of the input instead of the columns. That is, the i’th row of the output below is the mapping of the i’th row of the input under A, plus the bias term. Look at the example below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRY62LuNK3DX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76fada60-df44-496a-b9d8-7327f92daf85"
      },
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fec9afd1630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIHKk_9rLKhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "edfc21f7-8b1d-4d79-94a9-af59b170297a"
      },
      "source": [
        "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
        "\n",
        "# basically lin is nothing but a matrix of wts which will be updated by grad descent\n",
        "\n",
        "print (lin.weight.shape)\n",
        "\n",
        "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
        "\n",
        "data = torch.randn(2, 5)\n",
        "\n",
        "# op: 2x5 5x3 -> 2x3\n",
        "\n",
        "print(lin(data))\n",
        "\n",
        "print (lin(data).shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n",
            "tensor([[-0.0120,  0.3745, -0.3695],\n",
            "        [ 0.0722,  0.7715, -0.4374]], grad_fn=<AddmmBackward>)\n",
            "torch.Size([2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mb4x7RINNFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9b35d1a5-80c8-4cfa-c9c0-ea85446eae69"
      },
      "source": [
        "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
        "# Note that non-linearites typically don't have parameters like affine maps do.\n",
        "# That is, they don't have weights that are updated during training.\n",
        "data = torch.randn(2, 2)\n",
        "print(data)\n",
        "print(F.relu(data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2912, -0.8317],\n",
            "        [-0.5525,  0.6355]])\n",
            "tensor([[0.2912, 0.0000],\n",
            "        [0.0000, 0.6355]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qZBdhY9NIFQ",
        "colab_type": "text"
      },
      "source": [
        "The function Softmax(x) is also just a non-linearity, but it is special in that it usually is the last operation done in a network. This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. Let x be a vector of real numbers (positive, negative, whatever, there are no constraints). Then the i’th component of Softmax(x) is\n",
        "\n",
        "exp(xi)∑jexp(xj)\n",
        "\n",
        "It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is 1.\n",
        "\n",
        "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm1lf7rsNXL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2d724c13-69de-4a96-e99f-e60eb1e471b9"
      },
      "source": [
        "# Softmax is also in torch.nn.functional\n",
        "data = torch.randn(5)\n",
        "print(data)\n",
        "\n",
        "print(F.softmax(data, dim=0))\n",
        "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
        "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-2.5667, -1.4303,  0.5009,  0.5438, -0.4057])\n",
            "tensor([0.0176, 0.0549, 0.3789, 0.3955, 0.1530])\n",
            "tensor(1.)\n",
            "tensor([-4.0381, -2.9017, -0.9705, -0.9276, -1.8771])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzQxQwPrOZbi",
        "colab_type": "text"
      },
      "source": [
        "### Creating Network Components in PyTorch\n",
        "\n",
        "Before we move on to our focus on NLP, lets do an annotated example of building a network in PyTorch using only affine maps and non-linearities. We will also see how to compute a loss function, using PyTorch’s built in negative log likelihood, and update parameters by backpropagation.\n",
        "\n",
        "All network components should inherit from nn.Module and override the forward() method. That is about it, as far as the boilerplate is concerned. Inheriting from nn.Module provides functionality to your component. For example, it makes it keep track of its trainable parameters, you can swap it between CPU and GPU with the .to(device) method, where device can be a CPU device torch.device(\"cpu\") or CUDA device torch.device(\"cuda:0\").\n",
        "\n",
        "Let’s write an annotated example of a network that takes in a sparse bag-of-words representation and outputs a probability distribution over two labels: “English” and “Spanish”. This model is just logistic regression.\n",
        "\n",
        "### Example: Logistic Regression Bag-of-Words classifier\n",
        "\n",
        "\n",
        "Our model will map a sparse BoW representation to log probabilities over labels. We assign each word in the vocab an index. For example, say our entire vocab is two words “hello” and “world”, with indices 0 and 1 respectively. The BoW vector for the sentence “hello hello hello hello” is\n",
        "\n",
        "[4,0]\n",
        "For “hello world world hello”, it is\n",
        "\n",
        "[2,2]\n",
        "etc. In general, it is\n",
        "\n",
        "[Count(hello),Count(world)]\n",
        "Denote this BOW vector as x. The output of our network is:\n",
        "\n",
        "logSoftmax(Ax+b)\n",
        "That is, we pass the input through an affine map and then do log softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPJTzFnjO802",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c40e6a3f-8f57-409c-f3c4-77005d1cee23"
      },
      "source": [
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "print (data[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkGuEfpP3zP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c07a5963-8947-482f-f99e-3c52947ffb44"
      },
      "source": [
        "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
        "# index into the Bag of words vector\n",
        "\n",
        "\n",
        "word_to_ix = {}\n",
        "for sent, _ in data + test_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8nvc1vwQDu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc9ad754-f78b-4f05-8b2f-01d5ffb82ba6"
      },
      "source": [
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "# print (VOCAB_SIZE)\n",
        "\n",
        "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
        "  \n",
        "  def __init__(self, num_labels, vocab_size):\n",
        "    # calls the init function of nn.Module.  Dont get confused by syntax,\n",
        "    # just always do it in an nn.Module\n",
        "    super(BoWClassifier, self).__init__()\n",
        "\n",
        "    # Define the parameters that you will need.  In this case, we need A and b,\n",
        "    # the parameters of the affine mapping.\n",
        "    # Torch defines nn.Linear(), which provides the affine map.\n",
        "    # Make sure you understand why the input dimension is vocab_size\n",
        "    # and the output is num_labels!\n",
        "\n",
        "    self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "    # linear is matrix of num_labelsxvocab_size\n",
        "    # ip is matrx of 1xvocab_size\n",
        "    # op will be tensor of num_labels\n",
        "    \n",
        "    # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
        "    # to worry about that here\n",
        "    \n",
        "  def forward(self, bow_vec):\n",
        "    # Pass the input through the linear layer,\n",
        "    # then pass that through log_softmax.\n",
        "    # Many non-linearities and other functions are in torch.nn.functional\n",
        "    return F.log_softmax(self.linear(bow_vec), dim=1)\n",
        "    \n",
        "\n",
        "    \n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "  vec = torch.zeros(len(word_to_ix))\n",
        "  for word in sentence:\n",
        "    vec[word_to_ix[word]] += 1\n",
        "  \n",
        "  # the size of op vec will be [1, vocab_size]\n",
        "  return vec.view(1, -1)\n",
        "\n",
        "def make_target(label, label_to_idx):\n",
        "  return torch.LongTensor([label_to_idx[label]])\n",
        "\n",
        "\n",
        "print (make_bow_vector(data[0][0], word_to_ix).shape)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 26])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjecs8xNbFoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "57d2c92f-d60b-4f6b-ccf8-5d0202f4d518"
      },
      "source": [
        "\n",
        "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "\n",
        "# the model knows its parameters.  The first output below is A, the second is b.\n",
        "# Whenever you assign a component to a class variable in the __init__ function\n",
        "# of a module, which was done with the line\n",
        "# self.linear = nn.Linear(...)\n",
        "# Then through some Python magic from the PyTorch devs, your module\n",
        "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
        "for param in model.parameters():\n",
        "  # first param will be matrix A\n",
        "  # second will be the biases\n",
        "  # Ax + b\n",
        "  # A: num_labelsxvocab_size x: 1xvocab_size b: num_labels\n",
        "  print(param.shape)\n",
        "  \n",
        "# To run the model, pass in a BoW vector\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "\n",
        "# To run the model, pass in a BoW vector\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "  sample = data[0]\n",
        "  bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
        "  log_probs = model(bow_vector)\n",
        "  print(log_probs)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 26])\n",
            "torch.Size([2])\n",
            "tensor([[-0.6784, -0.7081]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfI8qIibb6Th",
        "colab_type": "text"
      },
      "source": [
        "So F.softmax returns a prob dist. F.log_softmax returns a log of this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLGj0BUyb_mU",
        "colab_type": "text"
      },
      "source": [
        "Which of the above values corresponds to the log probability of ENGLISH, and which to SPANISH? We never defined it, but we need to if we want to train the thing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN4DNuM2b-Cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6iWD71ShhFu",
        "colab_type": "text"
      },
      "source": [
        "So lets train! To do this, we pass instances through to get log probabilities, compute a loss function, compute the gradient of the loss function, and then update the parameters with a gradient step. Loss functions are provided by Torch in the nn package. nn.NLLLoss() is the negative log likelihood loss we want. It also defines optimization functions in torch.optim. Here, we will just use SGD.\n",
        "\n",
        "**Note that the input to NLLLoss is a vector of log probabilities, and a target label. It doesn’t compute the log probabilities for us. This is why the last layer of our network is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log softmax for you.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdeZaN8NhpAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "341d7709-37a9-404d-a3d2-cf2694a726eb"
      },
      "source": [
        "# Run on test data before we train, just to see a before-and-after\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5154, -0.9095]])\n",
            "tensor([[-0.8217, -0.5792]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6EG_ahdh6qA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "ed8dfde6-decf-40a8-fe3e-88e4a9515652"
      },
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
        "\n",
        "for instance, label in data:\n",
        "  # Step 1. Remember that PyTorch accumulates gradients.\n",
        "  # We need to clear them out before each instance\n",
        "  # model.zero_grad()\n",
        "  # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "  # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "  # we wrap the integer 0. The loss function then knows that the 0th\n",
        "  # element of the log probabilities is the log probability\n",
        "  # corresponding to SPANISH\n",
        "  bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "  target = make_target(label, label_to_ix)\n",
        "\n",
        "  # Step 3. Run our forward pass.\n",
        "  log_probs = model(bow_vec)\n",
        "\n",
        "  # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "  # calling optimizer.step()\n",
        "  print (log_probs, target)\n",
        "  loss = loss_function(log_probs, target)\n",
        "  print (loss)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1002, -2.3500]], grad_fn=<LogSoftmaxBackward>) tensor([0])\n",
            "tensor(0.1002, grad_fn=<NllLossBackward>)\n",
            "tensor([[-1.3585, -0.2971]], grad_fn=<LogSoftmaxBackward>) tensor([1])\n",
            "tensor(0.2971, grad_fn=<NllLossBackward>)\n",
            "tensor([[-0.3231, -1.2869]], grad_fn=<LogSoftmaxBackward>) tensor([0])\n",
            "tensor(0.3231, grad_fn=<NllLossBackward>)\n",
            "tensor([[-1.1336, -0.3884]], grad_fn=<LogSoftmaxBackward>) tensor([1])\n",
            "tensor(0.3884, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ek4jyj6k7VT",
        "colab_type": "text"
      },
      "source": [
        "What can we decipher from the above op?\n",
        "\n",
        "Note we are getting the log of prob values and log(1) = 0\n",
        "\n",
        "When target is 0 and if we get [-0.1002, -2.3500] as op , op should have been [0, x] (remember we are getting log(prob) values), So loss here is 0-(--0.1002) = 0.1002\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O9q4t1elilL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(100):\n",
        "  for instance, label in data:\n",
        "    # Step 1. Remember that PyTorch accumulates gradients.\n",
        "    # We need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "    # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "    # we wrap the integer 0. The loss function then knows that the 0th\n",
        "    # element of the log probabilities is the log probability\n",
        "    # corresponding to SPANISH\n",
        "    bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "    target = make_target(label, label_to_ix)\n",
        "\n",
        "    # Step 3. Run our forward pass.\n",
        "    log_probs = model(bow_vec)\n",
        "\n",
        "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "    # calling optimizer.step()\n",
        "    loss = loss_function(log_probs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NuN5dhelsYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e04ced19-2312-4025-f9e7-54eae1ba8ff2"
      },
      "source": [
        "with torch.no_grad():\n",
        "  for instance, label in test_data:\n",
        "    print (instance, label)\n",
        "    bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(log_probs)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Yo', 'creo', 'que', 'si'] SPANISH\n",
            "tensor([[-0.1424, -2.0193]])\n",
            "['it', 'is', 'lost', 'on', 'me'] ENGLISH\n",
            "tensor([[-2.6018, -0.0770]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6PraeRxl6sg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "524bceb4-d6ac-44fa-dfce-a6081d0452c7"
      },
      "source": [
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.2901, -0.3370], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}