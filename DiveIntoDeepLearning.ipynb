{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiveIntoDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXWbmGf4FeGjn/D20OUUSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/DiveIntoDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfDmkz1ChPaM",
        "outputId": "9817d4e5-c714-471d-ef29-1d516dc2d0f6"
      },
      "source": [
        "!pip install -U d2l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting d2l\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/1f/13de7e8cafaba15739caee0596032412aaf51a22726649b317bdb53c4f9a/d2l-0.16.2-py3-none-any.whl (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 14.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (7.6.3)\n",
            "Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.0.3)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (4.7.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.9.2)\n",
            "Requirement already satisfied, skipping upgrade: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.1.2)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (22.0.3)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->d2l) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->d2l) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l) (0.2.5)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.16.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBJj7CuQ8ev"
      },
      "source": [
        "## Dive Into Deep Learning\n",
        "\n",
        "> Self notes on the book: http://www.d2l.ai/\n",
        "---\n",
        "\n",
        "If you are able to devise a solution to the problem that will work 100% of the time, __do not__ use ML!\n",
        "\n",
        "The supervision comes into play because for choosing the parameters, we (the supervisors) provide the model with a dataset consisting of labeled examples, where each example is matched with the ground-truth label. In probabilistic terms, we typically are interested in estimating the conditional probability of a label given input features. While it is just one among several paradigms within machine learning, supervised learning accounts for the majority of successful applications of machine learning in industry. Partly, that is because many important tasks can be described crisply as estimating the probability of something unknown given a particular set of available data:\n",
        "\n",
        "Lots of practical problems are well-described regression problems. Predicting the rating that a user will assign to a movie can be thought of as a regression problem and if you designed a great algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar Netflix prize. Predicting the length of stay for patients in the hospital is also a regression problem. A good rule of thumb is that any how much? or how many? problem should suggest regression, such as:\n",
        "\n",
        "- How many hours will this surgery take?\n",
        "\n",
        "- How much rainfall will this town have in the next six hours?\n",
        "\n",
        "NOTE: whole number prediction problems are regression ones, not classification\n",
        "\n",
        "## Types of Supervised ML\n",
        "\n",
        "### 1. Regression + 2. Classification - we know\n",
        "\n",
        "### 2.1 Tagging\n",
        "\n",
        "Some classification problems fit neatly into the binary or multiclass classification setups. For example, we could train a normal binary classifier to distinguish cats from dogs. Given the current state of computer vision, we can do this easily, with off-the-shelf tools. Nonetheless, no matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters an image of the Town Musicians of Bremen, a popular German fairy tale featuring four animals in Fig. 1.3.3.\n",
        "\n",
        "![](http://www.d2l.ai/_images/stackedanimals.png)\n",
        "\n",
        "As you can see, there is a cat in Fig. 1.3.3, and a rooster, a dog, and a donkey, with some trees in the background. Depending on what we want to do with our model ultimately, treating this as a binary classification problem might not make a lot of sense. Instead, we might want to give the model the option of saying the image depicts a cat, a dog, a donkey, and a rooster.\n",
        "\n",
        "The problem of learning to predict classes that are not mutually exclusive is called multi-label classification. Auto-tagging problems are typically best described as multi-label classification problems. Think of the tags people might apply to posts on a technical blog, e.g., “machine learning”, “technology”, “gadgets”, “programming languages”, “Linux”, “cloud computing”, “AWS”. A typical article might have 5–10 tags applied because these concepts are correlated. Posts about “cloud computing” are likely to mention “AWS” and posts about “machine learning” could also deal with “programming languages”.\n",
        "\n",
        "We also have to deal with this kind of problem when dealing with the biomedical literature, where correctly tagging articles is important because it allows researchers to do exhaustive reviews of the literature. At the National Library of Medicine, a number of professional annotators go over each article that gets indexed in PubMed to associate it with the relevant terms from MeSH, a collection of roughly 28000 tags. This is a time-consuming process and the annotators typically have a one-year lag between archiving and tagging. Machine learning can be used here to provide provisional tags until each article can have a proper manual review. Indeed, for several years, the BioASQ organization has hosted competitions to do precisely this.\n",
        "\n",
        "### 3. Search\n",
        "\n",
        "Sometimes we do not just want to assign each example to a bucket or to a real value. In the field of information retrieval, we want to impose a ranking on a set of items. Take web search for an example. The goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results is most relevant for a particular user. We really care about the ordering of the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning “A B C D E” and “C A B E D”. Even if the result set is the same, the ordering within the set matters.\n",
        "\n",
        "\n",
        "One possible solution to this problem is to first assign to every element in the set a corresponding relevance score and then to retrieve the top-rated elements. PageRank, the original secret sauce behind the Google search engine was an early example of such a scoring system but it was peculiar in that it did not depend on the actual query. Here they relied on a simple relevance filter to identify the set of relevant items and then on PageRank to order those results that contained the query term. Nowadays, search engines use machine learning and behavioral models to obtain query-dependent relevance scores. There are entire academic conferences devoted to this subject.\n",
        "\n",
        "### 4. Recommender Systems\n",
        "\n",
        "In the simplest formulations, these systems are trained to estimate some score, such as an estimated rating or the probability of purchase, given a user and an item.\n",
        "\n",
        "Given such a model, for any given user, we could retrieve the set of objects with the largest scores, which could then be recommended to the user. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores\n",
        "\n",
        "Despite their tremendous economic value, recommendation systems naively built on top of predictive models suffer some serious conceptual flaws. To start, we only observe censored feedback: users preferentially rate movies that they feel strongly about. For example, on a five-point scale, you might notice that items receive many five and one star ratings but that there are conspicuously few three-star ratings. Moreover, current purchase habits are often a result of the recommendation algorithm currently in place, but learning algorithms do not always take this detail into account. Thus it is possible for feedback loops to form where a recommender system preferentially pushes an item that is then taken to be better (due to greater purchases) and in turn is recommended even more frequently. Many of these problems about how to deal with censoring, incentives, and feedback loops, are important open research questions.\n",
        "\n",
        "### Sequence learning\n",
        "\n",
        "So far, we have looked at problems where we have some fixed number of inputs and produce a fixed number of outputs. For example, we considered predicting house prices from a fixed set of features: square footage, number of bedrooms, number of bathrooms, walking time to downtown. We also discussed mapping from an image (of fixed dimension) to the predicted probabilities that it belongs to each of a fixed number of classes, or taking a user ID and a product ID, and predicting a star rating. In these cases, once we feed our fixed-length input into the model to generate an output, the model immediately forgets what it just saw.\n",
        "\n",
        "This might be fine if our inputs truly all have the same dimensions and if successive inputs truly have nothing to do with each other. But how would we deal with video snippets? In this case, each snippet might consist of a different number of frames. And our guess of what is going on in each frame might be much stronger if we take into account the previous or succeeding frames. Same goes for language. One popular deep learning problem is machine translation: the task of ingesting sentences in some source language and predicting their translation in another language.\n",
        "\n",
        "These problems also occur in medicine. We might want a model to monitor patients in the intensive care unit and to fire off alerts if their risk of death in the next 24 hours exceeds some threshold. We definitely would not want this model to throw away everything it knows about the patient history each hour and just make its predictions based on the most recent measurements.\n",
        "\n",
        "These problems are among the most exciting applications of machine learning and they are instances of sequence learning. They require a model to either ingest sequences of inputs or to emit sequences of outputs (or both). Specifically, sequence to sequence learning considers problems where input and output are both variable-length sequences, such as machine translation and transcribing text from the spoken speech. While it is impossible to consider all types of sequence transformations, the following special cases are worth mentioning.\n",
        "\n",
        "__Tagging and Parsing__. This involves annotating a text sequence with attributes. In other words, the number of inputs and outputs is essentially the same. For instance, we might want to know where the verbs and subjects are. Alternatively, we might want to know which words are the named entities.\n",
        "\n",
        "__Automatic Speech Recognition__. With speech recognition, the input sequence is an audio recording of a speaker (shown in Fig. 1.3.5), and the output is the textual transcript of what the speaker said. The challenge is that there are many more audio frames (sound is typically sampled at 8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text, since thousands of samples may correspond to a single spoken word. These are sequence to sequence learning problems where the output is much shorter than the input.\n",
        "\n",
        "__Text to Speech__. This is the inverse of automatic speech recognition. In other words, the input is text and the output is an audio file. In this case, the output is much longer than the input. While it is easy for humans to recognize a bad audio file, this is not quite so trivial for computers.\n",
        "\n",
        "__Machine Translation__. Unlike the case of speech recognition, where corresponding inputs and outputs occur in the same order (after alignment), in machine translation, order inversion can be vital. In other words, while we are still converting one sequence into another, neither the number of inputs and outputs nor the order of corresponding data examples are assumed to be the same. Consider the following illustrative example of the peculiar tendency of Germans to place the verbs at the end of sentences.\n",
        "\n",
        "```\n",
        "German:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?\n",
        "English:          Did you already check out this excellent tutorial?\n",
        "Wrong alignment:  Did you yourself already this excellent tutorial looked-at?\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEieofVh0SKV"
      },
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "In a completely opposite way, it could be frustrating to work for a boss who has no idea what they want you to do. However, if you plan to be a data scientist, you had better get used to it. The boss might just hand you a giant dump of data and tell you to do some data science with it! This sounds vague because it is. We call this class of problems unsupervised learning, and the type and number of questions we could ask is limited only by our creativity. We will address unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a few of the following questions you might ask.\n",
        "\n",
        "- Can we find a small number of prototypes that accurately summarize the data? Given a set of photos, can we group them into landscape photos, pictures of dogs, babies, cats, and mountain peaks? Likewise, given a collection of users’ browsing activities, can we group them into users with similar behavior? This problem is typically known as clustering.\n",
        "\n",
        "- Can we find a small number of parameters that accurately capture the relevant properties of the data? The trajectories of a ball are quite well described by velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as subspace estimation. If the dependence is linear, it is called principal component analysis.\n",
        "\n",
        "- Is there a representation of (arbitrarily structured) objects in Euclidean space such that symbolic properties can be well matched? This can be used to describe entities and their relations, such as “Rome”  −  “Italy”  +  “France”  =  “Paris”.\n",
        "\n",
        "- Is there a description of the root causes of much of the data that we observe? For instance, if we have demographic data about house prices, pollution, crime, location, education, and salaries, can we discover how they are related simply based on empirical data? The fields concerned with causality and probabilistic graphical models address this problem.\n",
        "\n",
        "- Another important and exciting recent development in unsupervised learning is the advent of generative adversarial networks. These give us a procedural way to synthesize data, even complicated structured data like images and audio. The underlying statistical mechanisms are tests to check whether real and fake data are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1GazwOMNQg"
      },
      "source": [
        "## Basic Data Manipulation\n",
        "\n",
        "To start, we can use arange to create a row vector x containing the first 12 integers starting with 0, though they are created as floats by default. Each of the values in a tensor is called an element of the tensor. For instance, there are 12 elements in the tensor x. Unless otherwise specified, a new tensor will be stored in main memory and designated for CPU-based computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMhTF98VQ5Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed33158a-1fd7-4701-ec28-cf11341d7507"
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "print (tf.__version__)\n",
        "print (torch.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzRRI4QMh0P",
        "outputId": "f3f64297-a500-4e7f-ecd0-02341c063953"
      },
      "source": [
        "x = torch.arange(start=0, end=12, step=1)\n",
        "\n",
        "print (x)\n",
        "\n",
        "print (x.shape)\n",
        "\n",
        "print (x.numel())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "torch.Size([12])\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB60MjLVMm7r",
        "outputId": "cd7ab145-eb04-4765-bb40-c8f4761c32c2"
      },
      "source": [
        "X = x.reshape(3, 4)\n",
        "print (X)\n",
        "\n",
        "## automatically infer the first dimension\n",
        "X = x.reshape(-1, 4)\n",
        "print (X) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJoUaOTzN6VC"
      },
      "source": [
        "Typically, we will want our matrices initialized either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. We can create a tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBz0Q1kCM1tA",
        "outputId": "f679700d-288e-4640-af62-6275bf515167"
      },
      "source": [
        "print (torch.zeros((2,3,4)))\n",
        "\n",
        "## Similarly, we can create tensors with each element set to 1 as follows:\n",
        "\n",
        "print (torch.ones((2,3,4)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCRy-5_BN_2w",
        "outputId": "98b93934-9c92-44de-c114-1dbb7384844e"
      },
      "source": [
        "print (torch.randn((2,3,4)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.0626,  1.0005,  3.6282, -0.2103],\n",
            "         [-0.2923, -0.3126, -1.1658,  0.1266],\n",
            "         [ 1.2817,  1.7867, -1.2784, -0.5347]],\n",
            "\n",
            "        [[ 0.4182,  0.1116, -0.2413, -0.2941],\n",
            "         [ 0.0742,  0.6742,  0.1664, -0.8610],\n",
            "         [ 1.1863, -0.4719, -0.3933,  1.3284]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7r_ePh7PBXF"
      },
      "source": [
        "We can also specify the exact values for each element in the desired tensor by supplying a Python list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis 0, and the inner list to axis 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHfwNTrOuZN",
        "outputId": "c263c803-9531-4141-ffa9-e82b795cdcf4"
      },
      "source": [
        "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqgH550PPD2Z",
        "outputId": "efc01a22-7530-4dc0-dd7e-75e3fbd4cab8"
      },
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\n",
        "y = torch.tensor([2, 2, 2, 2])\n",
        "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgfod02MTRCA",
        "outputId": "f0c396a2-8da7-4e28-e7cd-cbefedaae4ec"
      },
      "source": [
        "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "\n",
        "print (X)\n",
        "\n",
        "print (Y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([[2., 1., 4., 3.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [4., 3., 2., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFvOeDmlTtAM",
        "outputId": "03b1149b-9d08-4f3c-bef7-38494f73e788"
      },
      "source": [
        "torch.cat(tensors=[X, Y], dim=0)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.],\n",
              "        [ 2.,  1.,  4.,  3.],\n",
              "        [ 1.,  2.,  3.,  4.],\n",
              "        [ 4.,  3.,  2.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7svhcO4T1MX",
        "outputId": "7168fdec-50b9-431a-e0c4-ae2fbd51eaf8"
      },
      "source": [
        "torch.cat((X, Y), dim=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9SZccD-eR3u"
      },
      "source": [
        "### Broadcasting Mechanism\n",
        "\n",
        "In the above section, we saw how to perform elementwise operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays.\n",
        "\n",
        "In most cases, we broadcast along an axis where an array initially only has length 1, such as in the following example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJQSaGPtUH_y",
        "outputId": "b8d48eb0-466b-401b-efb9-621b288b6321"
      },
      "source": [
        "a = torch.arange(3).reshape((3,1))\n",
        "print (a)\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "print (b)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]])\n",
            "tensor([[0, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V-UY17zVelhu",
        "outputId": "c7a7f5cf-7be7-42a8-c955-bd348ae4f88d"
      },
      "source": [
        "a+b\n",
        "\n",
        "\"\"\"\n",
        "The broadcasting happens as:\n",
        "a -> [[0, 0]\n",
        "     [1, 1]\n",
        "     [2, 2]]\n",
        "b -> [[0, 1]\n",
        "     [0, 1]\n",
        "     [0, 1]]   \n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe broadcasting happens as:\\na -> [[0, 0]\\n     [1, 1]\\n     [2, 2]]\\nb -> [[0, 1]\\n     [0, 1]\\n     [0, 1]]   \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7_7I9Rfc8A"
      },
      "source": [
        "### Indexing and slicing\n",
        "\n",
        "Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMfORByvew0m",
        "outputId": "bdc32703-f012-439e-d878-1effee374dd8"
      },
      "source": [
        "print (X)\n",
        "\n",
        "print (X[-1])\n",
        "\n",
        "print (X[1:3])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([ 8.,  9., 10., 11.])\n",
            "tensor([[ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96XTzZFif8Nq",
        "outputId": "42379fdf-980a-4954-aee2-134b864d093f"
      },
      "source": [
        "X[1, 2] = 9\n",
        "print (X)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  9.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M4bvm_0gHc1",
        "outputId": "a2a5954f-f575-463b-9a1d-ea293f69c00a"
      },
      "source": [
        "X[0:2, :] = 12\n",
        "\n",
        "print (X)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[12., 12., 12., 12.],\n",
            "        [12., 12., 12., 12.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXF-aQbm8hKz"
      },
      "source": [
        "### Saving memeory\n",
        "\n",
        "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Python’s id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v0YsUS5gO_1",
        "outputId": "9571f0a9-514d-4359-c77b-eae54ff745d7"
      },
      "source": [
        "before = id(Y)\n",
        "print (before)\n",
        "Y=Y+X\n",
        "print (id(Y))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140373009755264\n",
            "140373002487040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WNjxT5T8_Cs"
      },
      "source": [
        "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters.\n",
        "\n",
        "Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., Y[:] = <expression>. To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJiDCWO8t6a",
        "outputId": "0c335590-4d22-43a3-b54a-7d5a16443db9"
      },
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print (Z)\n",
        "print('id(Z):', id(Z))\n",
        "Z[:] = X + Y\n",
        "print('id(Z):', id(Z))\n",
        "Z = X+Y\n",
        "print('id(Z):', id(Z))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "id(Z): 140373002514672\n",
            "id(Z): 140373002514672\n",
            "id(Z): 140373002516672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2vm-2tI_jpC"
      },
      "source": [
        "If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X += Y to reduce the memory overhead of the operation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGjqrm7F_R2s",
        "outputId": "e1b4eec9-1744-4cce-f190-4ffc7e5159d9"
      },
      "source": [
        "before = id(X)\n",
        "X = X + Y\n",
        "id(X) == before"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjFb86mE_lJM",
        "outputId": "6303edc1-d06f-4ea5-b71f-6eadbdd2278e"
      },
      "source": [
        "before = id(X)\n",
        "X +=  Y\n",
        "id(X) == before"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTKxYwGkRg1G"
      },
      "source": [
        "## Basic Linear Algebra\n",
        "\n",
        "### Dot Product\n",
        "\n",
        "one of the most fundamental operations is the dot product. Given two vectors  x,y their dot product\n",
        "\n",
        "$\\mathbf{x}^\\top \\mathbf{y}$ or $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$\n",
        "is a sum over the products of the elements at the same position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pw3MafZ_sMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59319978-5629-403c-9a8d-635596e2b8de"
      },
      "source": [
        "print (x)\n",
        "y = torch.ones(4, dtype=torch.float32)\n",
        "\n",
        "print (x.dot(y))\n",
        "\n",
        "### Note that we can express the dot product of two vectors equivalently by performing an elementwise multiplication and then a sum:\n",
        "\n",
        "print (torch.sum(x*y))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 4., 8.])\n",
            "tensor(15.)\n",
            "tensor(15.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxcS0rFrSZlU"
      },
      "source": [
        "Dot products are useful in a wide range of contexts. For example, given some set of values, and a set of wts the dot product bw these vectors give us a weighted sum. When the wts are non-negative and sum upto 1 the dot prod gives us the weighted avg\n",
        "\n",
        "After normalizing two vectors to have the unit length, the dot products express the cosine of the angle between them. We will formally introduce this notion of length later in this section.\n",
        "\n",
        "### Matrix-Vector Products\n",
        "\n",
        "Now that we know how to calculate dot products, we can begin to understand matrix-vector products. Recall the matrix  $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and the vector $\\mathbf{x} \\in \\mathbb{R}^n$ \n",
        "\n",
        "$\\begin{split}\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_m \\\\\n",
        "\\end{bmatrix},\\end{split}$\n",
        "\n",
        "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the ith row of the matrix A. The matrix-vector product Ax is simply a column vector of length  m , whose  ith  element is the dot product  $\\mathbf{a}^\\top_i \\mathbf{x}$\n",
        "\n",
        "$\\begin{split}\\mathbf{A}\\mathbf{x}\n",
        "= \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_m \\\\\n",
        "\\end{bmatrix}\\mathbf{x}\n",
        "= \\begin{bmatrix}\n",
        " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
        " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
        "\\vdots\\\\\n",
        " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
        "\\end{bmatrix}.\\end{split}$\n",
        "\n",
        "We can think of multiplication by a matrix  A as a transformation that projects vectors (here x) from  $\\mathbb{R}^n$ to $\\mathbb{R}^m$. These transformations turn out to be remarkably useful. For example, we can represent rotations as multiplications by a square matrix. As we will see in subsequent chapters, we can also use matrix-vector products to describe the most intensive calculations required when computing each layer in a neural network given the values of the previous layer.\n",
        "\n",
        "Expressing matrix-vector products in code with tensors, we use the same dot function as for dot products. When we call np.dot(A, x) with a matrix A and a vector x, the matrix-vector product is performed. Note that the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFxOAxynR6YK",
        "outputId": "6131447c-7941-418a-a417-aec6e5bf22fb"
      },
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "\n",
        "\n",
        "print (A.shape, x.shape)\n",
        "\n",
        "print (A)\n",
        "\n",
        "print (x)\n",
        "\n",
        "print (torch.mv(input=A, vec=x))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 4]) torch.Size([4])\n",
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "tensor([1., 2., 4., 8.])\n",
            "tensor([ 34.,  94., 154., 214., 274.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJvNloGpZee8"
      },
      "source": [
        "### Matrix Multiplication\n",
        "\n",
        "![](https://i.imgur.com/8lGoukC.png)\n",
        "\n",
        "#### Walkthrough\n",
        "\n",
        "![](https://i.imgur.com/YPiBqU8.jpeg)\n",
        "\n",
        "![](https://i.imgur.com/YPiBqU8.jpeg)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnAkZ0RbY8lp",
        "outputId": "6c787e92-a0d6-4411-be92-627e727b2204"
      },
      "source": [
        "print (A)\n",
        "\n",
        "B = torch.ones((4,3))\n",
        "\n",
        "print (A.shape, B.shape)\n",
        "mul = torch.mm(A, B)\n",
        "print (mul)\n",
        "print (mul.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "torch.Size([5, 4]) torch.Size([4, 3])\n",
            "tensor([[ 6.,  6.,  6.],\n",
            "        [22., 22., 22.],\n",
            "        [38., 38., 38.],\n",
            "        [54., 54., 54.],\n",
            "        [70., 70., 70.]])\n",
            "torch.Size([5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdaGhEkcvSA8",
        "outputId": "b33d90df-0713-4a40-c7b8-87b26a608b96"
      },
      "source": [
        "### a is each row vector; extract the one at idx 1\n",
        "a_2 = A[1, :]\n",
        "\n",
        "print (a_2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4., 5., 6., 7.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQWBLkUFv0hE",
        "outputId": "fe3e00a2-31d6-47c2-84a0-9c5635973b8e"
      },
      "source": [
        "### b is each col vector; extract the last one: b_m\n",
        "b_m = B[:, -1]\n",
        "\n",
        "print (b_m)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx3Zrc2vwKiI",
        "outputId": "21536897-2a22-40e3-b52d-f89ce05dfd31"
      },
      "source": [
        "### a2_T . b_m should give the result at pos 2, m\n",
        "\n",
        "print (a_2.T.dot(b_m))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(22.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc99Do5HwQJc",
        "outputId": "ee95870d-0fcc-4e0b-fbff-43a3e0e26056"
      },
      "source": [
        "print (mul)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.,  6.,  6.],\n",
            "        [22., 22., 22.],\n",
            "        [38., 38., 38.],\n",
            "        [54., 54., 54.],\n",
            "        [70., 70., 70.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny6VcmB44k7O"
      },
      "source": [
        "### Norms\n",
        "\n",
        "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components.\n",
        "\n",
        "You might notice that norms sound a lot like measures of distance. And if you remember Euclidean distances (think Pythagoras’ theorem) from grade school, then the concepts of non-negativity and the triangle inequality might ring a bell. In fact, the Euclidean distance is a norm:\n",
        "\n",
        "specifically it is the  L2  norm. Suppose that the elements in the  n -dimensional vector  x  are  x1,…,xn . The  L2  norm of  x  is the square root of the sum of the squares of the vector elements:\n",
        "\n",
        "$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$\n",
        "\n",
        "where the subscript  2  is often omitted in  L2  norms, i.e., $\\|\\mathbf{x}\\|$ is  equivalent to $\\|\\mathbf{x}\\|_2$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Jk0JO0wsI4",
        "outputId": "c93b91c0-495f-4ca7-9add-9f485b7b4c30"
      },
      "source": [
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHhmigt05O4w"
      },
      "source": [
        "In deep learning, we work more often with the squared  L2  norm. You will also frequently encounter the  L1  norm, which is expressed as the sum of the absolute values of the vector elements:\n",
        "\n",
        "$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$\n",
        "\n",
        "As compared with the  L2  norm, it is less influenced by outliers. To calculate the  L1  norm, we compose the absolute value function with a sum over the elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qr-pYGb5HNK",
        "outputId": "28fcb123-18f4-4ac9-f03b-bbc9381fae3e"
      },
      "source": [
        "torch.abs(u).sum()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oABEOPdz8WBH"
      },
      "source": [
        "Both the  L2  norm and the  L1  norm are special cases of the more general  Lp  norm:\n",
        "\n",
        "$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.|$\n",
        "\n",
        "Analogous to  L2  norms of vectors, the Frobenius norm of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
        "\n",
        "$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$\n",
        "\n",
        "The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an  L2  norm of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSRHFAEv5XME",
        "outputId": "7b6d47b2-e9cd-4fcc-b105-6b02ee5cd386"
      },
      "source": [
        "print (A)\n",
        "torch.norm(A)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(49.6991)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1huW-719815F"
      },
      "source": [
        "While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; minimize the distance between predictions and the ground-truth observations. Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, the objectives, perhaps the most important components of deep learning algorithms (besides the data), are expressed as norms.\n",
        "\n",
        "### Exercises\n",
        "\n",
        "6. Run A / A.sum(axis=1) and see what happens. Can you analyze the reason?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBt3e6mv8sgR",
        "outputId": "5feb9ddd-5af3-4d76-bbc6-1dda722ea59a"
      },
      "source": [
        "print (A)\n",
        "\n",
        "print (A.shape)\n",
        "\n",
        "print (A.sum(axis=1))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "torch.Size([5, 4])\n",
            "tensor([ 6., 22., 38., 54., 70.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba2jBr9m-ucA",
        "outputId": "6e300394-7d4f-43cd-b1b7-cc1f37a37676"
      },
      "source": [
        "A.size(), A.sum(axis=1).size()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), torch.Size([5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdu9s725-Of7"
      },
      "source": [
        "## A/A.sum(axis=1) -> error"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dBuofki_Y94"
      },
      "source": [
        "This will be fine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnHz5L5a-j0U",
        "outputId": "c7dca66a-6309-49c8-8385-19eb06e4ddd6"
      },
      "source": [
        "B = torch.arange(25, dtype = torch.float32).reshape(5, 5)\n",
        "print (B)\n",
        "print (B.sum(axis=1))\n",
        "B / B.sum(axis=1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
            "        [ 5.,  6.,  7.,  8.,  9.],\n",
            "        [10., 11., 12., 13., 14.],\n",
            "        [15., 16., 17., 18., 19.],\n",
            "        [20., 21., 22., 23., 24.]])\n",
            "tensor([ 10.,  35.,  60.,  85., 110.])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0286, 0.0333, 0.0353, 0.0364],\n",
              "        [0.5000, 0.1714, 0.1167, 0.0941, 0.0818],\n",
              "        [1.0000, 0.3143, 0.2000, 0.1529, 0.1273],\n",
              "        [1.5000, 0.4571, 0.2833, 0.2118, 0.1727],\n",
              "        [2.0000, 0.6000, 0.3667, 0.2706, 0.2182]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxmAFhBP_hNQ"
      },
      "source": [
        "8. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along axis 0, 1, and 2?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7Zf8xV_Txg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28040577-4380-454b-b148-87193daf893a"
      },
      "source": [
        "a = torch.randint(low=0, high=9, size=(2,3,4))\n",
        "a"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[7, 3, 8, 6],\n",
              "         [8, 7, 5, 4],\n",
              "         [2, 8, 4, 6]],\n",
              "\n",
              "        [[3, 3, 0, 3],\n",
              "         [2, 6, 7, 0],\n",
              "         [0, 0, 1, 5]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKP0oPnkT2_V",
        "outputId": "7502a403-d948-436e-aba0-87df04aa786f"
      },
      "source": [
        "print (a.shape)\n",
        "\n",
        "print (a.sum(axis=0))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 4])\n",
            "tensor([[10,  6,  8,  9],\n",
            "        [10, 13, 12,  4],\n",
            "        [ 2,  8,  5, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ268RB9T87C",
        "outputId": "ba297825-6294-4edf-8e0a-3eef5803424d"
      },
      "source": [
        "### shape of a is (2,3,4) i.e 2 separate 3x4 matrics\n",
        "a[0], a[1]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[7, 3, 8, 6],\n",
              "         [8, 7, 5, 4],\n",
              "         [2, 8, 4, 6]]), tensor([[3, 3, 0, 3],\n",
              "         [2, 6, 7, 0],\n",
              "         [0, 0, 1, 5]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8G2OYhDUCvZ",
        "outputId": "8dc489fc-dce1-4255-d5dd-b4c47160800c"
      },
      "source": [
        "## a.sum(axis=0) will sum up the 2 3x4 matrices resulting in a 3x4 matrix\n",
        "a.sum(axis=0).shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szp5dynXUPEE",
        "outputId": "08acef6e-4e79-4772-a968-c6b7b505869f"
      },
      "source": [
        "a[0]+a[1]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10,  6,  8,  9],\n",
              "        [10, 13, 12,  4],\n",
              "        [ 2,  8,  5, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxADjUv1UzNT",
        "outputId": "adaef6d7-65c6-4a9a-ef9b-e9adc2f6c8c5"
      },
      "source": [
        "a.sum(axis=0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10,  6,  8,  9],\n",
              "        [10, 13, 12,  4],\n",
              "        [ 2,  8,  5, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4YSh7UjZraj",
        "outputId": "58df6483-3df4-4acb-a2bf-f172e3f6ab7a"
      },
      "source": [
        "a.sum(axis=1).shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T_SSPJ5aCwA",
        "outputId": "bc9e618e-9879-424d-bce0-24143c7a0921"
      },
      "source": [
        "a"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[7, 3, 8, 6],\n",
              "         [8, 7, 5, 4],\n",
              "         [2, 8, 4, 6]],\n",
              "\n",
              "        [[3, 3, 0, 3],\n",
              "         [2, 6, 7, 0],\n",
              "         [0, 0, 1, 5]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8McnX_nZvvC",
        "outputId": "3780c7db-3d5d-4e1c-ad04-093b46f48f64"
      },
      "source": [
        "### 2x4 shape: 2 rows for 2 matrices; 4 elems each for the sum of each matrix along the columns\n",
        "a.sum(axis=1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[17, 18, 17, 16],\n",
              "        [ 5,  9,  8,  8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY41-rTGZ5RY",
        "outputId": "ff4af3ee-131d-4563-952f-e681745fa63f"
      },
      "source": [
        "a.sum(axis=2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 24, 20],\n",
              "        [ 9, 15,  6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBwBD24LaqVr",
        "outputId": "8ea44185-3972-4e6e-bd05-2186e6276043"
      },
      "source": [
        "### 2x3 shape: 2 rows for 2 matrices; 3 elems each for the sum of each matrix along the rows\n",
        "\n",
        "a.sum(axis=2).shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAigyPVUbkuW"
      },
      "source": [
        "9. Feed a tensor with 3 or more axes to the linalg.norm function and observe its output. What does this function compute for tensors of arbitrary shape?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSEuphS5axz6",
        "outputId": "ab0a6b20-1751-45db-d891-70e0e38422cb"
      },
      "source": [
        "Y= torch.arange(24,dtype = torch.float32).reshape(2, 3, 4)\n",
        "print (Y)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.,  1.,  2.,  3.],\n",
            "         [ 4.,  5.,  6.,  7.],\n",
            "         [ 8.,  9., 10., 11.]],\n",
            "\n",
            "        [[12., 13., 14., 15.],\n",
            "         [16., 17., 18., 19.],\n",
            "         [20., 21., 22., 23.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0skacSdobx21",
        "outputId": "26ac6fc1-a2cd-4263-f62b-faed3de862f8"
      },
      "source": [
        "torch.linalg.norm(Y)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(65.7571)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XCz-F4mdqWa"
      },
      "source": [
        "Analogous to  L2  norms of vectors, the Frobenius norm of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
        "\n",
        "$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw45HzFLb8gF",
        "outputId": "c8a14172-059c-4755-ae4f-c6dfad78cf69"
      },
      "source": [
        "sum = 0\n",
        "for elem in Y[:]:\n",
        "    print (elem.shape)\n",
        "    for elem1 in elem:\n",
        "        for elem2 in elem1:\n",
        "            sum+= elem2.item()**2\n",
        "\n",
        "print(sum**0.5)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([3, 4])\n",
            "65.75712889109438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDFEGSKue8Xb"
      },
      "source": [
        "This has computed the Frobenus norm for the tensor of shape 2x3x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABOtW9fghj3e"
      },
      "source": [
        "## Calculus\n",
        "\n",
        "### Derivatives and Differentiation\n",
        "\n",
        "We begin by addressing the calculation of derivatives, a crucial step in nearly all deep learning optimization algorithms. In deep learning, we typically choose loss functions that are differentiable with respect to our model’s parameters. Put simply, this means that for each parameter, we can determine how rapidly the loss would increase or decrease, were we to increase or decrease that parameter by an infinitesimally small amount.\n",
        "\n",
        "Suppose that we have a function  $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ whose input and output are both scalars. The derivative of  f  is defined as\n",
        "\n",
        "$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$\n",
        "\n",
        "if this limit exists. If  f'(a) exists,  f  is said to be differentiable at a . If  f  is differentiable at every number of an interval, then this function is differentiable on this interval. We can interpret the derivative  f′(x)  in (2.4.1) as the instantaneous rate of change of  f(x)  with respect to  x . The so-called instantaneous rate of change is based on the variation  h  in  x , which approaches  0 .\n",
        "\n",
        "To illustrate derivatives, let us experiment with an example. Define  \n",
        "$u = f(x) = 3x^2-4x$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wVMppOkchB3"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MtrHWBAiOzE",
        "outputId": "f1cf88d6-0ae6-40e7-ae2c-aae3fac40179"
      },
      "source": [
        "def numerical_lim(f, x, h):\n",
        "    return (f(x+h) - f(x))/h\n",
        "\n",
        "h = 0.1\n",
        "for i in range(10):\n",
        "    print (f'h = {h}, numerical limit = {numerical_lim(f, 1, h)}')\n",
        "    h *= 0.1"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h = 0.1, numerical limit = 2.3000000000000043\n",
            "h = 0.010000000000000002, numerical limit = 2.029999999999976\n",
            "h = 0.0010000000000000002, numerical limit = 2.0029999999993104\n",
            "h = 0.00010000000000000003, numerical limit = 2.000299999997956\n",
            "h = 1.0000000000000004e-05, numerical limit = 2.0000300000155837\n",
            "h = 1.0000000000000004e-06, numerical limit = 2.0000030001021676\n",
            "h = 1.0000000000000005e-07, numerical limit = 2.000000298707504\n",
            "h = 1.0000000000000005e-08, numerical limit = 1.999999987845057\n",
            "h = 1.0000000000000005e-09, numerical limit = 2.000000165480741\n",
            "h = 1.0000000000000006e-10, numerical limit = 2.000000165480741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct4Rs_DquGvz"
      },
      "source": [
        "Let us familiarize ourselves with a few equivalent notations for derivatives. Given  y=f(x) , where  x  and  y  are the independent variable and the dependent variable of the function  f , respectively. The following expressions are equivalent:\n",
        "\n",
        "$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$\n",
        "\n",
        "### Partial Derivatives\n",
        "\n",
        "So far we have dealt with the differentiation of functions of just one variable. In deep learning, functions often depend on many variables. Thus, we need to extend the ideas of differentiation to these multivariate functions.\n",
        "\n",
        "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with  n  variables. The partial derivative of  y  with respect to its  ith  parameter  xi  is:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$\n",
        "\n",
        "To calculate  $\\frac{\\partial y}{\\partial x_i}$ we can simply treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ s constants and calculate the derivative of  y  with respect to  xi . For notation of partial derivatives, the following are equivalent:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$\n",
        "\n",
        "### Gradients\n",
        "\n",
        "Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an  n -dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function  $f(\\mathbf{x})$  with respect to  $\\mathbf{x}$  is a vector of  n  partial derivatives:\n",
        "\n",
        "$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$\n",
        "\n",
        "Note that this also returns an n-dimensional vector\n",
        "\n",
        "where $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ is often replaced by $\\nabla f(\\mathbf{x})$  when there is no ambiguity.\n",
        "\n",
        "![](https://i.imgur.com/2qsZFkH.png)\n",
        "\n",
        "### Chain Rule\n",
        "\n",
        "However, such gradients can be hard to find. This is because multivariate functions in deep learning are often composite, so we may not apply any of the aforementioned rules to differentiate these functions. Fortunately, the chain rule enables us to differentiate composite functions.\n",
        "\n",
        "Let us first consider functions of a single variable. Suppose that functions y = f(u) and u = g(x) are both differentiable, then the chain rule states that\n",
        "\n",
        "$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$\n",
        "\n",
        "Now let us turn our attention to a more general scenario where functions have an arbitrary number of variables. Suppose that the differentiable function  y  has variables u1, u2... um,  where each differentiable function  ui  has variables  x1,x2,…,xn . Note that  y  is a function of  x1,x2,…,xn . Then the chain rule gives\n",
        "\n",
        "$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHjGw2Ug-OSv"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "As we have explained in Section 2.4, differentiation is a crucial step in nearly all deep learning optimization algorithms. While the calculations for taking these derivatives are straightforward, requiring only some basic calculus, for complex models, working out the updates by hand can be a pain (and often error-prone).\n",
        "\n",
        "Deep learning frameworks expedite this work by automatically calculating derivatives, i.e., automatic differentiation. In practice, based on our designed model the system builds a computational graph, tracking which data combined through which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Here, backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter.\n",
        "\n",
        "As a toy example, say that we are interested in differentiating the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to the column vector  x . To start, let us create the variable x and assign it an initial value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4a3ybnkXqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87a90fb-1118-404a-a47b-e0a33aa31db8"
      },
      "source": [
        "x = torch.arange(4.0)\n",
        "print (x.shape)\n",
        "x"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQgekokX-pzD"
      },
      "source": [
        "The shape of x is 4x1. 2 x^T x will be a scalar. Also we know that a gradient of a scalar-valued function with respect to a vector  x  is itself vector-valued and has the same shape as  x .\n",
        "\n",
        "Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an  n -dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function  $f(\\mathbf{x})$  with respect to  $\\mathbf{x}$  is a vector of  n  partial derivatives:\n",
        "\n",
        "$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$\n",
        "\n",
        "Before we even calculate the gradient of  y  with respect to  x , we will need a place to store it. It is important that we do not allocate new memory every time we take a derivative with respect to a parameter because we will often update the same parameters thousands or millions of times and could quickly run out of memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkHEwOwW-fdJ"
      },
      "source": [
        "x.requires_grad_(True)  # Same as `x = torch.arange(4.0, requires_grad=True)` ### This means we want to calculate grad wrt x\n",
        "x.grad  # The default value is None"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Ud_wmwKmGV"
      },
      "source": [
        "Now let us calculate  y . Note: y here is a scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQIxoU4xKi7r",
        "outputId": "4d8e26b6-51a0-40dd-b3b2-da339e2bcda7"
      },
      "source": [
        "y = 2*torch.dot(x, x) ## 2 (0.0 + 1.1 + 2.2 + 3.3)\n",
        "print (x)\n",
        "print (y)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor(28., grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3tPMwRMK8PI"
      },
      "source": [
        "Since x is a vector of length 4, an inner product of x and x is performed, yielding the scalar output that we assign to y. Next, we can automatically calculate the gradient of y with respect to each component of x by calling the function for backpropagation and printing the gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJkKpe9MKsye",
        "outputId": "d81c3619-8a90-485c-969c-5564ed06823d"
      },
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnGnn55eMvc4"
      },
      "source": [
        "The gradient of the function  $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$  with respect to $\\mathbf{x}$ will be $4\\mathbf{x}$.  Let us quickly verify that our desired gradient was calculated correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJNZX2f7LN1h",
        "outputId": "a3e48874-9da7-4fcf-8f8b-8328411da21f"
      },
      "source": [
        "4*x"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0U86YuONPqY"
      },
      "source": [
        "Now let us calculate another function of x.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n-8wJgXNCgi",
        "outputId": "2d01b071-28f9-4477-eede-469e2662b51e"
      },
      "source": [
        "# PyTorch accumulates the gradient in default, we need to clear the previous values\n",
        "x.grad.zero_()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DPF9EVwNU8e",
        "outputId": "d3e036e3-4256-4bad-e67c-13a215429569"
      },
      "source": [
        "y = x.sum()\n",
        "print (y)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoZlHaCTNfot",
        "outputId": "3c6a97fb-3393-49ec-bfe0-66a111c0c6ee"
      },
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9IpuNv8Npjf"
      },
      "source": [
        "y = x1 + x2 + x3 + x4 where x1, x2, x3, x4 are each elem of vector x\n",
        "\n",
        "dy/dx = [dy/dx1, dy/dx2, dy/dx3, dy/dx4]: Note each of these are the partial derivatives\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGm3ORJWN-xE"
      },
      "source": [
        "### Backward for Non-Scalar Variables\n",
        "\n",
        "Technically, when y is not a scalar, the most natural interpretation of the differentiation of a vector y with respect to a vector x is a matrix. For higher-order and higher-dimensional y and x, the differentiation result could be a high-order tensor.\n",
        "\n",
        "However, while these more exotic objects do show up in advanced machine learning (including in deep learning), more often when we are calling backward on a vector, we are trying to calculate the derivatives of the loss functions for each constituent of a batch of training examples. Here, our intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives computed individually for each example in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onxWh7qnNpEj"
      },
      "source": [
        "x.grad.zero_()\n",
        "y = x * x"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-uW7OEtNhmX",
        "outputId": "011fda0b-c842-4fd2-8d5c-f94398fb2c90"
      },
      "source": [
        "print (x)\n",
        "print (y)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz71ntCvObV4",
        "outputId": "a68bbaaa-2053-4567-dc63-96cf35de18e0"
      },
      "source": [
        "print (y.sum())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(14., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5PjwOdLOrxm"
      },
      "source": [
        "y.sum().backward()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLVTDnvfOv2r",
        "outputId": "9f2027d4-37c5-47c8-ee2e-e5fc51b77913"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N8DtSySQ3v3"
      },
      "source": [
        "Here again y is basically y = x1^2 + x2^2 + x3^2 + x4^2\n",
        "\n",
        "so dy/dx = [dy/dx1, dy/dx2, dy/dx3, dy/dx4]: Note each of these are the partial derivatives = [2x1, 2x2, 2x3, 2x4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG2azvLCRf6h"
      },
      "source": [
        "### Detaching Computation\n",
        "\n",
        "Sometimes, we wish to move some calculations outside of the recorded computational graph. For example, say that y was calculated as a function of x, and that subsequently z was calculated as a function of both y and x. Now, imagine that we wanted to calculate the gradient of z with respect to x, but wanted for some reason to treat y as a constant, and only take into account the role that x played after y was calculated.\n",
        "\n",
        "say y = x^2 and z = y*x = x^2 * x\n",
        "\n",
        "Here, we can detach y to return a new variable u that has the same value as y but discards any information about how y was computed in the computational graph. In other words, the gradient will not flow backwards through u to x. Thus, the following backpropagation function computes the partial derivative of z = u * x with respect to x while treating u as a constant, instead of the partial derivative of z = x * x * x with respect to x.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl7jraMqPAII",
        "outputId": "4d0da3cb-a7e0-4c75-9cac-4e349479a345"
      },
      "source": [
        "x.grad.zero_()\n",
        "print (x)\n",
        "y = x * x\n",
        "print (y)\n",
        "\n",
        "u = y.detach()\n",
        "print (u)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
            "tensor([0., 1., 4., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDpyTgbYSL3-",
        "outputId": "36a2b3df-84e3-48cb-d315-cbdec0f319c0"
      },
      "source": [
        "z = u * x\n",
        "z.sum().backward()\n",
        "print (x.grad) ### should return u"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 4., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXPaxITIbYht",
        "outputId": "c3139276-c8e8-4cb6-c6af-871ad68fd1a5"
      },
      "source": [
        "### without detaching\n",
        "x.grad.zero_()\n",
        "print (x)\n",
        "y = x * x\n",
        "print (y)\n",
        "z = y * x\n",
        "z.sum().backward()\n",
        "print (x.grad) ### 3 x^2 = [3.0^2, 3.1^2, 3.2^2, 3.3^2]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
            "tensor([ 0.,  3., 12., 27.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYCKmQOb706"
      },
      "source": [
        "### Computing the Gradient of Python Control Flow\n",
        "\n",
        "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. In the following snippet, note that the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input a.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pcEsDpfblLg"
      },
      "source": [
        "def f(a):\n",
        "    ## init b = 2a\n",
        "    b = a*2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        print ('here', b)\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBKVG4dod7CW",
        "outputId": "16a386ca-3dc1-4a83-9059-9cdd80492953"
      },
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "print (a)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0015, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr4u454XeBK9",
        "outputId": "59c745f3-cc54-4ece-bdcb-d78c5a89920c"
      },
      "source": [
        "print (a.norm())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0015, grad_fn=<CopyBackwards>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt1T4F81eFIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ecf490-69ab-42e4-c35e-72324e475d76"
      },
      "source": [
        "d = f(a)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here tensor(1025.4880, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXCtaI4relYB"
      },
      "source": [
        "d = f(a) = 2*a * 2^k = k(some const) * a\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjZNlvk2eTPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78dea9e-f5bd-4c87-9503-25bd2e6d329e"
      },
      "source": [
        "d = f(a)\n",
        "d.backward()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here tensor(1025.4880, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjGsDU2Ne18-",
        "outputId": "92a5c924-5bbd-41cc-d3e7-cfac55beaffc"
      },
      "source": [
        "print (a.grad)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1024.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZjNa501e4Jt",
        "outputId": "e49fa3cd-b029-482a-ba0c-e4bc20e8b392"
      },
      "source": [
        "d/a"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1024., grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xv_qTMpfE3s"
      },
      "source": [
        "We can now analyze the f function defined above. Note that it is piecewise linear in its input a. In other words, for any a there exists some constant scalar k such that f(a) = k * a, where the value of k depends on the input a\n",
        "\n",
        "f(a) = k*a => df/da = k  = f(a)/a = d/a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsFPT2tojPS1"
      },
      "source": [
        "## Probablility\n",
        "\n",
        "### Basic Probability Theory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH5rOQQ2gQmX"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.distributions import multinomial"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGmXUiVmjfzc"
      },
      "source": [
        "Next, we will want to be able to cast the die. In statistics we call this process of drawing examples from probability distributions sampling. The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution. We will give a more formal definition of distribution later, but at a high level, think of it as just an assignment of probabilities to events.\n",
        "\n",
        "To draw a single sample, we simply pass in a vector of probabilities. The output is another vector of the same length: its value at index  i  is the number of times the sampling outcome corresponds to  i ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUioZE4PjYtv",
        "outputId": "1675ed7d-1c30-4b26-d83f-c5285c161b0b"
      },
      "source": [
        "fair_probs = torch.ones([6]) / 6\n",
        "print (fair_probs)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73hBC2siAAOV",
        "outputId": "48ce7c5b-5833-4f3d-a8e2-be002b880db4"
      },
      "source": [
        "### sample from a multinomial total_count times with prob of each event given by fair_probs\n",
        "multinomial.Multinomial(total_count=1, probs=fair_probs).sample()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAdvh_EAh_M"
      },
      "source": [
        "If you run the sampler a bunch of times, you will find that you get out random values each time. As with estimating the fairness of a die, we often want to generate many samples from the same distribution. It would be unbearably slow to do this with a Python for loop, so the function we are using supports drawing multiple samples at once, returning an array of independent samples in any shape we might desire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI46a64TAS7y",
        "outputId": "fc7fbc29-e7cb-4a45-a755-59e2e2013d30"
      },
      "source": [
        "multinomial.Multinomial(10, fair_probs).sample()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 0., 0., 2., 2., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDMpY1iVAmZT",
        "outputId": "29525764-e6f7-4d4b-94c6-56d6a4203e5e"
      },
      "source": [
        "# Store the results as 32-bit floats for division\n",
        "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
        "counts / 1000  # Relative frequency as the estimate"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1760, 0.1590, 0.1570, 0.1760, 0.1530, 0.1790])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhOtn-IqAu9u"
      },
      "source": [
        "Because we generated the data from a fair die, we know that each outcome has true probability  16 , roughly  0.167 , so the above output estimates look good.\n",
        "\n",
        "We can also visualize how these probabilities converge over time towards the true probability. Let us conduct 500 groups of experiments where each group draws 10 samples.\n",
        "\n",
        "![](https://d2l.ai/_images/output_probability_245b7d_54_0.svg)\n",
        "\n",
        "Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments, the  6  solid curves converge towards the true probability.\n",
        "\n",
        "### Random variables\n",
        "\n",
        "![](https://i.imgur.com/ikJ3lk8.png)\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/lVO07r1.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGc3Pf9zKL4r"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8WhQqAQagM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu3dwP9zQbRf"
      },
      "source": [
        "## Linear Neural Networks\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "### Some assumptions\n",
        "\n",
        "1. First, we assume that the relationship between the independent variables $\\mathbf{x}$ and the dependent variable  y  is linear, i.e., that  y  can be expressed as a weighted sum of the elements in  $\\mathbf{x}$ , given some noise on the observations. Note here we represent $\\mathbf{x}$ as a vetor\n",
        "\n",
        "2. Second, we assume that any noise is well-behaved (following a Gaussian distribution).\n",
        "\n",
        "To motivate the approach, let us start with a running example. Suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To actually develop a model for predicting house prices, we would need to get our hands on a dataset consisting of sales for which we know the sale price, area, and age for each home. In the terminology of machine learning, the dataset is called a training dataset or training set, and each row (here the data corresponding to one sale) is called an example (or data point, data instance, sample). The thing we are trying to predict (price) is called a label (or target). The independent variables (age and area) upon which the predictions are based are called features (or covariates).\n",
        "\n",
        "Typically, we will use  n  to denote the number of examples in our dataset. We index the data examples by  i , denoting each input as\n",
        "\n",
        "\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$ and the corresponding label as $y^{(i)}$\n",
        "\n",
        "The linearity assumption just says that the target (price) can be expressed as a weighted sum of the features (area and age):\n",
        "\n",
        "$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b.$\n",
        "\n",
        "The weights determine the influence of each feature on our prediction and the bias just says what value the predicted price should take when all of the features take value 0. Even if we will never see any homes with zero area, or that are precisely zero years old, we still need the bias or else we will limit the expressivity of our model. Strictly speaking, the above equation is an affine transformation of input features, which is characterized by a linear transformation of features via weighted sum, combined with a translation via the added bias.\n",
        "\n",
        "Given a dataset, our goal is to choose the weights  $\\mathbf{w}$  and the bias  b  such that on average, the predictions made according to our model best fit the true prices observed in the data. Models whose output prediction is determined by the affine transformation of input features are linear models, where the affine transformation is specified by the chosen weights and bias.\n",
        "\n",
        "![](https://i.imgur.com/Oi0F47h.png)\n",
        "\n",
        "#### Some calculation walkthroughs\n",
        "\n",
        "![](https://i.imgur.com/EfgcBH2.jpg)\n",
        "\n",
        "![](https://i.imgur.com/sph1CMM.jpg)\n",
        "\n",
        "### Loss functions\n",
        "\n",
        "![](https://i.imgur.com/b4Nei7P.png)\n",
        "\n",
        "\n",
        "Note here $\\mathbf{x}^{(i)}$ is the ith feature vector "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgbJh_3HLjHr"
      },
      "source": [
        "### Analytic Solution\n",
        "\n",
        "Linear regression happens to be an unusually simple optimization problem. Unlike most other models that we will encounter in this book, linear regression can be solved analytically by applying a simple formula. To start, we can subsume the bias b into the parameter $\\mathbf{w}$ by appending a column to the design matrix consisting of all ones. Then our prediction problem is to minimize $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$\n",
        "\n",
        "#### Verification\n",
        "\n",
        "Let's verify that $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$ does indeed give the previous equation we had\n",
        "\n",
        "Matrix norms reference: https://learn.lboro.ac.uk/archive/olmp/olmp_resources/pages/workbooks_1_50_jan2008/Workbook30/30_4_mtrx_norms.pdf\n",
        "\n",
        "![](https://i.imgur.com/0A19uqO.jpeg)\n",
        "\n",
        "Taking the derivative of the loss with respect to  $\\mathbf{w}$  and setting it equal to zero yields the analytic (closed-form) solution:\n",
        "\n",
        "$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$\n",
        "\n",
        "https://i.imgur.com/g0HwF5V.jpeg\n",
        "\n",
        "![](https://i.imgur.com/g0HwF5V.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbpU_0D8TQ_M"
      },
      "source": [
        "\n",
        "### Minibatch Stochastic Gradient Descent\n",
        "\n",
        "Even in cases where we cannot solve the models analytically, it turns out that we can still train models effectively in practice. Moreover, for many tasks, those difficult-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.\n",
        "\n",
        "The key technique for optimizing nearly any deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called gradient descent.\n",
        "\n",
        "The most naive application of gradient descent consists of taking the derivative of the loss function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update. Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called minibatch stochastic gradient descent.\n",
        "\n",
        "![](https://i.imgur.com/UP5geII.png)\n",
        "\n",
        "#### Calculations watkthrough:\n",
        "\n",
        "![](https://i.imgur.com/tTjHJv9.jpg)\n",
        "\n",
        "![](https://i.imgur.com/XS86Z1v.jpeg)\n",
        "\n",
        "This shows the derivations of the weight vector and bias. pay careful attention to the dimensionalities shown\n",
        "\n",
        "After training for some predetermined number of iterations (or until some other stopping criteria are met), we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$. Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss because, although the algorithm converges slowly towards the minimizers it cannot achieve it exactly in a finite number of steps.\n",
        "\n",
        "Linear regression happens to be a learning problem where there is only one minimum over the entire domain. However, for more complicated models, like deep networks, the loss surfaces contain many minima. Fortunately, __for reasons that are not yet fully understood__, deep learning practitioners seldom struggle to find parameters that minimize the loss on training sets. The more formidable task is to find parameters that will achieve low loss on data that we have not seen before, a challenge called generalization. We return to these topics throughout the book.\n",
        "\n",
        "---\n",
        "\n",
        "Given the learned linear regression model $\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$ we can now estimate the price of a new house (not contained in the training data) given its area  x1  and age  x2 . Estimating targets given features is commonly called prediction or inference.\n",
        "\n",
        "We will try to stick with prediction because calling this step inference, despite emerging as standard jargon in deep learning, is somewhat of a misnomer. In statistics, inference more often denotes estimating parameters based on a dataset. This misuse of terminology is a common source of confusion when deep learning practitioners talk to statisticians.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uy6I878hcDe"
      },
      "source": [
        "## Linear Regression from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPmWcRkNAsHt"
      },
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz8H7nYshlaA"
      },
      "source": [
        "### Generating the Dataset\n",
        "\n",
        "To keep things simple, we will construct an artificial dataset according to a linear model with additive noise. Our task will be to recover this model’s parameters using the finite set of examples contained in our dataset. We will keep the data low-dimensional so we can visualize it easily. In the following code snippet, we generate a dataset containing 1000 examples, each consisting of 2 features sampled from a standard normal distribution. Thus our synthetic dataset will be a matrix $\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$\n",
        "\n",
        "The true parameters generating our dataset will be $\\mathbf{w} = [2, -3.4]^\\top$\n",
        " and b = 4.2 and our synthetic labels will be assigned according to the following linear model with the noise term $\\epsilon$\n",
        "\n",
        "$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$\n",
        "\n",
        "You could think of  ϵ  as capturing potential measurement errors on the features and labels. We will assume that the standard assumptions hold and thus that  ϵ  obeys a normal distribution with mean of 0. To make our problem easy, we will set its standard deviation to 0.01. The following code generates our synthetic dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mstUFcuMhjBR",
        "outputId": "491512d7-a668-418e-d543-8c5edf2ddb20"
      },
      "source": [
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"\n",
        "    Generate y = Xw + b + noise.\n",
        "    Return features: shape (num_examples, w)\n",
        "    labels : shape (num_examples)\n",
        "    \"\"\"\n",
        "    X = torch.normal(mean=0, std=1, size=(num_examples, len(w)))\n",
        "    print (X.shape)\n",
        "    y = torch.matmul(X, w) + b\n",
        "    print (y.shape)\n",
        "    ### add noise\n",
        "    y += torch.normal(mean=0, std=0.01, size=y.shape)\n",
        "    return X, y.reshape((-1, 1))\n",
        "\n",
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)\n",
        "print (features.shape, labels.shape)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 2])\n",
            "torch.Size([1000])\n",
            "torch.Size([1000, 2]) torch.Size([1000, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMfJfBLevkd_"
      },
      "source": [
        "By generating a scatter plot using the second feature features[:, 1] and labels, we can clearly observe the linear correlation between the two.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "LxKt8BD-qc0L",
        "outputId": "30f2a1b7-8f95-4a98-acef-5e909efb2bf9"
      },
      "source": [
        "d2l.set_figsize()\n",
        "# The semicolon is for displaying the plot only\n",
        "d2l.plt.scatter(features[:, (1)].detach().numpy(),\n",
        "                labels.detach().numpy(), 1);\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"166.978125pt\" version=\"1.1\" viewBox=\"0 0 231.442187 166.978125\" width=\"231.442187pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 166.978125 \nL 231.442187 166.978125 \nL 231.442187 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 28.942188 143.1 \nL 224.242188 143.1 \nL 224.242188 7.2 \nL 28.942188 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 0.5 \nC 0.132602 0.5 0.25979 0.447317 0.353553 0.353553 \nC 0.447317 0.25979 0.5 0.132602 0.5 0 \nC 0.5 -0.132602 0.447317 -0.25979 0.353553 -0.353553 \nC 0.25979 -0.447317 0.132602 -0.5 0 -0.5 \nC -0.132602 -0.5 -0.25979 -0.447317 -0.353553 -0.353553 \nC -0.447317 -0.25979 -0.5 -0.132602 -0.5 0 \nC -0.5 0.132602 -0.447317 0.25979 -0.353553 0.353553 \nC -0.25979 0.447317 -0.132602 0.5 0 0.5 \nz\n\" id=\"maac0ae2ca9\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#p0b5a6b2379)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.068506\" xlink:href=\"#maac0ae2ca9\" y=\"82.551411\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.241658\" xlink:href=\"#maac0ae2ca9\" y=\"62.60849\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.083116\" xlink:href=\"#maac0ae2ca9\" y=\"92.377617\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.536565\" xlink:href=\"#maac0ae2ca9\" y=\"78.182732\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.500939\" xlink:href=\"#maac0ae2ca9\" y=\"79.868919\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.940293\" xlink:href=\"#maac0ae2ca9\" y=\"59.136792\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.206613\" xlink:href=\"#maac0ae2ca9\" y=\"72.522962\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.472025\" xlink:href=\"#maac0ae2ca9\" y=\"80.540665\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.730867\" xlink:href=\"#maac0ae2ca9\" y=\"70.987829\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.029636\" xlink:href=\"#maac0ae2ca9\" y=\"61.368218\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.388636\" xlink:href=\"#maac0ae2ca9\" y=\"51.061292\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.622625\" xlink:href=\"#maac0ae2ca9\" y=\"83.648108\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"143.21241\" xlink:href=\"#maac0ae2ca9\" y=\"90.544113\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.905455\" xlink:href=\"#maac0ae2ca9\" y=\"53.288339\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.743094\" xlink:href=\"#maac0ae2ca9\" y=\"70.011572\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.10039\" xlink:href=\"#maac0ae2ca9\" y=\"75.325069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.98283\" xlink:href=\"#maac0ae2ca9\" y=\"92.062764\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"73.538278\" xlink:href=\"#maac0ae2ca9\" y=\"49.928717\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.739766\" xlink:href=\"#maac0ae2ca9\" y=\"64.553781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.960539\" xlink:href=\"#maac0ae2ca9\" y=\"97.942459\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.35002\" xlink:href=\"#maac0ae2ca9\" y=\"99.793477\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.153455\" xlink:href=\"#maac0ae2ca9\" y=\"71.472482\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.86553\" xlink:href=\"#maac0ae2ca9\" y=\"99.924675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.614632\" xlink:href=\"#maac0ae2ca9\" y=\"60.398057\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"68.639136\" xlink:href=\"#maac0ae2ca9\" y=\"44.856254\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.595011\" xlink:href=\"#maac0ae2ca9\" y=\"56.426027\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.8468\" xlink:href=\"#maac0ae2ca9\" y=\"62.107868\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.876247\" xlink:href=\"#maac0ae2ca9\" y=\"104.141008\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.731991\" xlink:href=\"#maac0ae2ca9\" y=\"59.44762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"72.166537\" xlink:href=\"#maac0ae2ca9\" y=\"36.611914\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.238511\" xlink:href=\"#maac0ae2ca9\" y=\"81.915064\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.045367\" xlink:href=\"#maac0ae2ca9\" y=\"90.024908\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"170.779306\" xlink:href=\"#maac0ae2ca9\" y=\"103.73776\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"72.44749\" xlink:href=\"#maac0ae2ca9\" y=\"60.108271\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.160098\" xlink:href=\"#maac0ae2ca9\" y=\"89.276659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.764905\" xlink:href=\"#maac0ae2ca9\" y=\"106.055235\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.503365\" xlink:href=\"#maac0ae2ca9\" y=\"44.546785\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.167551\" xlink:href=\"#maac0ae2ca9\" y=\"47.897838\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"163.427341\" xlink:href=\"#maac0ae2ca9\" y=\"98.35161\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.196751\" xlink:href=\"#maac0ae2ca9\" y=\"73.627193\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"64.821074\" xlink:href=\"#maac0ae2ca9\" y=\"39.659791\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.868827\" xlink:href=\"#maac0ae2ca9\" y=\"59.557706\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.451178\" xlink:href=\"#maac0ae2ca9\" y=\"77.701267\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.929774\" xlink:href=\"#maac0ae2ca9\" y=\"72.69759\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.540925\" xlink:href=\"#maac0ae2ca9\" y=\"98.335018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.246326\" xlink:href=\"#maac0ae2ca9\" y=\"63.479073\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.625736\" xlink:href=\"#maac0ae2ca9\" y=\"96.801149\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.484589\" xlink:href=\"#maac0ae2ca9\" y=\"106.038054\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.577145\" xlink:href=\"#maac0ae2ca9\" y=\"52.786667\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.606999\" xlink:href=\"#maac0ae2ca9\" y=\"47.38864\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.272201\" xlink:href=\"#maac0ae2ca9\" y=\"64.068469\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.957655\" xlink:href=\"#maac0ae2ca9\" y=\"52.62462\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.900781\" xlink:href=\"#maac0ae2ca9\" y=\"72.966212\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.068718\" xlink:href=\"#maac0ae2ca9\" y=\"74.462971\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.714362\" xlink:href=\"#maac0ae2ca9\" y=\"72.752915\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.907148\" xlink:href=\"#maac0ae2ca9\" y=\"73.001376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.249198\" xlink:href=\"#maac0ae2ca9\" y=\"81.839037\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.827083\" xlink:href=\"#maac0ae2ca9\" y=\"74.32522\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.815459\" xlink:href=\"#maac0ae2ca9\" y=\"66.988408\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.448706\" xlink:href=\"#maac0ae2ca9\" y=\"87.179958\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.699407\" xlink:href=\"#maac0ae2ca9\" y=\"86.511262\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"169.659636\" xlink:href=\"#maac0ae2ca9\" y=\"101.389964\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.777047\" xlink:href=\"#maac0ae2ca9\" y=\"76.763095\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.757619\" xlink:href=\"#maac0ae2ca9\" y=\"81.898678\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"40.451252\" xlink:href=\"#maac0ae2ca9\" y=\"32.718562\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.690728\" xlink:href=\"#maac0ae2ca9\" y=\"60.184687\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.218428\" xlink:href=\"#maac0ae2ca9\" y=\"91.623523\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.134494\" xlink:href=\"#maac0ae2ca9\" y=\"75.289618\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.931747\" xlink:href=\"#maac0ae2ca9\" y=\"43.007136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.493466\" xlink:href=\"#maac0ae2ca9\" y=\"73.658788\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.695104\" xlink:href=\"#maac0ae2ca9\" y=\"54.776334\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.225382\" xlink:href=\"#maac0ae2ca9\" y=\"71.649718\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.714486\" xlink:href=\"#maac0ae2ca9\" y=\"82.624085\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.073884\" xlink:href=\"#maac0ae2ca9\" y=\"82.055018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.728142\" xlink:href=\"#maac0ae2ca9\" y=\"90.323816\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.824666\" xlink:href=\"#maac0ae2ca9\" y=\"89.044798\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.97071\" xlink:href=\"#maac0ae2ca9\" y=\"96.416133\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.313105\" xlink:href=\"#maac0ae2ca9\" y=\"83.628767\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.728159\" xlink:href=\"#maac0ae2ca9\" y=\"91.773557\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.594172\" xlink:href=\"#maac0ae2ca9\" y=\"81.982852\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.880128\" xlink:href=\"#maac0ae2ca9\" y=\"78.286982\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.076472\" xlink:href=\"#maac0ae2ca9\" y=\"59.13066\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.332291\" xlink:href=\"#maac0ae2ca9\" y=\"76.480792\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.500635\" xlink:href=\"#maac0ae2ca9\" y=\"107.470029\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.419324\" xlink:href=\"#maac0ae2ca9\" y=\"65.085355\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.612772\" xlink:href=\"#maac0ae2ca9\" y=\"53.701716\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.809592\" xlink:href=\"#maac0ae2ca9\" y=\"66.296733\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"89.906662\" xlink:href=\"#maac0ae2ca9\" y=\"55.178813\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.089329\" xlink:href=\"#maac0ae2ca9\" y=\"92.436711\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.111263\" xlink:href=\"#maac0ae2ca9\" y=\"103.086248\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.254392\" xlink:href=\"#maac0ae2ca9\" y=\"81.456411\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.299409\" xlink:href=\"#maac0ae2ca9\" y=\"52.744812\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.102928\" xlink:href=\"#maac0ae2ca9\" y=\"51.407727\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"143.193841\" xlink:href=\"#maac0ae2ca9\" y=\"91.005566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.688965\" xlink:href=\"#maac0ae2ca9\" y=\"89.156798\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.154845\" xlink:href=\"#maac0ae2ca9\" y=\"61.031917\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.356962\" xlink:href=\"#maac0ae2ca9\" y=\"100.387803\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.094321\" xlink:href=\"#maac0ae2ca9\" y=\"91.162132\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.069778\" xlink:href=\"#maac0ae2ca9\" y=\"81.339305\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.28325\" xlink:href=\"#maac0ae2ca9\" y=\"64.090993\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.403773\" xlink:href=\"#maac0ae2ca9\" y=\"73.463426\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.27303\" xlink:href=\"#maac0ae2ca9\" y=\"46.092087\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"169.896599\" xlink:href=\"#maac0ae2ca9\" y=\"109.685477\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.391256\" xlink:href=\"#maac0ae2ca9\" y=\"84.765599\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.603012\" xlink:href=\"#maac0ae2ca9\" y=\"61.252657\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.945109\" xlink:href=\"#maac0ae2ca9\" y=\"60.320839\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.275004\" xlink:href=\"#maac0ae2ca9\" y=\"82.913535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.040572\" xlink:href=\"#maac0ae2ca9\" y=\"82.581044\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.587371\" xlink:href=\"#maac0ae2ca9\" y=\"85.800473\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.998421\" xlink:href=\"#maac0ae2ca9\" y=\"98.514443\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"152.683682\" xlink:href=\"#maac0ae2ca9\" y=\"101.678235\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.776961\" xlink:href=\"#maac0ae2ca9\" y=\"60.301638\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.645436\" xlink:href=\"#maac0ae2ca9\" y=\"58.567936\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.252002\" xlink:href=\"#maac0ae2ca9\" y=\"55.860899\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.909476\" xlink:href=\"#maac0ae2ca9\" y=\"80.068226\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"51.289679\" xlink:href=\"#maac0ae2ca9\" y=\"21.678396\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.054996\" xlink:href=\"#maac0ae2ca9\" y=\"96.124138\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.439441\" xlink:href=\"#maac0ae2ca9\" y=\"40.164878\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.590222\" xlink:href=\"#maac0ae2ca9\" y=\"77.282335\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.541307\" xlink:href=\"#maac0ae2ca9\" y=\"65.348521\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.339171\" xlink:href=\"#maac0ae2ca9\" y=\"77.861242\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.942391\" xlink:href=\"#maac0ae2ca9\" y=\"66.988967\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"57.750812\" xlink:href=\"#maac0ae2ca9\" y=\"53.297478\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.246689\" xlink:href=\"#maac0ae2ca9\" y=\"100.126441\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.032976\" xlink:href=\"#maac0ae2ca9\" y=\"80.40555\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.552502\" xlink:href=\"#maac0ae2ca9\" y=\"50.528914\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.201461\" xlink:href=\"#maac0ae2ca9\" y=\"83.829731\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.78262\" xlink:href=\"#maac0ae2ca9\" y=\"72.316579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.16129\" xlink:href=\"#maac0ae2ca9\" y=\"55.644871\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.600153\" xlink:href=\"#maac0ae2ca9\" y=\"106.52637\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.279498\" xlink:href=\"#maac0ae2ca9\" y=\"58.780526\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.881183\" xlink:href=\"#maac0ae2ca9\" y=\"57.504484\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.384184\" xlink:href=\"#maac0ae2ca9\" y=\"92.23927\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"82.048242\" xlink:href=\"#maac0ae2ca9\" y=\"57.184387\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.384677\" xlink:href=\"#maac0ae2ca9\" y=\"63.767766\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.212799\" xlink:href=\"#maac0ae2ca9\" y=\"92.309496\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.406346\" xlink:href=\"#maac0ae2ca9\" y=\"81.218336\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.464474\" xlink:href=\"#maac0ae2ca9\" y=\"64.137116\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.964313\" xlink:href=\"#maac0ae2ca9\" y=\"82.619525\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.806329\" xlink:href=\"#maac0ae2ca9\" y=\"38.599374\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.096002\" xlink:href=\"#maac0ae2ca9\" y=\"79.177062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"65.790708\" xlink:href=\"#maac0ae2ca9\" y=\"48.197448\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.484276\" xlink:href=\"#maac0ae2ca9\" y=\"83.25251\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.927279\" xlink:href=\"#maac0ae2ca9\" y=\"85.586861\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.20466\" xlink:href=\"#maac0ae2ca9\" y=\"77.110717\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.164518\" xlink:href=\"#maac0ae2ca9\" y=\"90.815896\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.456093\" xlink:href=\"#maac0ae2ca9\" y=\"63.819566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.348944\" xlink:href=\"#maac0ae2ca9\" y=\"69.593295\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.230651\" xlink:href=\"#maac0ae2ca9\" y=\"62.068175\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.250484\" xlink:href=\"#maac0ae2ca9\" y=\"74.729092\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.198722\" xlink:href=\"#maac0ae2ca9\" y=\"60.839529\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.623305\" xlink:href=\"#maac0ae2ca9\" y=\"99.105827\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.18516\" xlink:href=\"#maac0ae2ca9\" y=\"52.431691\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.065748\" xlink:href=\"#maac0ae2ca9\" y=\"86.073659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.974841\" xlink:href=\"#maac0ae2ca9\" y=\"77.628064\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.421025\" xlink:href=\"#maac0ae2ca9\" y=\"62.421178\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.956501\" xlink:href=\"#maac0ae2ca9\" y=\"63.884079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.755337\" xlink:href=\"#maac0ae2ca9\" y=\"81.233699\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.199001\" xlink:href=\"#maac0ae2ca9\" y=\"88.123885\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.843843\" xlink:href=\"#maac0ae2ca9\" y=\"81.841177\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.529257\" xlink:href=\"#maac0ae2ca9\" y=\"85.337782\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.511835\" xlink:href=\"#maac0ae2ca9\" y=\"70.384457\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.6442\" xlink:href=\"#maac0ae2ca9\" y=\"72.2961\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"74.269658\" xlink:href=\"#maac0ae2ca9\" y=\"49.935261\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.105777\" xlink:href=\"#maac0ae2ca9\" y=\"81.724004\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"74.794472\" xlink:href=\"#maac0ae2ca9\" y=\"53.282762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.808523\" xlink:href=\"#maac0ae2ca9\" y=\"79.187049\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.465365\" xlink:href=\"#maac0ae2ca9\" y=\"51.483305\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.114698\" xlink:href=\"#maac0ae2ca9\" y=\"65.801737\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.13464\" xlink:href=\"#maac0ae2ca9\" y=\"86.050207\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.693687\" xlink:href=\"#maac0ae2ca9\" y=\"82.858708\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.454839\" xlink:href=\"#maac0ae2ca9\" y=\"85.381908\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.986878\" xlink:href=\"#maac0ae2ca9\" y=\"99.260937\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.497006\" xlink:href=\"#maac0ae2ca9\" y=\"72.870829\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.425763\" xlink:href=\"#maac0ae2ca9\" y=\"78.820873\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.321501\" xlink:href=\"#maac0ae2ca9\" y=\"86.652812\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.336939\" xlink:href=\"#maac0ae2ca9\" y=\"82.25003\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.185501\" xlink:href=\"#maac0ae2ca9\" y=\"64.080934\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"53.959825\" xlink:href=\"#maac0ae2ca9\" y=\"13.377273\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.348715\" xlink:href=\"#maac0ae2ca9\" y=\"83.010053\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.589058\" xlink:href=\"#maac0ae2ca9\" y=\"94.21913\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.099084\" xlink:href=\"#maac0ae2ca9\" y=\"101.577725\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.52212\" xlink:href=\"#maac0ae2ca9\" y=\"84.922428\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"163.101688\" xlink:href=\"#maac0ae2ca9\" y=\"93.419148\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.814028\" xlink:href=\"#maac0ae2ca9\" y=\"93.624878\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"164.977032\" xlink:href=\"#maac0ae2ca9\" y=\"90.1724\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.276356\" xlink:href=\"#maac0ae2ca9\" y=\"96.495971\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.432242\" xlink:href=\"#maac0ae2ca9\" y=\"64.889774\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.809855\" xlink:href=\"#maac0ae2ca9\" y=\"97.297534\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"152.546926\" xlink:href=\"#maac0ae2ca9\" y=\"88.234445\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.25164\" xlink:href=\"#maac0ae2ca9\" y=\"90.242968\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.09723\" xlink:href=\"#maac0ae2ca9\" y=\"84.087771\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.78576\" xlink:href=\"#maac0ae2ca9\" y=\"91.049786\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.444164\" xlink:href=\"#maac0ae2ca9\" y=\"56.260769\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.40393\" xlink:href=\"#maac0ae2ca9\" y=\"86.196849\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.716986\" xlink:href=\"#maac0ae2ca9\" y=\"109.032397\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"74.668053\" xlink:href=\"#maac0ae2ca9\" y=\"87.269674\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.167848\" xlink:href=\"#maac0ae2ca9\" y=\"57.787038\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.830073\" xlink:href=\"#maac0ae2ca9\" y=\"96.032355\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.312846\" xlink:href=\"#maac0ae2ca9\" y=\"59.001757\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.46321\" xlink:href=\"#maac0ae2ca9\" y=\"75.941975\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"160.410431\" xlink:href=\"#maac0ae2ca9\" y=\"96.364125\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.885411\" xlink:href=\"#maac0ae2ca9\" y=\"68.978821\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.70642\" xlink:href=\"#maac0ae2ca9\" y=\"90.112487\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.513901\" xlink:href=\"#maac0ae2ca9\" y=\"72.261875\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.297439\" xlink:href=\"#maac0ae2ca9\" y=\"65.064622\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.120896\" xlink:href=\"#maac0ae2ca9\" y=\"92.092877\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.513534\" xlink:href=\"#maac0ae2ca9\" y=\"82.48564\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.881442\" xlink:href=\"#maac0ae2ca9\" y=\"60.634121\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.354605\" xlink:href=\"#maac0ae2ca9\" y=\"94.855358\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"45.064951\" xlink:href=\"#maac0ae2ca9\" y=\"22.376492\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.377063\" xlink:href=\"#maac0ae2ca9\" y=\"72.992958\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"184.355147\" xlink:href=\"#maac0ae2ca9\" y=\"109.996054\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.212616\" xlink:href=\"#maac0ae2ca9\" y=\"90.106659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.169803\" xlink:href=\"#maac0ae2ca9\" y=\"89.689104\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.379202\" xlink:href=\"#maac0ae2ca9\" y=\"63.27592\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.564619\" xlink:href=\"#maac0ae2ca9\" y=\"57.995979\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.55558\" xlink:href=\"#maac0ae2ca9\" y=\"52.74761\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.670524\" xlink:href=\"#maac0ae2ca9\" y=\"49.169418\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.462359\" xlink:href=\"#maac0ae2ca9\" y=\"74.760224\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.135582\" xlink:href=\"#maac0ae2ca9\" y=\"66.947226\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.293325\" xlink:href=\"#maac0ae2ca9\" y=\"43.028305\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.900396\" xlink:href=\"#maac0ae2ca9\" y=\"48.316554\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.530672\" xlink:href=\"#maac0ae2ca9\" y=\"98.334855\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"76.142563\" xlink:href=\"#maac0ae2ca9\" y=\"69.672203\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.952279\" xlink:href=\"#maac0ae2ca9\" y=\"71.556754\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.426446\" xlink:href=\"#maac0ae2ca9\" y=\"92.911405\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.321906\" xlink:href=\"#maac0ae2ca9\" y=\"83.208534\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.308688\" xlink:href=\"#maac0ae2ca9\" y=\"70.699873\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.433812\" xlink:href=\"#maac0ae2ca9\" y=\"79.53889\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.353913\" xlink:href=\"#maac0ae2ca9\" y=\"92.614691\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.048\" xlink:href=\"#maac0ae2ca9\" y=\"63.946954\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"72.122657\" xlink:href=\"#maac0ae2ca9\" y=\"50.663689\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.042314\" xlink:href=\"#maac0ae2ca9\" y=\"84.77771\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.91949\" xlink:href=\"#maac0ae2ca9\" y=\"60.98154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.575309\" xlink:href=\"#maac0ae2ca9\" y=\"90.092638\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"55.319926\" xlink:href=\"#maac0ae2ca9\" y=\"40.309748\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.026728\" xlink:href=\"#maac0ae2ca9\" y=\"80.674958\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.717435\" xlink:href=\"#maac0ae2ca9\" y=\"65.912943\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.008649\" xlink:href=\"#maac0ae2ca9\" y=\"98.786632\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.947573\" xlink:href=\"#maac0ae2ca9\" y=\"69.538328\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.996865\" xlink:href=\"#maac0ae2ca9\" y=\"80.414399\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.762894\" xlink:href=\"#maac0ae2ca9\" y=\"39.539049\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.005612\" xlink:href=\"#maac0ae2ca9\" y=\"36.710591\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.347875\" xlink:href=\"#maac0ae2ca9\" y=\"65.505791\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"164.620824\" xlink:href=\"#maac0ae2ca9\" y=\"101.780661\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.493991\" xlink:href=\"#maac0ae2ca9\" y=\"72.298504\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"45.895059\" xlink:href=\"#maac0ae2ca9\" y=\"22.079893\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.698546\" xlink:href=\"#maac0ae2ca9\" y=\"70.64878\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.495549\" xlink:href=\"#maac0ae2ca9\" y=\"48.893144\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.097922\" xlink:href=\"#maac0ae2ca9\" y=\"53.02781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"176.872708\" xlink:href=\"#maac0ae2ca9\" y=\"102.882046\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.068268\" xlink:href=\"#maac0ae2ca9\" y=\"99.367223\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.215083\" xlink:href=\"#maac0ae2ca9\" y=\"88.206731\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.256894\" xlink:href=\"#maac0ae2ca9\" y=\"80.637689\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.359903\" xlink:href=\"#maac0ae2ca9\" y=\"58.129713\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.924558\" xlink:href=\"#maac0ae2ca9\" y=\"87.906537\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"179.069049\" xlink:href=\"#maac0ae2ca9\" y=\"122.285763\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.441107\" xlink:href=\"#maac0ae2ca9\" y=\"68.100053\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.707981\" xlink:href=\"#maac0ae2ca9\" y=\"84.227624\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"73.573252\" xlink:href=\"#maac0ae2ca9\" y=\"49.404002\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.013306\" xlink:href=\"#maac0ae2ca9\" y=\"98.561459\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.856597\" xlink:href=\"#maac0ae2ca9\" y=\"88.152167\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.663972\" xlink:href=\"#maac0ae2ca9\" y=\"111.917347\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"60.182598\" xlink:href=\"#maac0ae2ca9\" y=\"34.321819\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.113357\" xlink:href=\"#maac0ae2ca9\" y=\"79.523599\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.358519\" xlink:href=\"#maac0ae2ca9\" y=\"60.618036\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.018773\" xlink:href=\"#maac0ae2ca9\" y=\"104.982546\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.951237\" xlink:href=\"#maac0ae2ca9\" y=\"77.67619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.101116\" xlink:href=\"#maac0ae2ca9\" y=\"54.78749\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.218074\" xlink:href=\"#maac0ae2ca9\" y=\"59.925941\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.108174\" xlink:href=\"#maac0ae2ca9\" y=\"85.673375\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.845894\" xlink:href=\"#maac0ae2ca9\" y=\"64.564823\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"162.548512\" xlink:href=\"#maac0ae2ca9\" y=\"112.447742\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.38084\" xlink:href=\"#maac0ae2ca9\" y=\"63.619118\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.186401\" xlink:href=\"#maac0ae2ca9\" y=\"107.938925\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.397149\" xlink:href=\"#maac0ae2ca9\" y=\"52.214795\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.694731\" xlink:href=\"#maac0ae2ca9\" y=\"79.019753\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.112444\" xlink:href=\"#maac0ae2ca9\" y=\"91.231332\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.781105\" xlink:href=\"#maac0ae2ca9\" y=\"94.88223\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.190641\" xlink:href=\"#maac0ae2ca9\" y=\"61.418\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.18222\" xlink:href=\"#maac0ae2ca9\" y=\"65.104768\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.434288\" xlink:href=\"#maac0ae2ca9\" y=\"59.271474\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"51.503625\" xlink:href=\"#maac0ae2ca9\" y=\"43.364795\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.18983\" xlink:href=\"#maac0ae2ca9\" y=\"85.149473\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.339818\" xlink:href=\"#maac0ae2ca9\" y=\"97.081465\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.49129\" xlink:href=\"#maac0ae2ca9\" y=\"82.523517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.805185\" xlink:href=\"#maac0ae2ca9\" y=\"82.118865\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.539846\" xlink:href=\"#maac0ae2ca9\" y=\"82.703559\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.452884\" xlink:href=\"#maac0ae2ca9\" y=\"115.428538\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.558935\" xlink:href=\"#maac0ae2ca9\" y=\"81.400765\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.651908\" xlink:href=\"#maac0ae2ca9\" y=\"84.284872\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.911042\" xlink:href=\"#maac0ae2ca9\" y=\"42.039993\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"64.009121\" xlink:href=\"#maac0ae2ca9\" y=\"39.069861\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.498863\" xlink:href=\"#maac0ae2ca9\" y=\"68.142842\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.029896\" xlink:href=\"#maac0ae2ca9\" y=\"109.051939\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.948377\" xlink:href=\"#maac0ae2ca9\" y=\"60.210855\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.429326\" xlink:href=\"#maac0ae2ca9\" y=\"97.142827\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.363504\" xlink:href=\"#maac0ae2ca9\" y=\"74.521504\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.429171\" xlink:href=\"#maac0ae2ca9\" y=\"55.242183\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.884668\" xlink:href=\"#maac0ae2ca9\" y=\"79.1234\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.887325\" xlink:href=\"#maac0ae2ca9\" y=\"74.81292\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.886609\" xlink:href=\"#maac0ae2ca9\" y=\"72.067846\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.613235\" xlink:href=\"#maac0ae2ca9\" y=\"90.046875\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.894271\" xlink:href=\"#maac0ae2ca9\" y=\"84.963765\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.290951\" xlink:href=\"#maac0ae2ca9\" y=\"88.313534\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.457698\" xlink:href=\"#maac0ae2ca9\" y=\"54.998681\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.606409\" xlink:href=\"#maac0ae2ca9\" y=\"50.059209\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"162.823663\" xlink:href=\"#maac0ae2ca9\" y=\"107.050664\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.322941\" xlink:href=\"#maac0ae2ca9\" y=\"78.244447\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.528896\" xlink:href=\"#maac0ae2ca9\" y=\"83.602964\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.270451\" xlink:href=\"#maac0ae2ca9\" y=\"45.631592\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.696758\" xlink:href=\"#maac0ae2ca9\" y=\"46.98885\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"82.476325\" xlink:href=\"#maac0ae2ca9\" y=\"29.702089\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.603818\" xlink:href=\"#maac0ae2ca9\" y=\"82.250243\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"53.325412\" xlink:href=\"#maac0ae2ca9\" y=\"28.463633\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.582293\" xlink:href=\"#maac0ae2ca9\" y=\"95.436668\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.241707\" xlink:href=\"#maac0ae2ca9\" y=\"66.143207\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.692807\" xlink:href=\"#maac0ae2ca9\" y=\"59.049784\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.750843\" xlink:href=\"#maac0ae2ca9\" y=\"65.533355\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.210298\" xlink:href=\"#maac0ae2ca9\" y=\"104.157064\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.716896\" xlink:href=\"#maac0ae2ca9\" y=\"103.162716\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.544012\" xlink:href=\"#maac0ae2ca9\" y=\"87.11175\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.175816\" xlink:href=\"#maac0ae2ca9\" y=\"55.078896\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"60.822771\" xlink:href=\"#maac0ae2ca9\" y=\"37.22769\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.108631\" xlink:href=\"#maac0ae2ca9\" y=\"94.303358\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.931432\" xlink:href=\"#maac0ae2ca9\" y=\"68.804411\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.358494\" xlink:href=\"#maac0ae2ca9\" y=\"48.654932\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.571192\" xlink:href=\"#maac0ae2ca9\" y=\"91.148999\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.999137\" xlink:href=\"#maac0ae2ca9\" y=\"85.394311\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.165864\" xlink:href=\"#maac0ae2ca9\" y=\"65.925778\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.570951\" xlink:href=\"#maac0ae2ca9\" y=\"64.970377\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.503333\" xlink:href=\"#maac0ae2ca9\" y=\"93.790258\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.528207\" xlink:href=\"#maac0ae2ca9\" y=\"73.670304\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.386959\" xlink:href=\"#maac0ae2ca9\" y=\"59.131514\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.801038\" xlink:href=\"#maac0ae2ca9\" y=\"78.711931\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.977737\" xlink:href=\"#maac0ae2ca9\" y=\"67.412713\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.928231\" xlink:href=\"#maac0ae2ca9\" y=\"62.769264\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"170.802471\" xlink:href=\"#maac0ae2ca9\" y=\"103.368925\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.345675\" xlink:href=\"#maac0ae2ca9\" y=\"55.984868\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.688981\" xlink:href=\"#maac0ae2ca9\" y=\"84.992252\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.356247\" xlink:href=\"#maac0ae2ca9\" y=\"89.96325\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.678738\" xlink:href=\"#maac0ae2ca9\" y=\"52.397851\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.747756\" xlink:href=\"#maac0ae2ca9\" y=\"107.242565\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.049371\" xlink:href=\"#maac0ae2ca9\" y=\"51.122662\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.585772\" xlink:href=\"#maac0ae2ca9\" y=\"108.901018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.06801\" xlink:href=\"#maac0ae2ca9\" y=\"62.58157\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.055304\" xlink:href=\"#maac0ae2ca9\" y=\"71.631416\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.109406\" xlink:href=\"#maac0ae2ca9\" y=\"52.480342\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"53.88761\" xlink:href=\"#maac0ae2ca9\" y=\"51.557051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.681561\" xlink:href=\"#maac0ae2ca9\" y=\"50.947744\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.649984\" xlink:href=\"#maac0ae2ca9\" y=\"54.063651\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"195.895079\" xlink:href=\"#maac0ae2ca9\" y=\"125.089784\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.092898\" xlink:href=\"#maac0ae2ca9\" y=\"50.09793\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.65845\" xlink:href=\"#maac0ae2ca9\" y=\"80.203953\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.503376\" xlink:href=\"#maac0ae2ca9\" y=\"93.266076\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.762214\" xlink:href=\"#maac0ae2ca9\" y=\"62.860089\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.446264\" xlink:href=\"#maac0ae2ca9\" y=\"64.165648\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.131257\" xlink:href=\"#maac0ae2ca9\" y=\"70.736189\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.159917\" xlink:href=\"#maac0ae2ca9\" y=\"69.006886\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.52988\" xlink:href=\"#maac0ae2ca9\" y=\"55.789435\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"76.270671\" xlink:href=\"#maac0ae2ca9\" y=\"53.691325\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.593995\" xlink:href=\"#maac0ae2ca9\" y=\"67.049172\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.03039\" xlink:href=\"#maac0ae2ca9\" y=\"85.110929\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.543133\" xlink:href=\"#maac0ae2ca9\" y=\"93.047482\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.533194\" xlink:href=\"#maac0ae2ca9\" y=\"77.777535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.196084\" xlink:href=\"#maac0ae2ca9\" y=\"79.814999\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.912359\" xlink:href=\"#maac0ae2ca9\" y=\"65.97725\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.266364\" xlink:href=\"#maac0ae2ca9\" y=\"33.323383\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.952263\" xlink:href=\"#maac0ae2ca9\" y=\"91.27175\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.087435\" xlink:href=\"#maac0ae2ca9\" y=\"70.498067\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.957785\" xlink:href=\"#maac0ae2ca9\" y=\"74.269025\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.131995\" xlink:href=\"#maac0ae2ca9\" y=\"104.336743\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.375425\" xlink:href=\"#maac0ae2ca9\" y=\"92.796951\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"196.045307\" xlink:href=\"#maac0ae2ca9\" y=\"111.031851\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.416656\" xlink:href=\"#maac0ae2ca9\" y=\"71.069692\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.120153\" xlink:href=\"#maac0ae2ca9\" y=\"72.90447\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.318331\" xlink:href=\"#maac0ae2ca9\" y=\"61.368135\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.610857\" xlink:href=\"#maac0ae2ca9\" y=\"64.367577\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.357005\" xlink:href=\"#maac0ae2ca9\" y=\"74.268222\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.345231\" xlink:href=\"#maac0ae2ca9\" y=\"51.627578\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.915857\" xlink:href=\"#maac0ae2ca9\" y=\"94.274499\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.892493\" xlink:href=\"#maac0ae2ca9\" y=\"81.085745\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.343841\" xlink:href=\"#maac0ae2ca9\" y=\"69.738856\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.216691\" xlink:href=\"#maac0ae2ca9\" y=\"79.994087\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.591404\" xlink:href=\"#maac0ae2ca9\" y=\"68.559232\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.896637\" xlink:href=\"#maac0ae2ca9\" y=\"77.949414\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"68.895403\" xlink:href=\"#maac0ae2ca9\" y=\"48.025693\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.35178\" xlink:href=\"#maac0ae2ca9\" y=\"41.118462\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.943937\" xlink:href=\"#maac0ae2ca9\" y=\"66.28068\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.02408\" xlink:href=\"#maac0ae2ca9\" y=\"85.677467\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.630135\" xlink:href=\"#maac0ae2ca9\" y=\"83.674168\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.111221\" xlink:href=\"#maac0ae2ca9\" y=\"59.238769\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.187254\" xlink:href=\"#maac0ae2ca9\" y=\"92.729567\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.103453\" xlink:href=\"#maac0ae2ca9\" y=\"103.696153\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.248978\" xlink:href=\"#maac0ae2ca9\" y=\"43.927263\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.880434\" xlink:href=\"#maac0ae2ca9\" y=\"70.546993\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.822092\" xlink:href=\"#maac0ae2ca9\" y=\"79.485134\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.560359\" xlink:href=\"#maac0ae2ca9\" y=\"78.896902\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"59.835834\" xlink:href=\"#maac0ae2ca9\" y=\"42.128863\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.76094\" xlink:href=\"#maac0ae2ca9\" y=\"69.871062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.098244\" xlink:href=\"#maac0ae2ca9\" y=\"60.681753\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.154687\" xlink:href=\"#maac0ae2ca9\" y=\"72.130378\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.177008\" xlink:href=\"#maac0ae2ca9\" y=\"81.302627\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.567658\" xlink:href=\"#maac0ae2ca9\" y=\"84.938402\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.133578\" xlink:href=\"#maac0ae2ca9\" y=\"66.246033\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.55887\" xlink:href=\"#maac0ae2ca9\" y=\"71.775186\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.71255\" xlink:href=\"#maac0ae2ca9\" y=\"81.851258\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.890487\" xlink:href=\"#maac0ae2ca9\" y=\"77.989213\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.842281\" xlink:href=\"#maac0ae2ca9\" y=\"75.03628\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.409194\" xlink:href=\"#maac0ae2ca9\" y=\"46.948323\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.89024\" xlink:href=\"#maac0ae2ca9\" y=\"74.278084\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.977327\" xlink:href=\"#maac0ae2ca9\" y=\"67.239874\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"74.696157\" xlink:href=\"#maac0ae2ca9\" y=\"61.864695\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.708033\" xlink:href=\"#maac0ae2ca9\" y=\"80.656396\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.735466\" xlink:href=\"#maac0ae2ca9\" y=\"65.725506\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"76.751643\" xlink:href=\"#maac0ae2ca9\" y=\"34.655706\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.012137\" xlink:href=\"#maac0ae2ca9\" y=\"41.393042\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.886687\" xlink:href=\"#maac0ae2ca9\" y=\"86.781283\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.677832\" xlink:href=\"#maac0ae2ca9\" y=\"66.83493\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.543309\" xlink:href=\"#maac0ae2ca9\" y=\"96.746368\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.358947\" xlink:href=\"#maac0ae2ca9\" y=\"81.587731\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.891988\" xlink:href=\"#maac0ae2ca9\" y=\"48.013378\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.02719\" xlink:href=\"#maac0ae2ca9\" y=\"71.87069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.129884\" xlink:href=\"#maac0ae2ca9\" y=\"92.048811\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.127692\" xlink:href=\"#maac0ae2ca9\" y=\"69.478037\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.257065\" xlink:href=\"#maac0ae2ca9\" y=\"83.986215\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.777489\" xlink:href=\"#maac0ae2ca9\" y=\"105.243109\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.463614\" xlink:href=\"#maac0ae2ca9\" y=\"87.757329\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.932252\" xlink:href=\"#maac0ae2ca9\" y=\"61.202152\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.773034\" xlink:href=\"#maac0ae2ca9\" y=\"76.584199\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.098645\" xlink:href=\"#maac0ae2ca9\" y=\"81.170824\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.488235\" xlink:href=\"#maac0ae2ca9\" y=\"37.841948\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.758228\" xlink:href=\"#maac0ae2ca9\" y=\"63.994977\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.503068\" xlink:href=\"#maac0ae2ca9\" y=\"70.787951\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.873814\" xlink:href=\"#maac0ae2ca9\" y=\"75.612468\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.129173\" xlink:href=\"#maac0ae2ca9\" y=\"73.876697\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.475773\" xlink:href=\"#maac0ae2ca9\" y=\"60.11372\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.501353\" xlink:href=\"#maac0ae2ca9\" y=\"70.2254\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.396659\" xlink:href=\"#maac0ae2ca9\" y=\"91.737892\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.33641\" xlink:href=\"#maac0ae2ca9\" y=\"83.521693\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.108149\" xlink:href=\"#maac0ae2ca9\" y=\"52.081771\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.296953\" xlink:href=\"#maac0ae2ca9\" y=\"77.557538\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.427945\" xlink:href=\"#maac0ae2ca9\" y=\"86.684659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"182.773477\" xlink:href=\"#maac0ae2ca9\" y=\"112.350287\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.066463\" xlink:href=\"#maac0ae2ca9\" y=\"76.097891\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.489982\" xlink:href=\"#maac0ae2ca9\" y=\"84.437386\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.956862\" xlink:href=\"#maac0ae2ca9\" y=\"83.394468\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.139658\" xlink:href=\"#maac0ae2ca9\" y=\"64.002929\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.478452\" xlink:href=\"#maac0ae2ca9\" y=\"84.751546\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.025245\" xlink:href=\"#maac0ae2ca9\" y=\"74.0047\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.190865\" xlink:href=\"#maac0ae2ca9\" y=\"84.481097\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.7614\" xlink:href=\"#maac0ae2ca9\" y=\"63.105835\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.906111\" xlink:href=\"#maac0ae2ca9\" y=\"63.220291\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.311703\" xlink:href=\"#maac0ae2ca9\" y=\"89.039422\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.785186\" xlink:href=\"#maac0ae2ca9\" y=\"86.981051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.717114\" xlink:href=\"#maac0ae2ca9\" y=\"44.67732\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.504628\" xlink:href=\"#maac0ae2ca9\" y=\"93.111161\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.369843\" xlink:href=\"#maac0ae2ca9\" y=\"68.658108\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.21405\" xlink:href=\"#maac0ae2ca9\" y=\"68.411497\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.546325\" xlink:href=\"#maac0ae2ca9\" y=\"90.277392\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.579449\" xlink:href=\"#maac0ae2ca9\" y=\"75.52285\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.362538\" xlink:href=\"#maac0ae2ca9\" y=\"45.901729\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.907451\" xlink:href=\"#maac0ae2ca9\" y=\"76.380775\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.536768\" xlink:href=\"#maac0ae2ca9\" y=\"81.10747\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.112303\" xlink:href=\"#maac0ae2ca9\" y=\"61.472574\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.590914\" xlink:href=\"#maac0ae2ca9\" y=\"46.362139\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.360376\" xlink:href=\"#maac0ae2ca9\" y=\"72.167405\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.479892\" xlink:href=\"#maac0ae2ca9\" y=\"77.575569\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.076584\" xlink:href=\"#maac0ae2ca9\" y=\"58.925819\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.109993\" xlink:href=\"#maac0ae2ca9\" y=\"70.567543\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.860208\" xlink:href=\"#maac0ae2ca9\" y=\"55.047097\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.188207\" xlink:href=\"#maac0ae2ca9\" y=\"81.029064\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.84392\" xlink:href=\"#maac0ae2ca9\" y=\"99.594499\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.214233\" xlink:href=\"#maac0ae2ca9\" y=\"79.621485\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.014237\" xlink:href=\"#maac0ae2ca9\" y=\"52.721162\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.305572\" xlink:href=\"#maac0ae2ca9\" y=\"95.670259\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.343631\" xlink:href=\"#maac0ae2ca9\" y=\"76.695193\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.898438\" xlink:href=\"#maac0ae2ca9\" y=\"71.287252\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.618603\" xlink:href=\"#maac0ae2ca9\" y=\"51.231658\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.653691\" xlink:href=\"#maac0ae2ca9\" y=\"90.052574\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.712436\" xlink:href=\"#maac0ae2ca9\" y=\"65.054801\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.588577\" xlink:href=\"#maac0ae2ca9\" y=\"72.2737\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.522022\" xlink:href=\"#maac0ae2ca9\" y=\"80.092079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.916573\" xlink:href=\"#maac0ae2ca9\" y=\"75.99675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.349491\" xlink:href=\"#maac0ae2ca9\" y=\"49.425589\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.486869\" xlink:href=\"#maac0ae2ca9\" y=\"100.107659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.547391\" xlink:href=\"#maac0ae2ca9\" y=\"79.396578\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.437642\" xlink:href=\"#maac0ae2ca9\" y=\"87.947561\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.153458\" xlink:href=\"#maac0ae2ca9\" y=\"53.940113\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.475458\" xlink:href=\"#maac0ae2ca9\" y=\"99.020796\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.710303\" xlink:href=\"#maac0ae2ca9\" y=\"60.991376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.071023\" xlink:href=\"#maac0ae2ca9\" y=\"58.876775\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.248503\" xlink:href=\"#maac0ae2ca9\" y=\"84.44703\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.93853\" xlink:href=\"#maac0ae2ca9\" y=\"89.844431\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.974803\" xlink:href=\"#maac0ae2ca9\" y=\"77.065624\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"52.183457\" xlink:href=\"#maac0ae2ca9\" y=\"47.222936\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"65.798423\" xlink:href=\"#maac0ae2ca9\" y=\"40.456307\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.974238\" xlink:href=\"#maac0ae2ca9\" y=\"58.648837\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.983521\" xlink:href=\"#maac0ae2ca9\" y=\"91.795459\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"82.649722\" xlink:href=\"#maac0ae2ca9\" y=\"56.180874\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.934442\" xlink:href=\"#maac0ae2ca9\" y=\"46.527131\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.272443\" xlink:href=\"#maac0ae2ca9\" y=\"87.774124\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.760674\" xlink:href=\"#maac0ae2ca9\" y=\"94.724139\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.980066\" xlink:href=\"#maac0ae2ca9\" y=\"54.459407\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.453215\" xlink:href=\"#maac0ae2ca9\" y=\"83.65762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"63.899111\" xlink:href=\"#maac0ae2ca9\" y=\"44.642095\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.12053\" xlink:href=\"#maac0ae2ca9\" y=\"67.1137\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.398424\" xlink:href=\"#maac0ae2ca9\" y=\"61.303484\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"101.108013\" xlink:href=\"#maac0ae2ca9\" y=\"54.356472\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.29523\" xlink:href=\"#maac0ae2ca9\" y=\"88.644533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.499991\" xlink:href=\"#maac0ae2ca9\" y=\"62.187898\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.415808\" xlink:href=\"#maac0ae2ca9\" y=\"40.779117\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.04672\" xlink:href=\"#maac0ae2ca9\" y=\"76.342119\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.134823\" xlink:href=\"#maac0ae2ca9\" y=\"70.550903\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.425616\" xlink:href=\"#maac0ae2ca9\" y=\"48.718137\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.774303\" xlink:href=\"#maac0ae2ca9\" y=\"68.296621\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.357127\" xlink:href=\"#maac0ae2ca9\" y=\"73.074001\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.939256\" xlink:href=\"#maac0ae2ca9\" y=\"79.558387\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"171.022067\" xlink:href=\"#maac0ae2ca9\" y=\"110.444814\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.03209\" xlink:href=\"#maac0ae2ca9\" y=\"91.087332\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.409222\" xlink:href=\"#maac0ae2ca9\" y=\"68.465519\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.502502\" xlink:href=\"#maac0ae2ca9\" y=\"54.405588\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.372917\" xlink:href=\"#maac0ae2ca9\" y=\"84.147891\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.545254\" xlink:href=\"#maac0ae2ca9\" y=\"106.16413\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.874504\" xlink:href=\"#maac0ae2ca9\" y=\"59.195019\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.899919\" xlink:href=\"#maac0ae2ca9\" y=\"89.208446\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.849688\" xlink:href=\"#maac0ae2ca9\" y=\"67.797885\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.520437\" xlink:href=\"#maac0ae2ca9\" y=\"66.235517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.934827\" xlink:href=\"#maac0ae2ca9\" y=\"59.538149\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.568141\" xlink:href=\"#maac0ae2ca9\" y=\"86.778175\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.131597\" xlink:href=\"#maac0ae2ca9\" y=\"136.922727\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.535023\" xlink:href=\"#maac0ae2ca9\" y=\"84.812622\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.311734\" xlink:href=\"#maac0ae2ca9\" y=\"76.487539\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.47251\" xlink:href=\"#maac0ae2ca9\" y=\"69.69945\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"68.04249\" xlink:href=\"#maac0ae2ca9\" y=\"63.298292\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"82.625651\" xlink:href=\"#maac0ae2ca9\" y=\"54.576697\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"55.261504\" xlink:href=\"#maac0ae2ca9\" y=\"28.769432\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.555597\" xlink:href=\"#maac0ae2ca9\" y=\"66.566808\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.988863\" xlink:href=\"#maac0ae2ca9\" y=\"65.41956\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.19017\" xlink:href=\"#maac0ae2ca9\" y=\"91.416189\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.770999\" xlink:href=\"#maac0ae2ca9\" y=\"84.455154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.189983\" xlink:href=\"#maac0ae2ca9\" y=\"71.956996\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.639201\" xlink:href=\"#maac0ae2ca9\" y=\"94.568598\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.963804\" xlink:href=\"#maac0ae2ca9\" y=\"102.004155\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.364915\" xlink:href=\"#maac0ae2ca9\" y=\"136.522695\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.958638\" xlink:href=\"#maac0ae2ca9\" y=\"59.655562\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"73.254783\" xlink:href=\"#maac0ae2ca9\" y=\"37.41138\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"63.693848\" xlink:href=\"#maac0ae2ca9\" y=\"49.604376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.111975\" xlink:href=\"#maac0ae2ca9\" y=\"79.104755\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.31565\" xlink:href=\"#maac0ae2ca9\" y=\"68.557689\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.446392\" xlink:href=\"#maac0ae2ca9\" y=\"61.653454\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.679072\" xlink:href=\"#maac0ae2ca9\" y=\"93.702009\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.904655\" xlink:href=\"#maac0ae2ca9\" y=\"55.733882\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.631134\" xlink:href=\"#maac0ae2ca9\" y=\"82.731977\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.113784\" xlink:href=\"#maac0ae2ca9\" y=\"74.18581\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.3078\" xlink:href=\"#maac0ae2ca9\" y=\"65.135154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"72.242305\" xlink:href=\"#maac0ae2ca9\" y=\"43.791412\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.319645\" xlink:href=\"#maac0ae2ca9\" y=\"96.98905\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.933958\" xlink:href=\"#maac0ae2ca9\" y=\"64.78897\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.453257\" xlink:href=\"#maac0ae2ca9\" y=\"48.543091\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.856032\" xlink:href=\"#maac0ae2ca9\" y=\"78.211277\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.860472\" xlink:href=\"#maac0ae2ca9\" y=\"55.428828\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.90318\" xlink:href=\"#maac0ae2ca9\" y=\"93.276037\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.7124\" xlink:href=\"#maac0ae2ca9\" y=\"84.476088\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.393915\" xlink:href=\"#maac0ae2ca9\" y=\"71.440381\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.800361\" xlink:href=\"#maac0ae2ca9\" y=\"87.749124\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.180785\" xlink:href=\"#maac0ae2ca9\" y=\"100.586062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.327293\" xlink:href=\"#maac0ae2ca9\" y=\"40.634635\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.099964\" xlink:href=\"#maac0ae2ca9\" y=\"58.236738\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.279353\" xlink:href=\"#maac0ae2ca9\" y=\"96.414154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.074179\" xlink:href=\"#maac0ae2ca9\" y=\"83.668689\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"60.311121\" xlink:href=\"#maac0ae2ca9\" y=\"22.428225\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.514984\" xlink:href=\"#maac0ae2ca9\" y=\"61.567703\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.192257\" xlink:href=\"#maac0ae2ca9\" y=\"70.114567\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"172.273863\" xlink:href=\"#maac0ae2ca9\" y=\"123.128914\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.44451\" xlink:href=\"#maac0ae2ca9\" y=\"66.108326\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.187397\" xlink:href=\"#maac0ae2ca9\" y=\"97.518969\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.873807\" xlink:href=\"#maac0ae2ca9\" y=\"60.042702\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"74.981561\" xlink:href=\"#maac0ae2ca9\" y=\"49.663237\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.236496\" xlink:href=\"#maac0ae2ca9\" y=\"85.961145\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.938762\" xlink:href=\"#maac0ae2ca9\" y=\"87.599675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.494669\" xlink:href=\"#maac0ae2ca9\" y=\"75.674025\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.503036\" xlink:href=\"#maac0ae2ca9\" y=\"90.339804\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.478057\" xlink:href=\"#maac0ae2ca9\" y=\"96.180391\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"82.107782\" xlink:href=\"#maac0ae2ca9\" y=\"56.047833\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.693726\" xlink:href=\"#maac0ae2ca9\" y=\"64.556619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.232401\" xlink:href=\"#maac0ae2ca9\" y=\"83.234491\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.866344\" xlink:href=\"#maac0ae2ca9\" y=\"74.216141\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.657361\" xlink:href=\"#maac0ae2ca9\" y=\"80.412611\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"57.143028\" xlink:href=\"#maac0ae2ca9\" y=\"36.508377\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.083053\" xlink:href=\"#maac0ae2ca9\" y=\"60.110286\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.280821\" xlink:href=\"#maac0ae2ca9\" y=\"71.779101\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.24345\" xlink:href=\"#maac0ae2ca9\" y=\"93.35541\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.814951\" xlink:href=\"#maac0ae2ca9\" y=\"99.287377\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.816413\" xlink:href=\"#maac0ae2ca9\" y=\"48.165637\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.146208\" xlink:href=\"#maac0ae2ca9\" y=\"73.315713\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.582179\" xlink:href=\"#maac0ae2ca9\" y=\"69.333437\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.225763\" xlink:href=\"#maac0ae2ca9\" y=\"77.154288\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.50628\" xlink:href=\"#maac0ae2ca9\" y=\"78.433458\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.41416\" xlink:href=\"#maac0ae2ca9\" y=\"66.608656\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.538942\" xlink:href=\"#maac0ae2ca9\" y=\"64.471892\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.449451\" xlink:href=\"#maac0ae2ca9\" y=\"52.620821\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.477987\" xlink:href=\"#maac0ae2ca9\" y=\"58.504025\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.202775\" xlink:href=\"#maac0ae2ca9\" y=\"61.019839\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.058758\" xlink:href=\"#maac0ae2ca9\" y=\"77.717386\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.030101\" xlink:href=\"#maac0ae2ca9\" y=\"77.431002\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.659545\" xlink:href=\"#maac0ae2ca9\" y=\"85.738297\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.987876\" xlink:href=\"#maac0ae2ca9\" y=\"66.688579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.781704\" xlink:href=\"#maac0ae2ca9\" y=\"33.524876\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.751163\" xlink:href=\"#maac0ae2ca9\" y=\"90.372255\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.316201\" xlink:href=\"#maac0ae2ca9\" y=\"77.447205\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.706245\" xlink:href=\"#maac0ae2ca9\" y=\"63.779707\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.371208\" xlink:href=\"#maac0ae2ca9\" y=\"66.707823\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"48.792487\" xlink:href=\"#maac0ae2ca9\" y=\"37.304628\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"89.905764\" xlink:href=\"#maac0ae2ca9\" y=\"57.849069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.746399\" xlink:href=\"#maac0ae2ca9\" y=\"84.260823\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"37.81946\" xlink:href=\"#maac0ae2ca9\" y=\"20.795893\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.778759\" xlink:href=\"#maac0ae2ca9\" y=\"68.192026\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.113905\" xlink:href=\"#maac0ae2ca9\" y=\"88.042173\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.978582\" xlink:href=\"#maac0ae2ca9\" y=\"80.775482\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.792586\" xlink:href=\"#maac0ae2ca9\" y=\"74.775285\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.848416\" xlink:href=\"#maac0ae2ca9\" y=\"47.983753\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"179.230781\" xlink:href=\"#maac0ae2ca9\" y=\"84.233891\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.634726\" xlink:href=\"#maac0ae2ca9\" y=\"63.03105\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.193628\" xlink:href=\"#maac0ae2ca9\" y=\"66.777513\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.035629\" xlink:href=\"#maac0ae2ca9\" y=\"81.870355\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.99698\" xlink:href=\"#maac0ae2ca9\" y=\"77.85527\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.498254\" xlink:href=\"#maac0ae2ca9\" y=\"63.524233\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.85234\" xlink:href=\"#maac0ae2ca9\" y=\"52.043961\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.8817\" xlink:href=\"#maac0ae2ca9\" y=\"109.584256\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.141948\" xlink:href=\"#maac0ae2ca9\" y=\"68.685488\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.619804\" xlink:href=\"#maac0ae2ca9\" y=\"60.798464\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.973381\" xlink:href=\"#maac0ae2ca9\" y=\"62.097743\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.981536\" xlink:href=\"#maac0ae2ca9\" y=\"84.548987\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.895296\" xlink:href=\"#maac0ae2ca9\" y=\"68.366\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.472475\" xlink:href=\"#maac0ae2ca9\" y=\"61.448274\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.161072\" xlink:href=\"#maac0ae2ca9\" y=\"60.870247\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.053602\" xlink:href=\"#maac0ae2ca9\" y=\"95.157079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.887572\" xlink:href=\"#maac0ae2ca9\" y=\"55.738466\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.950779\" xlink:href=\"#maac0ae2ca9\" y=\"60.855188\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.853564\" xlink:href=\"#maac0ae2ca9\" y=\"66.09651\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.493953\" xlink:href=\"#maac0ae2ca9\" y=\"25.27403\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.383122\" xlink:href=\"#maac0ae2ca9\" y=\"66.31177\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.079817\" xlink:href=\"#maac0ae2ca9\" y=\"67.506066\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"158.203215\" xlink:href=\"#maac0ae2ca9\" y=\"94.249605\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.02283\" xlink:href=\"#maac0ae2ca9\" y=\"85.191084\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"57.485387\" xlink:href=\"#maac0ae2ca9\" y=\"39.332372\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.825423\" xlink:href=\"#maac0ae2ca9\" y=\"85.374786\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.878795\" xlink:href=\"#maac0ae2ca9\" y=\"76.469571\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.664696\" xlink:href=\"#maac0ae2ca9\" y=\"33.851825\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.075392\" xlink:href=\"#maac0ae2ca9\" y=\"50.473458\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.904147\" xlink:href=\"#maac0ae2ca9\" y=\"72.112665\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.098987\" xlink:href=\"#maac0ae2ca9\" y=\"78.193853\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.291211\" xlink:href=\"#maac0ae2ca9\" y=\"50.915051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.57681\" xlink:href=\"#maac0ae2ca9\" y=\"82.632167\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.335017\" xlink:href=\"#maac0ae2ca9\" y=\"96.704233\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.62267\" xlink:href=\"#maac0ae2ca9\" y=\"92.651241\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.018736\" xlink:href=\"#maac0ae2ca9\" y=\"61.891394\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.404655\" xlink:href=\"#maac0ae2ca9\" y=\"94.38613\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.414436\" xlink:href=\"#maac0ae2ca9\" y=\"92.100996\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.242413\" xlink:href=\"#maac0ae2ca9\" y=\"89.694619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.906969\" xlink:href=\"#maac0ae2ca9\" y=\"89.055152\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.647554\" xlink:href=\"#maac0ae2ca9\" y=\"102.384376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.022434\" xlink:href=\"#maac0ae2ca9\" y=\"62.557907\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"174.900928\" xlink:href=\"#maac0ae2ca9\" y=\"100.33932\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.67653\" xlink:href=\"#maac0ae2ca9\" y=\"84.111571\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.409415\" xlink:href=\"#maac0ae2ca9\" y=\"59.953093\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.458391\" xlink:href=\"#maac0ae2ca9\" y=\"47.207358\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.83667\" xlink:href=\"#maac0ae2ca9\" y=\"69.657659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.445541\" xlink:href=\"#maac0ae2ca9\" y=\"60.735627\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.261349\" xlink:href=\"#maac0ae2ca9\" y=\"66.26688\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.937746\" xlink:href=\"#maac0ae2ca9\" y=\"55.196183\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.067428\" xlink:href=\"#maac0ae2ca9\" y=\"45.632071\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.090891\" xlink:href=\"#maac0ae2ca9\" y=\"50.08551\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.427857\" xlink:href=\"#maac0ae2ca9\" y=\"73.846524\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.299787\" xlink:href=\"#maac0ae2ca9\" y=\"86.935781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.441704\" xlink:href=\"#maac0ae2ca9\" y=\"70.391236\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.163145\" xlink:href=\"#maac0ae2ca9\" y=\"42.25348\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"55.050408\" xlink:href=\"#maac0ae2ca9\" y=\"42.591525\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.198864\" xlink:href=\"#maac0ae2ca9\" y=\"55.41201\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.48033\" xlink:href=\"#maac0ae2ca9\" y=\"90.394697\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.895848\" xlink:href=\"#maac0ae2ca9\" y=\"73.164051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.645565\" xlink:href=\"#maac0ae2ca9\" y=\"65.873407\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.909805\" xlink:href=\"#maac0ae2ca9\" y=\"44.100028\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.972187\" xlink:href=\"#maac0ae2ca9\" y=\"73.543466\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"199.517976\" xlink:href=\"#maac0ae2ca9\" y=\"119.149652\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.738401\" xlink:href=\"#maac0ae2ca9\" y=\"68.785625\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.582214\" xlink:href=\"#maac0ae2ca9\" y=\"103.94239\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.864919\" xlink:href=\"#maac0ae2ca9\" y=\"44.139535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.695927\" xlink:href=\"#maac0ae2ca9\" y=\"84.37374\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.362945\" xlink:href=\"#maac0ae2ca9\" y=\"77.143758\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"73.347628\" xlink:href=\"#maac0ae2ca9\" y=\"59.406649\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.22045\" xlink:href=\"#maac0ae2ca9\" y=\"68.05141\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.625171\" xlink:href=\"#maac0ae2ca9\" y=\"50.454461\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.315268\" xlink:href=\"#maac0ae2ca9\" y=\"78.704439\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.648417\" xlink:href=\"#maac0ae2ca9\" y=\"62.496466\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.12957\" xlink:href=\"#maac0ae2ca9\" y=\"75.312536\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.227676\" xlink:href=\"#maac0ae2ca9\" y=\"80.006303\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"48.611429\" xlink:href=\"#maac0ae2ca9\" y=\"50.707113\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.711355\" xlink:href=\"#maac0ae2ca9\" y=\"69.747799\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.496912\" xlink:href=\"#maac0ae2ca9\" y=\"65.295835\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.808036\" xlink:href=\"#maac0ae2ca9\" y=\"95.428648\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"72.298521\" xlink:href=\"#maac0ae2ca9\" y=\"57.013968\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.418491\" xlink:href=\"#maac0ae2ca9\" y=\"61.660146\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.527177\" xlink:href=\"#maac0ae2ca9\" y=\"86.777026\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.048093\" xlink:href=\"#maac0ae2ca9\" y=\"85.259621\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.483567\" xlink:href=\"#maac0ae2ca9\" y=\"76.805316\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"52.46728\" xlink:href=\"#maac0ae2ca9\" y=\"36.394032\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.872463\" xlink:href=\"#maac0ae2ca9\" y=\"80.098037\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.163714\" xlink:href=\"#maac0ae2ca9\" y=\"84.692958\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.972573\" xlink:href=\"#maac0ae2ca9\" y=\"80.757939\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.219126\" xlink:href=\"#maac0ae2ca9\" y=\"65.504923\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.23531\" xlink:href=\"#maac0ae2ca9\" y=\"37.980084\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"73.236889\" xlink:href=\"#maac0ae2ca9\" y=\"34.153835\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.705298\" xlink:href=\"#maac0ae2ca9\" y=\"55.580696\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"166.177084\" xlink:href=\"#maac0ae2ca9\" y=\"94.335948\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.513194\" xlink:href=\"#maac0ae2ca9\" y=\"55.901272\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.132015\" xlink:href=\"#maac0ae2ca9\" y=\"60.372779\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"194.631188\" xlink:href=\"#maac0ae2ca9\" y=\"111.716529\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.736028\" xlink:href=\"#maac0ae2ca9\" y=\"80.430688\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"143.464265\" xlink:href=\"#maac0ae2ca9\" y=\"92.234086\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.070571\" xlink:href=\"#maac0ae2ca9\" y=\"72.205571\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.84038\" xlink:href=\"#maac0ae2ca9\" y=\"40.012685\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.772616\" xlink:href=\"#maac0ae2ca9\" y=\"61.820343\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.611094\" xlink:href=\"#maac0ae2ca9\" y=\"87.13762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.587202\" xlink:href=\"#maac0ae2ca9\" y=\"71.236209\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.427353\" xlink:href=\"#maac0ae2ca9\" y=\"73.344603\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.361908\" xlink:href=\"#maac0ae2ca9\" y=\"60.970342\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.356717\" xlink:href=\"#maac0ae2ca9\" y=\"63.178747\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.451879\" xlink:href=\"#maac0ae2ca9\" y=\"81.245127\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.473138\" xlink:href=\"#maac0ae2ca9\" y=\"36.407102\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.220395\" xlink:href=\"#maac0ae2ca9\" y=\"102.650203\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.903523\" xlink:href=\"#maac0ae2ca9\" y=\"48.060392\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.730584\" xlink:href=\"#maac0ae2ca9\" y=\"58.890579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"176.049699\" xlink:href=\"#maac0ae2ca9\" y=\"113.99097\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"191.01928\" xlink:href=\"#maac0ae2ca9\" y=\"114.398376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.746439\" xlink:href=\"#maac0ae2ca9\" y=\"35.684536\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.135896\" xlink:href=\"#maac0ae2ca9\" y=\"71.196927\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.841337\" xlink:href=\"#maac0ae2ca9\" y=\"53.0394\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.990198\" xlink:href=\"#maac0ae2ca9\" y=\"44.701534\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"41.321072\" xlink:href=\"#maac0ae2ca9\" y=\"22.822917\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.251844\" xlink:href=\"#maac0ae2ca9\" y=\"80.659848\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.147331\" xlink:href=\"#maac0ae2ca9\" y=\"83.394952\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.36269\" xlink:href=\"#maac0ae2ca9\" y=\"65.66145\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.011219\" xlink:href=\"#maac0ae2ca9\" y=\"79.443511\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.2963\" xlink:href=\"#maac0ae2ca9\" y=\"77.437257\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.290529\" xlink:href=\"#maac0ae2ca9\" y=\"80.678066\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.673035\" xlink:href=\"#maac0ae2ca9\" y=\"86.92998\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.045471\" xlink:href=\"#maac0ae2ca9\" y=\"74.620855\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.571614\" xlink:href=\"#maac0ae2ca9\" y=\"73.949746\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"43.91789\" xlink:href=\"#maac0ae2ca9\" y=\"24.355344\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.165799\" xlink:href=\"#maac0ae2ca9\" y=\"69.496679\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.81571\" xlink:href=\"#maac0ae2ca9\" y=\"110.515103\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.030099\" xlink:href=\"#maac0ae2ca9\" y=\"55.817104\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"166.833955\" xlink:href=\"#maac0ae2ca9\" y=\"120.072381\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.453698\" xlink:href=\"#maac0ae2ca9\" y=\"56.151809\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.86437\" xlink:href=\"#maac0ae2ca9\" y=\"97.347617\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.113425\" xlink:href=\"#maac0ae2ca9\" y=\"90.254502\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.892305\" xlink:href=\"#maac0ae2ca9\" y=\"108.614762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"188.335359\" xlink:href=\"#maac0ae2ca9\" y=\"130.557989\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.767099\" xlink:href=\"#maac0ae2ca9\" y=\"52.820764\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.606989\" xlink:href=\"#maac0ae2ca9\" y=\"75.153693\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.223173\" xlink:href=\"#maac0ae2ca9\" y=\"99.55341\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.293505\" xlink:href=\"#maac0ae2ca9\" y=\"88.174096\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.461612\" xlink:href=\"#maac0ae2ca9\" y=\"43.342635\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.425868\" xlink:href=\"#maac0ae2ca9\" y=\"76.786014\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.935849\" xlink:href=\"#maac0ae2ca9\" y=\"76.761494\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.886382\" xlink:href=\"#maac0ae2ca9\" y=\"69.787108\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"60.692368\" xlink:href=\"#maac0ae2ca9\" y=\"40.850208\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"62.623687\" xlink:href=\"#maac0ae2ca9\" y=\"49.161697\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.593475\" xlink:href=\"#maac0ae2ca9\" y=\"73.572401\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.013904\" xlink:href=\"#maac0ae2ca9\" y=\"83.897708\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.173164\" xlink:href=\"#maac0ae2ca9\" y=\"75.893985\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.720251\" xlink:href=\"#maac0ae2ca9\" y=\"52.596233\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.51622\" xlink:href=\"#maac0ae2ca9\" y=\"101.38635\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.540274\" xlink:href=\"#maac0ae2ca9\" y=\"55.456706\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"61.68161\" xlink:href=\"#maac0ae2ca9\" y=\"55.78213\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"198.018671\" xlink:href=\"#maac0ae2ca9\" y=\"127.972244\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.730935\" xlink:href=\"#maac0ae2ca9\" y=\"80.77656\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.774177\" xlink:href=\"#maac0ae2ca9\" y=\"45.445516\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"39.008425\" xlink:href=\"#maac0ae2ca9\" y=\"30.127065\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"63.391498\" xlink:href=\"#maac0ae2ca9\" y=\"44.909168\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.528856\" xlink:href=\"#maac0ae2ca9\" y=\"78.779367\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.057521\" xlink:href=\"#maac0ae2ca9\" y=\"64.629093\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.625224\" xlink:href=\"#maac0ae2ca9\" y=\"65.08072\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.549739\" xlink:href=\"#maac0ae2ca9\" y=\"66.576615\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.871417\" xlink:href=\"#maac0ae2ca9\" y=\"91.687256\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.684212\" xlink:href=\"#maac0ae2ca9\" y=\"81.758287\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"56.660144\" xlink:href=\"#maac0ae2ca9\" y=\"43.954147\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.788112\" xlink:href=\"#maac0ae2ca9\" y=\"60.582761\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.346658\" xlink:href=\"#maac0ae2ca9\" y=\"74.020385\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.289341\" xlink:href=\"#maac0ae2ca9\" y=\"68.713739\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.281191\" xlink:href=\"#maac0ae2ca9\" y=\"54.597686\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.161982\" xlink:href=\"#maac0ae2ca9\" y=\"60.920961\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.869491\" xlink:href=\"#maac0ae2ca9\" y=\"66.190902\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.520243\" xlink:href=\"#maac0ae2ca9\" y=\"51.4832\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.506653\" xlink:href=\"#maac0ae2ca9\" y=\"113.491053\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.299411\" xlink:href=\"#maac0ae2ca9\" y=\"62.431882\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.763942\" xlink:href=\"#maac0ae2ca9\" y=\"71.071452\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"93.204772\" xlink:href=\"#maac0ae2ca9\" y=\"63.435062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.06837\" xlink:href=\"#maac0ae2ca9\" y=\"63.938414\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.196769\" xlink:href=\"#maac0ae2ca9\" y=\"96.716184\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.367903\" xlink:href=\"#maac0ae2ca9\" y=\"83.552016\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.205494\" xlink:href=\"#maac0ae2ca9\" y=\"69.188236\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"90.373384\" xlink:href=\"#maac0ae2ca9\" y=\"54.208495\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"202.387148\" xlink:href=\"#maac0ae2ca9\" y=\"111.708087\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"57.470729\" xlink:href=\"#maac0ae2ca9\" y=\"47.871049\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.918252\" xlink:href=\"#maac0ae2ca9\" y=\"82.162264\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.170929\" xlink:href=\"#maac0ae2ca9\" y=\"88.942273\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.320385\" xlink:href=\"#maac0ae2ca9\" y=\"82.823298\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.272074\" xlink:href=\"#maac0ae2ca9\" y=\"51.813659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.650914\" xlink:href=\"#maac0ae2ca9\" y=\"94.117868\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.281797\" xlink:href=\"#maac0ae2ca9\" y=\"63.975334\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.647095\" xlink:href=\"#maac0ae2ca9\" y=\"69.938969\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.988185\" xlink:href=\"#maac0ae2ca9\" y=\"76.209841\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.65241\" xlink:href=\"#maac0ae2ca9\" y=\"111.888704\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.620214\" xlink:href=\"#maac0ae2ca9\" y=\"67.88536\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.282892\" xlink:href=\"#maac0ae2ca9\" y=\"65.299809\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.724158\" xlink:href=\"#maac0ae2ca9\" y=\"82.808152\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.570329\" xlink:href=\"#maac0ae2ca9\" y=\"64.31255\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"80.876532\" xlink:href=\"#maac0ae2ca9\" y=\"52.93288\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.169616\" xlink:href=\"#maac0ae2ca9\" y=\"101.229071\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.542808\" xlink:href=\"#maac0ae2ca9\" y=\"60.792384\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.799121\" xlink:href=\"#maac0ae2ca9\" y=\"116.940035\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.106451\" xlink:href=\"#maac0ae2ca9\" y=\"102.717976\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.637496\" xlink:href=\"#maac0ae2ca9\" y=\"97.754567\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.918708\" xlink:href=\"#maac0ae2ca9\" y=\"68.470441\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.223584\" xlink:href=\"#maac0ae2ca9\" y=\"94.086676\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.99852\" xlink:href=\"#maac0ae2ca9\" y=\"44.316061\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.729796\" xlink:href=\"#maac0ae2ca9\" y=\"80.730955\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.877977\" xlink:href=\"#maac0ae2ca9\" y=\"92.14219\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.796317\" xlink:href=\"#maac0ae2ca9\" y=\"70.357189\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.955741\" xlink:href=\"#maac0ae2ca9\" y=\"94.980365\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.009768\" xlink:href=\"#maac0ae2ca9\" y=\"68.623661\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.324972\" xlink:href=\"#maac0ae2ca9\" y=\"56.150922\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.017399\" xlink:href=\"#maac0ae2ca9\" y=\"66.518681\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.817842\" xlink:href=\"#maac0ae2ca9\" y=\"92.542047\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.020247\" xlink:href=\"#maac0ae2ca9\" y=\"48.874398\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.728312\" xlink:href=\"#maac0ae2ca9\" y=\"94.623606\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"143.302968\" xlink:href=\"#maac0ae2ca9\" y=\"85.898728\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.92288\" xlink:href=\"#maac0ae2ca9\" y=\"61.169665\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.754594\" xlink:href=\"#maac0ae2ca9\" y=\"63.660803\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.010156\" xlink:href=\"#maac0ae2ca9\" y=\"91.412357\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.781505\" xlink:href=\"#maac0ae2ca9\" y=\"87.666976\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.374395\" xlink:href=\"#maac0ae2ca9\" y=\"53.451842\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"172.23414\" xlink:href=\"#maac0ae2ca9\" y=\"126.071195\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.481532\" xlink:href=\"#maac0ae2ca9\" y=\"75.518188\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.385309\" xlink:href=\"#maac0ae2ca9\" y=\"76.870524\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.7433\" xlink:href=\"#maac0ae2ca9\" y=\"107.631762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"164.66836\" xlink:href=\"#maac0ae2ca9\" y=\"93.836414\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.214429\" xlink:href=\"#maac0ae2ca9\" y=\"79.170486\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.60435\" xlink:href=\"#maac0ae2ca9\" y=\"88.687301\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.796253\" xlink:href=\"#maac0ae2ca9\" y=\"75.42317\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.127987\" xlink:href=\"#maac0ae2ca9\" y=\"77.924654\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.352841\" xlink:href=\"#maac0ae2ca9\" y=\"81.922566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"65.390252\" xlink:href=\"#maac0ae2ca9\" y=\"46.447371\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"75.191894\" xlink:href=\"#maac0ae2ca9\" y=\"51.923347\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.946683\" xlink:href=\"#maac0ae2ca9\" y=\"64.52152\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.6068\" xlink:href=\"#maac0ae2ca9\" y=\"100.892609\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"166.666937\" xlink:href=\"#maac0ae2ca9\" y=\"119.720307\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"110.273985\" xlink:href=\"#maac0ae2ca9\" y=\"70.089336\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.29631\" xlink:href=\"#maac0ae2ca9\" y=\"54.335111\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"59.145753\" xlink:href=\"#maac0ae2ca9\" y=\"49.245336\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.78722\" xlink:href=\"#maac0ae2ca9\" y=\"76.940631\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.89652\" xlink:href=\"#maac0ae2ca9\" y=\"85.81535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.182472\" xlink:href=\"#maac0ae2ca9\" y=\"75.628318\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.635716\" xlink:href=\"#maac0ae2ca9\" y=\"35.185903\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.116187\" xlink:href=\"#maac0ae2ca9\" y=\"81.231384\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.636552\" xlink:href=\"#maac0ae2ca9\" y=\"87.37555\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"172.969891\" xlink:href=\"#maac0ae2ca9\" y=\"101.837485\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.154467\" xlink:href=\"#maac0ae2ca9\" y=\"41.953177\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.46377\" xlink:href=\"#maac0ae2ca9\" y=\"82.750352\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.297237\" xlink:href=\"#maac0ae2ca9\" y=\"62.589865\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"179.170514\" xlink:href=\"#maac0ae2ca9\" y=\"105.84739\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.533876\" xlink:href=\"#maac0ae2ca9\" y=\"57.113481\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"47.899205\" xlink:href=\"#maac0ae2ca9\" y=\"37.407639\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.793054\" xlink:href=\"#maac0ae2ca9\" y=\"57.980323\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.425918\" xlink:href=\"#maac0ae2ca9\" y=\"78.977538\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.006774\" xlink:href=\"#maac0ae2ca9\" y=\"82.600555\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.009122\" xlink:href=\"#maac0ae2ca9\" y=\"76.231353\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.104137\" xlink:href=\"#maac0ae2ca9\" y=\"55.654642\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.751312\" xlink:href=\"#maac0ae2ca9\" y=\"52.869191\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"166.424965\" xlink:href=\"#maac0ae2ca9\" y=\"99.519415\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.856884\" xlink:href=\"#maac0ae2ca9\" y=\"87.174603\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"114.755965\" xlink:href=\"#maac0ae2ca9\" y=\"98.93038\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.14578\" xlink:href=\"#maac0ae2ca9\" y=\"93.384002\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.188004\" xlink:href=\"#maac0ae2ca9\" y=\"97.089806\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.350283\" xlink:href=\"#maac0ae2ca9\" y=\"86.462223\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.222401\" xlink:href=\"#maac0ae2ca9\" y=\"90.148324\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.522041\" xlink:href=\"#maac0ae2ca9\" y=\"75.194619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.012452\" xlink:href=\"#maac0ae2ca9\" y=\"72.176126\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.170602\" xlink:href=\"#maac0ae2ca9\" y=\"80.921498\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.988994\" xlink:href=\"#maac0ae2ca9\" y=\"124.505485\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"76.659806\" xlink:href=\"#maac0ae2ca9\" y=\"63.589181\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.086971\" xlink:href=\"#maac0ae2ca9\" y=\"77.960689\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.634707\" xlink:href=\"#maac0ae2ca9\" y=\"78.097889\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.752913\" xlink:href=\"#maac0ae2ca9\" y=\"97.816642\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.593559\" xlink:href=\"#maac0ae2ca9\" y=\"65.913327\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.536507\" xlink:href=\"#maac0ae2ca9\" y=\"89.91789\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"99.074346\" xlink:href=\"#maac0ae2ca9\" y=\"73.832514\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"63.674346\" xlink:href=\"#maac0ae2ca9\" y=\"39.835098\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.357016\" xlink:href=\"#maac0ae2ca9\" y=\"78.553822\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.406827\" xlink:href=\"#maac0ae2ca9\" y=\"61.7239\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.462541\" xlink:href=\"#maac0ae2ca9\" y=\"49.147115\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.86157\" xlink:href=\"#maac0ae2ca9\" y=\"81.677925\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.410202\" xlink:href=\"#maac0ae2ca9\" y=\"48.500515\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.812672\" xlink:href=\"#maac0ae2ca9\" y=\"80.603522\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.662768\" xlink:href=\"#maac0ae2ca9\" y=\"76.042444\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.958582\" xlink:href=\"#maac0ae2ca9\" y=\"82.073303\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.748184\" xlink:href=\"#maac0ae2ca9\" y=\"62.796451\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.726657\" xlink:href=\"#maac0ae2ca9\" y=\"93.136791\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.971697\" xlink:href=\"#maac0ae2ca9\" y=\"70.882106\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.535165\" xlink:href=\"#maac0ae2ca9\" y=\"69.373454\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.022819\" xlink:href=\"#maac0ae2ca9\" y=\"66.438824\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.354987\" xlink:href=\"#maac0ae2ca9\" y=\"66.233685\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.424315\" xlink:href=\"#maac0ae2ca9\" y=\"75.615302\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"164.830619\" xlink:href=\"#maac0ae2ca9\" y=\"113.094674\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.034368\" xlink:href=\"#maac0ae2ca9\" y=\"75.327283\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.998242\" xlink:href=\"#maac0ae2ca9\" y=\"107.429995\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"64.391779\" xlink:href=\"#maac0ae2ca9\" y=\"40.355155\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.849965\" xlink:href=\"#maac0ae2ca9\" y=\"72.159549\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.394017\" xlink:href=\"#maac0ae2ca9\" y=\"83.713273\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.839457\" xlink:href=\"#maac0ae2ca9\" y=\"54.729352\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.067288\" xlink:href=\"#maac0ae2ca9\" y=\"97.490514\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.767774\" xlink:href=\"#maac0ae2ca9\" y=\"96.541603\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.738941\" xlink:href=\"#maac0ae2ca9\" y=\"66.502733\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.455307\" xlink:href=\"#maac0ae2ca9\" y=\"62.892944\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.987741\" xlink:href=\"#maac0ae2ca9\" y=\"65.633364\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"121.317432\" xlink:href=\"#maac0ae2ca9\" y=\"74.231926\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"88.578655\" xlink:href=\"#maac0ae2ca9\" y=\"83.186746\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.767407\" xlink:href=\"#maac0ae2ca9\" y=\"68.425698\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.598733\" xlink:href=\"#maac0ae2ca9\" y=\"85.828532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.273244\" xlink:href=\"#maac0ae2ca9\" y=\"92.598\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.909858\" xlink:href=\"#maac0ae2ca9\" y=\"69.313266\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.977785\" xlink:href=\"#maac0ae2ca9\" y=\"78.863258\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"158.820452\" xlink:href=\"#maac0ae2ca9\" y=\"91.408963\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.407724\" xlink:href=\"#maac0ae2ca9\" y=\"92.850451\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.7679\" xlink:href=\"#maac0ae2ca9\" y=\"74.205937\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.683628\" xlink:href=\"#maac0ae2ca9\" y=\"71.131785\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.230443\" xlink:href=\"#maac0ae2ca9\" y=\"73.564865\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"112.709517\" xlink:href=\"#maac0ae2ca9\" y=\"78.213492\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"65.616198\" xlink:href=\"#maac0ae2ca9\" y=\"61.347058\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.986528\" xlink:href=\"#maac0ae2ca9\" y=\"86.386167\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.121804\" xlink:href=\"#maac0ae2ca9\" y=\"90.466286\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.203315\" xlink:href=\"#maac0ae2ca9\" y=\"79.620562\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"143.260526\" xlink:href=\"#maac0ae2ca9\" y=\"104.110654\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.951624\" xlink:href=\"#maac0ae2ca9\" y=\"67.597807\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.067567\" xlink:href=\"#maac0ae2ca9\" y=\"95.335647\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"91.520308\" xlink:href=\"#maac0ae2ca9\" y=\"54.975951\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.888633\" xlink:href=\"#maac0ae2ca9\" y=\"73.435137\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.825423\" xlink:href=\"#maac0ae2ca9\" y=\"82.486396\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"118.619485\" xlink:href=\"#maac0ae2ca9\" y=\"68.776524\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"115.4562\" xlink:href=\"#maac0ae2ca9\" y=\"55.286431\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.069606\" xlink:href=\"#maac0ae2ca9\" y=\"62.219249\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.964416\" xlink:href=\"#maac0ae2ca9\" y=\"56.065569\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.133342\" xlink:href=\"#maac0ae2ca9\" y=\"96.349726\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"128.495312\" xlink:href=\"#maac0ae2ca9\" y=\"73.101079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"170.034324\" xlink:href=\"#maac0ae2ca9\" y=\"100.969366\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.288246\" xlink:href=\"#maac0ae2ca9\" y=\"76.859252\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.630237\" xlink:href=\"#maac0ae2ca9\" y=\"69.273986\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"116.8104\" xlink:href=\"#maac0ae2ca9\" y=\"57.758533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.884425\" xlink:href=\"#maac0ae2ca9\" y=\"54.515629\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.149682\" xlink:href=\"#maac0ae2ca9\" y=\"43.996913\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.247254\" xlink:href=\"#maac0ae2ca9\" y=\"90.577533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.245068\" xlink:href=\"#maac0ae2ca9\" y=\"74.58045\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.482499\" xlink:href=\"#maac0ae2ca9\" y=\"71.144745\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"138.391805\" xlink:href=\"#maac0ae2ca9\" y=\"80.92511\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"68.864817\" xlink:href=\"#maac0ae2ca9\" y=\"27.917801\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.194061\" xlink:href=\"#maac0ae2ca9\" y=\"45.46881\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.473572\" xlink:href=\"#maac0ae2ca9\" y=\"74.716449\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.070846\" xlink:href=\"#maac0ae2ca9\" y=\"78.284142\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.544734\" xlink:href=\"#maac0ae2ca9\" y=\"83.161393\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.236769\" xlink:href=\"#maac0ae2ca9\" y=\"62.765221\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.728363\" xlink:href=\"#maac0ae2ca9\" y=\"77.45584\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"123.49141\" xlink:href=\"#maac0ae2ca9\" y=\"72.91376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.78061\" xlink:href=\"#maac0ae2ca9\" y=\"73.261376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"96.317218\" xlink:href=\"#maac0ae2ca9\" y=\"56.207964\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"70.550618\" xlink:href=\"#maac0ae2ca9\" y=\"46.223514\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"166.536308\" xlink:href=\"#maac0ae2ca9\" y=\"106.496464\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.351766\" xlink:href=\"#maac0ae2ca9\" y=\"91.429535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.085147\" xlink:href=\"#maac0ae2ca9\" y=\"35.34897\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"105.450865\" xlink:href=\"#maac0ae2ca9\" y=\"44.019016\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"46.048341\" xlink:href=\"#maac0ae2ca9\" y=\"17.698849\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"127.330239\" xlink:href=\"#maac0ae2ca9\" y=\"77.562455\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.049063\" xlink:href=\"#maac0ae2ca9\" y=\"92.141841\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.22669\" xlink:href=\"#maac0ae2ca9\" y=\"83.927824\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.131351\" xlink:href=\"#maac0ae2ca9\" y=\"84.208042\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.537345\" xlink:href=\"#maac0ae2ca9\" y=\"89.526413\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"142.67999\" xlink:href=\"#maac0ae2ca9\" y=\"74.058409\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.123505\" xlink:href=\"#maac0ae2ca9\" y=\"42.7631\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.803906\" xlink:href=\"#maac0ae2ca9\" y=\"43.359821\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.157403\" xlink:href=\"#maac0ae2ca9\" y=\"63.782534\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"85.244669\" xlink:href=\"#maac0ae2ca9\" y=\"77.778952\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"56.93015\" xlink:href=\"#maac0ae2ca9\" y=\"40.456402\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.44076\" xlink:href=\"#maac0ae2ca9\" y=\"53.614705\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"168.331772\" xlink:href=\"#maac0ae2ca9\" y=\"103.791069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"89.987735\" xlink:href=\"#maac0ae2ca9\" y=\"70.548043\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.069732\" xlink:href=\"#maac0ae2ca9\" y=\"79.523183\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"120.397365\" xlink:href=\"#maac0ae2ca9\" y=\"60.226412\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9a52baf6a1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.802859\" xlink:href=\"#m9a52baf6a1\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(46.431765 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.120321\" xlink:href=\"#m9a52baf6a1\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(109.939071 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"172.437784\" xlink:href=\"#m9a52baf6a1\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(169.256534 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3bd48fd078\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m3bd48fd078\" y=\"118.51628\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- −5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 122.315499)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m3bd48fd078\" y=\"93.653276\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <g transform=\"translate(15.579688 97.452495)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m3bd48fd078\" y=\"68.790272\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 5 -->\n      <g transform=\"translate(15.579688 72.589491)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m3bd48fd078\" y=\"43.927268\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(9.217188 47.726487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m3bd48fd078\" y=\"19.064264\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 15 -->\n      <g transform=\"translate(9.217188 22.863483)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 28.942188 143.1 \nL 28.942188 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 224.242188 143.1 \nL 224.242188 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 28.942188 143.1 \nL 224.242188 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 28.942188 7.2 \nL 224.242188 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0b5a6b2379\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"28.942188\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejj2zPFtvnKy"
      },
      "source": [
        "### Reading the Dataset\n",
        "\n",
        "Recall that training models consists of making multiple passes over the dataset, grabbing one minibatch of examples at a time, and using them to update our model. Since this process is so fundamental to training machine learning algorithms, it is worth defining a utility function to shuffle the dataset and access it in minibatches.\n",
        "\n",
        "In the following code, we define the data_iter function to demonstrate one possible implementation of this functionality. The function takes a batch size, a matrix of features, and a vector of labels, yielding minibatches of the size batch_size. Each minibatch consists of a tuple of features and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmz-iEFwvMjw"
      },
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    \"\"\"\n",
        "    Return (X, y) of shape (batch_size, d), (batch_size, 1)\n",
        "    \"\"\"\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        batch_indices = indices[i:min(i+batch_size, num_examples)]\n",
        "        yield features[batch_indices], labels[batch_indices]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH8WaNmw3K_r",
        "outputId": "4c8859df-92e5-4b01-d017-19788a95e6dd"
      },
      "source": [
        "batch_size = 256\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print (X.shape, y.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 2]) torch.Size([256, 1])\n",
            "torch.Size([256, 2]) torch.Size([256, 1])\n",
            "torch.Size([256, 2]) torch.Size([256, 1])\n",
            "torch.Size([232, 2]) torch.Size([232, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3vzic073FpC",
        "outputId": "363975c9-80cd-490a-e76d-90f46244fc44"
      },
      "source": [
        "batch_size = 10\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print(X, '\\n', y)\n",
        "    break"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.4020,  0.1737],\n",
            "        [-0.1554, -0.5340],\n",
            "        [-0.9614,  1.0495],\n",
            "        [ 0.7605,  1.1321],\n",
            "        [ 0.0327, -0.1008],\n",
            "        [-0.3723,  1.5544],\n",
            "        [ 0.4380, -0.3332],\n",
            "        [ 1.1616,  1.2298],\n",
            "        [ 1.9715, -1.4922],\n",
            "        [ 1.1500, -0.5093]]) \n",
            " tensor([[ 8.4140],\n",
            "        [ 5.7019],\n",
            "        [-1.3018],\n",
            "        [ 1.8661],\n",
            "        [ 4.6087],\n",
            "        [-1.8093],\n",
            "        [ 6.2116],\n",
            "        [ 2.3469],\n",
            "        [13.2195],\n",
            "        [ 8.2315]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-JSq5i3Yiu"
      },
      "source": [
        "As we run the iteration, we obtain distinct minibatches successively until the entire dataset has been exhausted (try this). While the iteration implemented above is good for didactic purposes, it is inefficient in ways that might get us in trouble on real problems. For example, it requires that we load all the data in memory and that we perform lots of random memory access. The built-in iterators implemented in a deep learning framework are considerably more efficient and they can deal with both data stored in files and data fed via data streams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzBJ4BuD3cB-"
      },
      "source": [
        "### Initializing Model Parameters\n",
        "\n",
        "Before we can begin optimizing our model’s parameters by minibatch stochastic gradient descent, we need to have some parameters in the first place. In the following code, we initialize weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of 0.01, and setting the bias to 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5j3JM__3Hyt",
        "outputId": "47406350-4c55-4b74-9fc5-12d2bbe05f84"
      },
      "source": [
        "w = torch.normal(mean=0, std=0.01, size=(2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "print (w)\n",
        "print (b)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0047],\n",
            "        [-0.0016]], requires_grad=True)\n",
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkiHXCcN37MT"
      },
      "source": [
        "After initializing our parameters, our next task is to update them until they fit our data sufficiently well. Each update requires taking the gradient of our loss function with respect to the parameters. Given this gradient, we can update each parameter in the direction that may reduce the loss.\n",
        "\n",
        "Since nobody wants to compute gradients explicitly (this is tedious and error prone), we use automatic differentiation, as introduced in Section 2.5, to compute the gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxxN4TxG37zP"
      },
      "source": [
        "### Defining the Model\n",
        "\n",
        "Next, we must define our model, relating its inputs and parameters to its outputs. Recall that to calculate the output of the linear model, we simply take the matrix-vector dot product of the input features $\\mathbf{X}$ and the model weights $\\mathbf{w}$ nd add the offset  b  to each example.\n",
        "\n",
        "Note that below $\\mathbf{Xw}$ is a vector and  b  is a scalar. Recall the broadcasting mechanism as described in Section 2.1.3. When we add a vector and a scalar, the scalar is added to each component of the vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvRkZrSG3zGT",
        "outputId": "356bcc1c-0479-41ab-ac97-62628a79a4e7"
      },
      "source": [
        "def linreg(X, w, b):  #@save\n",
        "    \"\"\"The linear regression model.\"\"\"\n",
        "    return torch.matmul(X, w) + b #### X.w : Nx2 . 2x1 : Nx1\n",
        "\n",
        "linreg(X, w, b).shape"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2sIxReE44W7"
      },
      "source": [
        "### Defining the Loss Function\n",
        "\n",
        "Since updating our model requires taking the gradient of our loss function, we ought to define the loss function first. Here we will use the squared loss function as described in Section 3.1. In the implementation, we need to transform the true value y into the predicted value’s shape y_hat. The result returned by the following function will also have the same shape as y_hat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-anbeoE4wZm"
      },
      "source": [
        "def squared_loss(y_hat, y):\n",
        "    return 0.5*(y_hat - y.reshape(y_hat.shape))**2"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5QrUMbi7rs7"
      },
      "source": [
        "### Defining the Optimization Algorithm\n",
        "\n",
        "As we discussed in Section 3.1, linear regression has a closed-form solution. However, this is not a book about linear regression: it is a book about deep learning. Since none of the other models that this book introduces can be solved analytically, we will take this opportunity to introduce your first working example of minibatch stochastic gradient descent.\n",
        "\n",
        "At each step, using one minibatch randomly drawn from our dataset, we will estimate the gradient of the loss with respect to our parameters. Next, we will update our parameters in the direction that may reduce the loss. The following code applies the minibatch stochastic gradient descent update, given a set of parameters, a learning rate, and a batch size. The size of the update step is determined by the learning rate lr. Because our loss is calculated as a sum over the minibatch of examples, we normalize our step size by the batch size (batch_size), so that the magnitude of a typical step size does not depend heavily on our choice of the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rp1vsxg7nRo"
      },
      "source": [
        "def sgd(params, lr, batch_size):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad/batch_size\n",
        "            param.grad.zero_()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg4_83oWHPAX"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now that we have all of the parts in place, we are ready to implement the main training loop. It is crucial that you understand this code because you will see nearly identical training loops over and over again throughout your career in deep learning.\n",
        "\n",
        "In each iteration, we will grab a minibatch of training examples, and pass them through our model to obtain a set of predictions. After calculating the loss, we initiate the backwards pass through the network, storing the gradients with respect to each parameter. Finally, we will call the optimization algorithm sgd to update the model parameters.\n",
        "\n",
        "In summary, we will execute the following loop:\n",
        "\n",
        "1. Init params $(\\mathbf{w}, b)$\n",
        "2. Repeat until done\n",
        "    2.1 Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
        "    2.2 Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
        "\n",
        "In each epoch, we will iterate through the entire dataset (using the data_iter function) once passing through every example in the training dataset (assuming that the number of examples is divisible by the batch size). The number of epochs num_epochs and the learning rate lr are both hyperparameters, which we set here to 3 and 0.03, respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th5faoxaHpyC"
      },
      "source": [
        "lr = 0.03\n",
        "num_epochs = 3\n",
        "net = linreg ### network\n",
        "loss = squared_loss"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4TdMkSoIJpe",
        "outputId": "7403daa3-e1db-44a4-cf21-a0244930a1da"
      },
      "source": [
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"\n",
        "    Generate y = Xw + b + noise.\n",
        "    Return features: shape (num_examples, w)\n",
        "    labels : shape (num_examples)\n",
        "    \"\"\"\n",
        "    X = torch.normal(mean=0, std=1, size=(num_examples, len(w)))\n",
        "    print (X.shape)\n",
        "    y = torch.matmul(X, w) + b\n",
        "    print (y.shape)\n",
        "    ### add noise\n",
        "    y += torch.normal(mean=0, std=0.01, size=y.shape)\n",
        "    return X, y.reshape((-1, 1))\n",
        "\n",
        "def data_iter(batch_size, features, labels):\n",
        "    \"\"\"\n",
        "    Return (X, y) of shape (batch_size, d), (batch_size, 1)\n",
        "    \"\"\"\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        batch_indices = indices[i:min(i+batch_size, num_examples)]\n",
        "        yield features[batch_indices], labels[batch_indices]\n",
        "\n",
        "def linreg(X, w, b):  #@save\n",
        "    \"\"\"The linear regression model.\"\"\"\n",
        "    return torch.matmul(X, w) + b #### X.w : Nx2 . 2x1 : Nx1\n",
        "\n",
        "def squared_loss(y_hat, y):\n",
        "    return 0.5*(y_hat - y.reshape(y_hat.shape))**2\n",
        "\n",
        "\n",
        "def sgd(params, lr, batch_size):\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad/batch_size\n",
        "            param.grad.zero_()\n",
        "\n",
        "\n",
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)\n",
        "w = torch.normal(mean=0, std=0.01, size=(2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 2])\n",
            "torch.Size([1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqpP1jJryaHE"
      },
      "source": [
        "#### Analytical verison of the diff of L wrt w\n",
        "\n",
        "Let us see how the update to w will be analytically\n",
        "\n",
        "![](https://i.imgur.com/KCmUlE7.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUpqfr91SNlU"
      },
      "source": [
        "def analytical_grad_L_wrt_w(X, w, b, y):\n",
        "    print (X.shape, w.shape, b, y.shape)\n",
        "    print(torch.matmul(X.T, torch.matmul(X, w) + b - y))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW6nt6UZHwBW",
        "outputId": "ac8332c6-26ad-49f4-efe5-1a2e76e3f7d7"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter(batch_size=batch_size, features=features, labels=labels):\n",
        "        print (X.shape, y.shape)\n",
        "        y_hat = net(X, w, b)\n",
        "        # -print (y_hat.shape)\n",
        "        l = loss(y_hat, y) # Minibatch loss in `X` and `y`: shape is (mini_batch, 1)\n",
        "        print (l.shape)\n",
        "        l = l.sum()\n",
        "        print (l.shape)\n",
        "        l.backward() ### now we have access to w.grad and b.grad\n",
        "        print (w.grad)\n",
        "        print (b.grad.shape)\n",
        "        w_grad = analytical_grad_L_wrt_w(X, w, b, y)\n",
        "        break\n",
        "    break\n",
        "            "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 2]) torch.Size([10, 1])\n",
            "torch.Size([10, 1])\n",
            "torch.Size([])\n",
            "tensor([[ 0.0365],\n",
            "        [22.1043]])\n",
            "torch.Size([1])\n",
            "torch.Size([10, 2]) torch.Size([2, 1]) tensor([0.], requires_grad=True) torch.Size([10, 1])\n",
            "tensor([[ 0.0365],\n",
            "        [22.1043]], grad_fn=<MmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoiJCMWGwfJO"
      },
      "source": [
        "So the analytical calculation matches with the `backward` method\n",
        "We need to sum up the elements of `l` vector here as we need to compute gradient for a scalar, we cant compute it for a vector. We can compute it wrt to a vector thought as we have done here (wrt w)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz_LiN4oJarp",
        "outputId": "71ca7128-9e49-4ffa-c221-21b25b67aaaa"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter(batch_size, features, labels):\n",
        "        l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
        "        # Compute gradient on `l` with respect to [`w`, `b`]\n",
        "        l.sum().backward()\n",
        "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
        "    with torch.no_grad():\n",
        "        train_l = loss(net(features, w, b), labels)\n",
        "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.054319\n",
            "epoch 2, loss 0.000252\n",
            "epoch 3, loss 0.000055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he4hDLCzxJL4",
        "outputId": "39e8274c-ac37-47f7-f7ac-cfcf27496ca0"
      },
      "source": [
        "print (true_w, true_b)\n",
        "print (w, b)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2.0000, -3.4000]) 4.2\n",
            "tensor([[ 1.9991],\n",
            "        [-3.3993]], requires_grad=True) tensor([4.1989], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pjXcjOvxiiG"
      },
      "source": [
        "Note that we should not take it for granted that we are able to recover the parameters perfectly. However, in machine learning, we are typically less concerned with recovering true underlying parameters, and more concerned with parameters that lead to highly accurate prediction. Fortunately, even on difficult optimization problems, stochastic gradient descent can often find remarkably good solutions, owing partly to the fact that, for deep networks, there exist many configurations of the parameters that lead to highly accurate prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DWxg_G7LDN"
      },
      "source": [
        "## Linear Regression - Concise Implementation\n",
        "\n",
        "In Section 3.2, we relied only on (i) tensors for data storage and linear algebra; and (ii) auto differentiation for calculating gradients. In practice, because data iterators, loss functions, optimizers, and neural network layers are so common, modern libraries implement these components for us as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZVWXO0xxYXm",
        "outputId": "96949baf-a4f3-4047-b59f-4535cf8f6734"
      },
      "source": [
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
        "\n",
        "print (features.shape, labels.shape)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 2]) torch.Size([1000, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSFaM0Cz7eE2"
      },
      "source": [
        "### Reading the Dataset\n",
        "\n",
        "Rather than rolling our own iterator, we can call upon the existing API in a framework to read data. We pass in features and labels as arguments and specify `batch_size` when instantiating a data iterator object. Besides, the boolean value `is_train` indicates whether or not we want the data iterator object to shuffle the data on each epoch (pass through the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIEerlHU7W8f",
        "outputId": "eaac8568-3b14-4240-9298-fa1568ca5930"
      },
      "source": [
        "### samples a single (feature, label) pair\n",
        "print ((torch.utils.data.TensorDataset(*(features, labels)))) ### converts the features, labels into -> torch.utils.data.dataset.TensorDataset \n",
        "### samples a single (feature, label) pair from the dataset\n",
        "print(next(iter(torch.utils.data.TensorDataset(*(features, labels)))))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataset.TensorDataset object at 0x7fab1f643cd0>\n",
            "(tensor([-0.4564,  0.1008]), tensor([2.9521]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9wZYF-88YOC",
        "outputId": "778fc722-480e-4ae8-93ee-b8913f8ec0b3"
      },
      "source": [
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    ### sample batches\n",
        "    return torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=is_train)\n",
        "\n",
        "batch_size = 10\n",
        "data_iter = load_array((features, labels), batch_size)\n",
        "\n",
        "print (data_iter)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7fab1f6327d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KvP0dvU9bJx",
        "outputId": "69ad82aa-5051-4b2c-c459-0f572bc94673"
      },
      "source": [
        "next(iter(data_iter))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-5.4767e-04,  2.0321e-01],\n",
              "         [-9.6652e-03,  7.1767e-01],\n",
              "         [-1.4388e+00, -1.3095e+00],\n",
              "         [-1.4924e+00, -6.0577e-01],\n",
              "         [-1.3565e+00, -3.0937e-01],\n",
              "         [ 2.8535e-01, -1.4642e+00],\n",
              "         [-1.4442e+00, -2.3259e+00],\n",
              "         [ 1.2518e+00, -3.3842e-01],\n",
              "         [ 2.1532e+00, -1.1780e+00],\n",
              "         [ 8.7594e-02,  1.3306e+00]]), tensor([[ 3.5227],\n",
              "         [ 1.7337],\n",
              "         [ 5.7915],\n",
              "         [ 3.2729],\n",
              "         [ 2.5344],\n",
              "         [ 9.7562],\n",
              "         [ 9.2043],\n",
              "         [ 7.8498],\n",
              "         [12.5234],\n",
              "         [-0.1415]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD27egf1-P1y"
      },
      "source": [
        "### Defining the Model\n",
        "\n",
        "For standard operations, we can use a framework’s predefined layers, which allow us to focus especially on the layers used to construct the model rather than having to focus on the implementation. We will first define a model variable net, which will refer to an instance of the Sequential class. The Sequential class defines a container for several layers that will be chained together. Given input data, a Sequential instance passes it through the first layer, in turn passing the output as the second layer’s input and so forth. In the following example, our model consists of only one layer, so we do not really need Sequential. But since nearly all of our future models will involve multiple layers, we will use it anyway just to familiarize you with the most standard workflow.\n",
        "\n",
        "n PyTorch, the fully-connected layer is defined in the Linear class. Note that we passed two arguments into nn.Linear. The first one specifies the input feature dimension, which is 2, and the second one is the output feature dimension, which is a single scalar and therefore 1.\n",
        "\n",
        "#### Fully Connected Layers as Matrix Multiplications\n",
        "\n",
        "![](https://i.imgur.com/NTORrPb.jpg)\n",
        "\n",
        "```\n",
        ">>> m = nn.Linear(20, 30)\n",
        ">>> input = torch.randn(128, 20)\n",
        ">>> output = m(input)\n",
        ">>> print(output.size())\n",
        "torch.Size([128, 30])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIHQJ2zl-MDE"
      },
      "source": [
        "from torch import nn"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT8XRmIOBsgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b55164-d374-4689-bb5d-0574d1fe5000"
      },
      "source": [
        "network = nn.Linear(3,2)\n",
        "input = torch.randn(3,3)\n",
        "output = network(input)\n",
        "print (output.shape)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEAdDfM6z7cE"
      },
      "source": [
        "### net for our regresion problem\n",
        "### input features dim = 2, op = 1\n",
        "net = nn.Sequential(nn.Linear(2,1))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g0ygI85D5bX"
      },
      "source": [
        "### Initializing Model Parameters\n",
        "\n",
        "Before using net, we need to initialize the model parameters, such as the weights and bias in the linear regression model. Deep learning frameworks often have a predefined way to initialize the parameters. Here we specify that each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01. The bias parameter will be initialized to zero.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkg2kaqm0Opz",
        "outputId": "3e67b6cd-cde1-4973-ce59-54da90a13f5d"
      },
      "source": [
        "net"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJtcd_G80VeG"
      },
      "source": [
        "As we have specified the input and output dimensions when constructing nn.Linear. Now we access the parameters directly to specify their initial values. We first locate the layer by `net[0]`, which is the first layer in the network, and then use the `weight.data` and `bias.data methods` to access the parameters. Next we use the replace methods `normal_` and `fill_` to overwrite parameter values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPJHWeoI0QV2",
        "outputId": "7e746927-b8a8-445b-af36-c1a43eeff154"
      },
      "source": [
        "net[0].weight.data.normal_(0, 0.01)\n",
        "net[0].bias.data.fill_(0)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpAtqpPB0mkf"
      },
      "source": [
        "### Defining the Loss Function\n",
        "\n",
        "The MSELoss class computes the mean squared error, also known as squared  L2  norm. By default it returns the average loss over examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMq8B6Kq0jrm"
      },
      "source": [
        "## measures the mean squared error (squared L2 norm) between each element in the input x and target y.\n",
        "loss = nn.MSELoss()"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UF-8sqt03ok"
      },
      "source": [
        "### Defining the Optimization Algorithm\n",
        "\n",
        "Minibatch stochastic gradient descent is a standard tool for optimizing neural networks and thus PyTorch supports it alongside a number of variations on this algorithm in the `optim` module. When we instantiate an SGD instance, we will specify the parameters to optimize over (obtainable from our net via net.parameters()), with a dictionary of hyperparameters required by our optimization algorithm. Minibatch stochastic gradient descent just requires that we set the value lr, which is set to 0.03 here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSXb75k_01tz"
      },
      "source": [
        "trainer = torch.optim.SGD(params=net.parameters(), lr=0.03)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-q-FaL1VY5"
      },
      "source": [
        "### Training\n",
        "\n",
        "You might have noticed that expressing our model through high-level APIs of a deep learning framework requires comparatively few lines of code. We did not have to individually allocate parameters, define our loss function, or implement minibatch stochastic gradient descent. Once we start working with much more complex models, advantages of high-level APIs will grow considerably. However, once we have all the basic pieces in place, the training loop itself is strikingly similar to what we did when implementing everything from scratch.\n",
        "\n",
        "To refresh your memory: for some number of epochs, we will make a complete pass over the dataset (train_data), iteratively grabbing one minibatch of inputs and the corresponding ground-truth labels. For each minibatch, we go through the following ritual:\n",
        "\n",
        "- Generate predictions by calling net(X) and calculate the loss l (the forward propagation).\n",
        "\n",
        "- Calculate gradients by running the backpropagation.\n",
        "\n",
        "- Update the model parameters by invoking our optimizer.\n",
        "\n",
        "For good measure, we compute the loss after each epoch and print it to monitor progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIFkpzC65_HG",
        "outputId": "58ab2903-3839-4b09-d26d-453634501ac4"
      },
      "source": [
        "print (features.shape)\n",
        "net(features).shape"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au56Z3f81TM_",
        "outputId": "7bb73b91-825b-4ed5-c3bc-ca1c2a3ffb98"
      },
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter:\n",
        "        l = loss(net(X), y)\n",
        "        trainer.zero_grad() ### zero out the grads before backpropagation\n",
        "        l.backward()\n",
        "        trainer.step() ### Performs a single optimization step: basically does w = w - lr * dL/dw and same for bias\n",
        "    ### loss over the entire data\n",
        "    l = loss(net(features), labels)\n",
        "    print(f'epoch {epoch + 1}, loss {l:f}')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.000418\n",
            "epoch 2, loss 0.000112\n",
            "epoch 3, loss 0.000114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1WxBgNj7Shs"
      },
      "source": [
        "Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. To access parameters, we first access the layer that we need from net and then access that layer’s weights and bias. As in our from-scratch implementation, note that our estimated parameters are close to their ground-truth counterparts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtPbNvTp6IJC",
        "outputId": "a3991ff4-04a4-4e62-a401-3dc47bce18bf"
      },
      "source": [
        "print (net[0].weight.data)\n",
        "print (net[0].bias.data)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.9991, -3.4012]])\n",
            "tensor([4.2013])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk5ekWg0BaPl"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. If we replace nn.MSELoss(reduction='sum') with nn.MSELoss(), how can we change the learning rate for the code to behave identically. Why?\n",
        "\n",
        "As we saw before the dimension of `l` would be N x 1 where N is batch size\n",
        "By default we aggregate it by mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egzZwKqj7Xuy",
        "outputId": "faa3e92d-15db-48ac-ffea-81862c7c38f6"
      },
      "source": [
        "sample_y_hat = torch.tensor(data=[3, 2.5, 5])\n",
        "sample_y_target = torch.tensor(data=[3.4, 2, 6])\n",
        "loss = nn.MSELoss(reduction='none')\n",
        "print (loss(sample_y_hat, sample_y_target))\n",
        "loss = nn.MSELoss(reduction='mean')\n",
        "print (loss(sample_y_hat, sample_y_target))\n",
        "loss = nn.MSELoss(reduction='sum')\n",
        "print (loss(sample_y_hat, sample_y_target))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.1600, 0.2500, 1.0000])\n",
            "tensor(0.4700)\n",
            "tensor(1.4100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXBDM1eCDjUQ",
        "outputId": "e5ae3c9d-a58f-4b7e-dbaf-817c9fc83695"
      },
      "source": [
        "net[0].weight.data.normal_(0, 0.01)\n",
        "net[0].bias.data.fill_(0)\n",
        "\n",
        "loss = nn.MSELoss(reduction='sum')\n",
        "trainer = torch.optim.SGD(params=net.parameters(), lr=0.0007)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter:\n",
        "        l = loss(net(X), y)\n",
        "        trainer.zero_grad() ### zero out the grads before backpropagation\n",
        "        l.backward()\n",
        "        trainer.step() ### Performs a single optimization step: basically does w = w - lr * dL/dw and same for bias\n",
        "    ### loss over the entire data\n",
        "    l = loss(net(features), labels)\n",
        "    print(f'epoch {epoch + 1}, loss {l:f}')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 2097.078125\n",
            "epoch 2, loss 146.410812\n",
            "epoch 3, loss 11.006961\n",
            "epoch 4, loss 0.960649\n",
            "epoch 5, loss 0.182210\n",
            "epoch 6, loss 0.117642\n",
            "epoch 7, loss 0.112370\n",
            "epoch 8, loss 0.111646\n",
            "epoch 9, loss 0.111551\n",
            "epoch 10, loss 0.111628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgAq4yzAFlWA"
      },
      "source": [
        "Since the magnitude of loss is sum and not mean it is quite large, larger LR might cause gradient to fluctuate more, so we need to reduce the LR\n",
        "\n",
        "Also we need more epochs to make the loss converge\n",
        "\n",
        "2. Review the PyTorch documentation to see what loss functions and initialization methods are provided. Replace the loss by Huber’s loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itrizITMFEBl",
        "outputId": "406b558a-53ee-49b5-a23a-fdf9c2bede97"
      },
      "source": [
        "net[0].weight.data.normal_(0, 0.01)\n",
        "net[0].bias.data.fill_(0)\n",
        "\n",
        "loss = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
        "trainer = torch.optim.SGD(params=net.parameters(), lr=0.03)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter:\n",
        "        l = loss(net(X), y)\n",
        "        trainer.zero_grad() ### zero out the grads before backpropagation\n",
        "        l.backward()\n",
        "        trainer.step() ### Performs a single optimization step: basically does w = w - lr * dL/dw and same for bias\n",
        "    ### loss over the entire data\n",
        "    l = loss(net(features), labels)\n",
        "    print(f'epoch {epoch + 1}, loss {l:f}')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 2.226193\n",
            "epoch 2, loss 0.466514\n",
            "epoch 3, loss 0.003010\n",
            "epoch 4, loss 0.000067\n",
            "epoch 5, loss 0.000056\n",
            "epoch 6, loss 0.000056\n",
            "epoch 7, loss 0.000056\n",
            "epoch 8, loss 0.000056\n",
            "epoch 9, loss 0.000056\n",
            "epoch 10, loss 0.000056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RIRYsFaMGLW",
        "outputId": "8f8d98fe-627d-4c22-d4a3-331a6e7bd860"
      },
      "source": [
        "print (net[0].weight.data)\n",
        "print (net[0].bias.data)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.9999, -3.4003]])\n",
            "tensor([4.2001])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQFMVM7Mm4-"
      },
      "source": [
        "3. How do you access the gradient of net[0].weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ5gLJYNMMmF",
        "outputId": "af4ddf6b-078a-439b-fcd6-fc4ed8fe7b2c"
      },
      "source": [
        "net[0].weight.grad"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0008,  0.0036]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "412YBLRtMjXc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}