{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiveIntoDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkzRwgowK8UbV4WhLikeVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/DiveIntoDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBJj7CuQ8ev"
      },
      "source": [
        "## Dive Into Deep Learning\r\n",
        "\r\n",
        "> Self notes on the book: http://www.d2l.ai/\r\n",
        "---\r\n",
        "\r\n",
        "If you are able to devise a solution to the problem that will work 100% of the time, __do not__ use ML!\r\n",
        "\r\n",
        "The supervision comes into play because for choosing the parameters, we (the supervisors) provide the model with a dataset consisting of labeled examples, where each example is matched with the ground-truth label. In probabilistic terms, we typically are interested in estimating the conditional probability of a label given input features. While it is just one among several paradigms within machine learning, supervised learning accounts for the majority of successful applications of machine learning in industry. Partly, that is because many important tasks can be described crisply as estimating the probability of something unknown given a particular set of available data:\r\n",
        "\r\n",
        "Lots of practical problems are well-described regression problems. Predicting the rating that a user will assign to a movie can be thought of as a regression problem and if you designed a great algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar Netflix prize. Predicting the length of stay for patients in the hospital is also a regression problem. A good rule of thumb is that any how much? or how many? problem should suggest regression, such as:\r\n",
        "\r\n",
        "- How many hours will this surgery take?\r\n",
        "\r\n",
        "- How much rainfall will this town have in the next six hours?\r\n",
        "\r\n",
        "NOTE: whole number prediction problems are regression ones, not classification\r\n",
        "\r\n",
        "## Types of Supervised ML\r\n",
        "\r\n",
        "### 1. Regression + 2. Classification - we know\r\n",
        "\r\n",
        "### 2.1 Tagging\r\n",
        "\r\n",
        "Some classification problems fit neatly into the binary or multiclass classification setups. For example, we could train a normal binary classifier to distinguish cats from dogs. Given the current state of computer vision, we can do this easily, with off-the-shelf tools. Nonetheless, no matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters an image of the Town Musicians of Bremen, a popular German fairy tale featuring four animals in Fig. 1.3.3.\r\n",
        "\r\n",
        "![](http://www.d2l.ai/_images/stackedanimals.png)\r\n",
        "\r\n",
        "As you can see, there is a cat in Fig. 1.3.3, and a rooster, a dog, and a donkey, with some trees in the background. Depending on what we want to do with our model ultimately, treating this as a binary classification problem might not make a lot of sense. Instead, we might want to give the model the option of saying the image depicts a cat, a dog, a donkey, and a rooster.\r\n",
        "\r\n",
        "The problem of learning to predict classes that are not mutually exclusive is called multi-label classification. Auto-tagging problems are typically best described as multi-label classification problems. Think of the tags people might apply to posts on a technical blog, e.g., “machine learning”, “technology”, “gadgets”, “programming languages”, “Linux”, “cloud computing”, “AWS”. A typical article might have 5–10 tags applied because these concepts are correlated. Posts about “cloud computing” are likely to mention “AWS” and posts about “machine learning” could also deal with “programming languages”.\r\n",
        "\r\n",
        "We also have to deal with this kind of problem when dealing with the biomedical literature, where correctly tagging articles is important because it allows researchers to do exhaustive reviews of the literature. At the National Library of Medicine, a number of professional annotators go over each article that gets indexed in PubMed to associate it with the relevant terms from MeSH, a collection of roughly 28000 tags. This is a time-consuming process and the annotators typically have a one-year lag between archiving and tagging. Machine learning can be used here to provide provisional tags until each article can have a proper manual review. Indeed, for several years, the BioASQ organization has hosted competitions to do precisely this.\r\n",
        "\r\n",
        "### 3. Search\r\n",
        "\r\n",
        "Sometimes we do not just want to assign each example to a bucket or to a real value. In the field of information retrieval, we want to impose a ranking on a set of items. Take web search for an example. The goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results is most relevant for a particular user. We really care about the ordering of the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning “A B C D E” and “C A B E D”. Even if the result set is the same, the ordering within the set matters.\r\n",
        "\r\n",
        "\r\n",
        "One possible solution to this problem is to first assign to every element in the set a corresponding relevance score and then to retrieve the top-rated elements. PageRank, the original secret sauce behind the Google search engine was an early example of such a scoring system but it was peculiar in that it did not depend on the actual query. Here they relied on a simple relevance filter to identify the set of relevant items and then on PageRank to order those results that contained the query term. Nowadays, search engines use machine learning and behavioral models to obtain query-dependent relevance scores. There are entire academic conferences devoted to this subject.\r\n",
        "\r\n",
        "### 4. Recommender Systems\r\n",
        "\r\n",
        "In the simplest formulations, these systems are trained to estimate some score, such as an estimated rating or the probability of purchase, given a user and an item.\r\n",
        "\r\n",
        "Given such a model, for any given user, we could retrieve the set of objects with the largest scores, which could then be recommended to the user. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores\r\n",
        "\r\n",
        "Despite their tremendous economic value, recommendation systems naively built on top of predictive models suffer some serious conceptual flaws. To start, we only observe censored feedback: users preferentially rate movies that they feel strongly about. For example, on a five-point scale, you might notice that items receive many five and one star ratings but that there are conspicuously few three-star ratings. Moreover, current purchase habits are often a result of the recommendation algorithm currently in place, but learning algorithms do not always take this detail into account. Thus it is possible for feedback loops to form where a recommender system preferentially pushes an item that is then taken to be better (due to greater purchases) and in turn is recommended even more frequently. Many of these problems about how to deal with censoring, incentives, and feedback loops, are important open research questions.\r\n",
        "\r\n",
        "### Sequence learning\r\n",
        "\r\n",
        "So far, we have looked at problems where we have some fixed number of inputs and produce a fixed number of outputs. For example, we considered predicting house prices from a fixed set of features: square footage, number of bedrooms, number of bathrooms, walking time to downtown. We also discussed mapping from an image (of fixed dimension) to the predicted probabilities that it belongs to each of a fixed number of classes, or taking a user ID and a product ID, and predicting a star rating. In these cases, once we feed our fixed-length input into the model to generate an output, the model immediately forgets what it just saw.\r\n",
        "\r\n",
        "This might be fine if our inputs truly all have the same dimensions and if successive inputs truly have nothing to do with each other. But how would we deal with video snippets? In this case, each snippet might consist of a different number of frames. And our guess of what is going on in each frame might be much stronger if we take into account the previous or succeeding frames. Same goes for language. One popular deep learning problem is machine translation: the task of ingesting sentences in some source language and predicting their translation in another language.\r\n",
        "\r\n",
        "These problems also occur in medicine. We might want a model to monitor patients in the intensive care unit and to fire off alerts if their risk of death in the next 24 hours exceeds some threshold. We definitely would not want this model to throw away everything it knows about the patient history each hour and just make its predictions based on the most recent measurements.\r\n",
        "\r\n",
        "These problems are among the most exciting applications of machine learning and they are instances of sequence learning. They require a model to either ingest sequences of inputs or to emit sequences of outputs (or both). Specifically, sequence to sequence learning considers problems where input and output are both variable-length sequences, such as machine translation and transcribing text from the spoken speech. While it is impossible to consider all types of sequence transformations, the following special cases are worth mentioning.\r\n",
        "\r\n",
        "__Tagging and Parsing__. This involves annotating a text sequence with attributes. In other words, the number of inputs and outputs is essentially the same. For instance, we might want to know where the verbs and subjects are. Alternatively, we might want to know which words are the named entities.\r\n",
        "\r\n",
        "__Automatic Speech Recognition__. With speech recognition, the input sequence is an audio recording of a speaker (shown in Fig. 1.3.5), and the output is the textual transcript of what the speaker said. The challenge is that there are many more audio frames (sound is typically sampled at 8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text, since thousands of samples may correspond to a single spoken word. These are sequence to sequence learning problems where the output is much shorter than the input.\r\n",
        "\r\n",
        "__Text to Speech__. This is the inverse of automatic speech recognition. In other words, the input is text and the output is an audio file. In this case, the output is much longer than the input. While it is easy for humans to recognize a bad audio file, this is not quite so trivial for computers.\r\n",
        "\r\n",
        "__Machine Translation__. Unlike the case of speech recognition, where corresponding inputs and outputs occur in the same order (after alignment), in machine translation, order inversion can be vital. In other words, while we are still converting one sequence into another, neither the number of inputs and outputs nor the order of corresponding data examples are assumed to be the same. Consider the following illustrative example of the peculiar tendency of Germans to place the verbs at the end of sentences.\r\n",
        "\r\n",
        "```\r\n",
        "German:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?\r\n",
        "English:          Did you already check out this excellent tutorial?\r\n",
        "Wrong alignment:  Did you yourself already this excellent tutorial looked-at?\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEieofVh0SKV"
      },
      "source": [
        "## Unsupervised Learning\r\n",
        "\r\n",
        "In a completely opposite way, it could be frustrating to work for a boss who has no idea what they want you to do. However, if you plan to be a data scientist, you had better get used to it. The boss might just hand you a giant dump of data and tell you to do some data science with it! This sounds vague because it is. We call this class of problems unsupervised learning, and the type and number of questions we could ask is limited only by our creativity. We will address unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a few of the following questions you might ask.\r\n",
        "\r\n",
        "- Can we find a small number of prototypes that accurately summarize the data? Given a set of photos, can we group them into landscape photos, pictures of dogs, babies, cats, and mountain peaks? Likewise, given a collection of users’ browsing activities, can we group them into users with similar behavior? This problem is typically known as clustering.\r\n",
        "\r\n",
        "- Can we find a small number of parameters that accurately capture the relevant properties of the data? The trajectories of a ball are quite well described by velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as subspace estimation. If the dependence is linear, it is called principal component analysis.\r\n",
        "\r\n",
        "- Is there a representation of (arbitrarily structured) objects in Euclidean space such that symbolic properties can be well matched? This can be used to describe entities and their relations, such as “Rome”  −  “Italy”  +  “France”  =  “Paris”.\r\n",
        "\r\n",
        "- Is there a description of the root causes of much of the data that we observe? For instance, if we have demographic data about house prices, pollution, crime, location, education, and salaries, can we discover how they are related simply based on empirical data? The fields concerned with causality and probabilistic graphical models address this problem.\r\n",
        "\r\n",
        "- Another important and exciting recent development in unsupervised learning is the advent of generative adversarial networks. These give us a procedural way to synthesize data, even complicated structured data like images and audio. The underlying statistical mechanisms are tests to check whether real and fake data are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1GazwOMNQg"
      },
      "source": [
        "## Basic Data Manipulation\r\n",
        "\r\n",
        "To start, we can use arange to create a row vector x containing the first 12 integers starting with 0, though they are created as floats by default. Each of the values in a tensor is called an element of the tensor. For instance, there are 12 elements in the tensor x. Unless otherwise specified, a new tensor will be stored in main memory and designated for CPU-based computation.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMhTF98VQ5Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80ae3ce-c316-418a-c185-4f7a14dc5857"
      },
      "source": [
        "import torch\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "print (tf.__version__)\r\n",
        "print (torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzRRI4QMh0P",
        "outputId": "a3c586ca-75bd-4df6-d1a8-c15c6ea49abd"
      },
      "source": [
        "x = torch.arange(start=0, end=12, step=1)\r\n",
        "\r\n",
        "print (x)\r\n",
        "\r\n",
        "print (x.shape)\r\n",
        "\r\n",
        "print (x.numel())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "torch.Size([12])\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB60MjLVMm7r",
        "outputId": "b13d2735-c50d-4b10-d066-588b3efac34c"
      },
      "source": [
        "X = x.reshape(3, 4)\r\n",
        "print (X)\r\n",
        "\r\n",
        "## automatically infer the first dimension\r\n",
        "X = x.reshape(-1, 4)\r\n",
        "print (X) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJoUaOTzN6VC"
      },
      "source": [
        "Typically, we will want our matrices initialized either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. We can create a tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBz0Q1kCM1tA",
        "outputId": "8e98b27d-1cac-4c5f-e40f-59560840fb72"
      },
      "source": [
        "print (torch.zeros((2,3,4)))\r\n",
        "\r\n",
        "## Similarly, we can create tensors with each element set to 1 as follows:\r\n",
        "\r\n",
        "print (torch.ones((2,3,4)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCRy-5_BN_2w",
        "outputId": "bff23e25-441f-4100-9648-f952e87454be"
      },
      "source": [
        "print (torch.randn((2,3,4)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.1200, -0.6909,  0.1441,  0.5265],\n",
            "         [ 0.0371,  0.5953, -1.8176, -0.8631],\n",
            "         [ 1.2461, -0.0912,  1.5808,  0.4090]],\n",
            "\n",
            "        [[-0.8581, -1.9528, -0.2606,  0.7925],\n",
            "         [-1.9569, -0.6926,  0.1306,  0.1748],\n",
            "         [-0.7900, -1.0730,  0.0062, -0.2780]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7r_ePh7PBXF"
      },
      "source": [
        "We can also specify the exact values for each element in the desired tensor by supplying a Python list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis 0, and the inner list to axis 1.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHfwNTrOuZN",
        "outputId": "0bc23b6f-12b1-422f-b6bd-90d28cbee855"
      },
      "source": [
        "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqgH550PPD2Z",
        "outputId": "8b9ca0b7-84e2-47ad-e347-30f4f7685445"
      },
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\r\n",
        "y = torch.tensor([2, 2, 2, 2])\r\n",
        "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgfod02MTRCA",
        "outputId": "8f6eaf33-baf5-43db-deb6-53fe872b1572"
      },
      "source": [
        "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\r\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\r\n",
        "\r\n",
        "print (X)\r\n",
        "\r\n",
        "print (Y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([[2., 1., 4., 3.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [4., 3., 2., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFvOeDmlTtAM",
        "outputId": "d0c7f2f7-f9b5-46fb-eb8a-985ecb94d0b1"
      },
      "source": [
        "torch.cat(tensors=[X, Y], dim=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.],\n",
              "        [ 2.,  1.,  4.,  3.],\n",
              "        [ 1.,  2.,  3.,  4.],\n",
              "        [ 4.,  3.,  2.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7svhcO4T1MX",
        "outputId": "85fac6a2-bff3-4d69-e9e1-4dadfdd9564f"
      },
      "source": [
        "torch.cat((X, Y), dim=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9SZccD-eR3u"
      },
      "source": [
        "### Broadcasting Mechanism\r\n",
        "\r\n",
        "In the above section, we saw how to perform elementwise operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays.\r\n",
        "\r\n",
        "In most cases, we broadcast along an axis where an array initially only has length 1, such as in the following example:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJQSaGPtUH_y",
        "outputId": "d88305de-c4a9-4f82-bfbf-88ac04cc791a"
      },
      "source": [
        "a = torch.arange(3).reshape((3,1))\r\n",
        "print (a)\r\n",
        "b = torch.arange(2).reshape((1, 2))\r\n",
        "print (b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]])\n",
            "tensor([[0, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V-UY17zVelhu",
        "outputId": "e1858f71-845c-4c98-a595-6a422627893f"
      },
      "source": [
        "a+b\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "The broadcasting happens as:\r\n",
        "a -> [[0, 0]\r\n",
        "     [1, 1]\r\n",
        "     [2, 2]]\r\n",
        "b -> [[0, 1]\r\n",
        "     [0, 1]\r\n",
        "     [0, 1]]   \r\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe broadcasting happens as:\\na -> [[0, 0]\\n     [1, 1]\\n     [2, 2]]\\nb -> [[0, 1]\\n     [0, 1]\\n     [0, 1]]   \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7_7I9Rfc8A"
      },
      "source": [
        "### Indexing and slicing\r\n",
        "\r\n",
        "Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMfORByvew0m",
        "outputId": "2f98fc52-e036-432b-e1b4-2451d1b887f6"
      },
      "source": [
        "print (X)\r\n",
        "\r\n",
        "print (X[-1])\r\n",
        "\r\n",
        "print (X[1:3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([ 8.,  9., 10., 11.])\n",
            "tensor([[ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96XTzZFif8Nq",
        "outputId": "b36d31d4-b9dc-4883-e9bd-3ef8639f9b40"
      },
      "source": [
        "X[1, 2] = 9\r\n",
        "print (X)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  9.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M4bvm_0gHc1",
        "outputId": "cadb0c2f-c3b7-489d-f9f6-462f2f336623"
      },
      "source": [
        "X[0:2, :] = 12\r\n",
        "\r\n",
        "print (X)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[12., 12., 12., 12.],\n",
            "        [12., 12., 12., 12.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXF-aQbm8hKz"
      },
      "source": [
        "### Saving memeory\r\n",
        "\r\n",
        "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Python’s id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v0YsUS5gO_1",
        "outputId": "1bb0302a-27ae-42d5-efd4-592d72d62bff"
      },
      "source": [
        "before = id(Y)\r\n",
        "print (before)\r\n",
        "Y=Y+X\r\n",
        "print (id(Y))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "139814611028720\n",
            "139814603673104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WNjxT5T8_Cs"
      },
      "source": [
        "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters.\r\n",
        "\r\n",
        "Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., Y[:] = <expression>. To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJiDCWO8t6a",
        "outputId": "1ec6dc3d-b652-43e5-dc6f-0d197f1939f5"
      },
      "source": [
        "Z = torch.zeros_like(Y)\r\n",
        "print (Z)\r\n",
        "print('id(Z):', id(Z))\r\n",
        "Z[:] = X + Y\r\n",
        "print('id(Z):', id(Z))\r\n",
        "Z = X+Y\r\n",
        "print('id(Z):', id(Z))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "id(Z): 139814603695840\n",
            "id(Z): 139814603695840\n",
            "id(Z): 139814603705232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2vm-2tI_jpC"
      },
      "source": [
        "If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X += Y to reduce the memory overhead of the operation.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGjqrm7F_R2s",
        "outputId": "bf171080-483d-4e4a-85a1-e30e371c0f13"
      },
      "source": [
        "before = id(X)\r\n",
        "X = X + Y\r\n",
        "id(X) == before"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjFb86mE_lJM",
        "outputId": "776561b3-5ccd-431c-98d7-b9fa1dec6e0c"
      },
      "source": [
        "before = id(X)\r\n",
        "X +=  Y\r\n",
        "id(X) == before"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pw3MafZ_sMO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}