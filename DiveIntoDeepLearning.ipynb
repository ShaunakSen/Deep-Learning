{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiveIntoDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMH1jSW+D5gHzEjRBdUPaDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/DiveIntoDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBJj7CuQ8ev"
      },
      "source": [
        "## Dive Into Deep Learning\n",
        "\n",
        "> Self notes on the book: http://www.d2l.ai/\n",
        "---\n",
        "\n",
        "If you are able to devise a solution to the problem that will work 100% of the time, __do not__ use ML!\n",
        "\n",
        "The supervision comes into play because for choosing the parameters, we (the supervisors) provide the model with a dataset consisting of labeled examples, where each example is matched with the ground-truth label. In probabilistic terms, we typically are interested in estimating the conditional probability of a label given input features. While it is just one among several paradigms within machine learning, supervised learning accounts for the majority of successful applications of machine learning in industry. Partly, that is because many important tasks can be described crisply as estimating the probability of something unknown given a particular set of available data:\n",
        "\n",
        "Lots of practical problems are well-described regression problems. Predicting the rating that a user will assign to a movie can be thought of as a regression problem and if you designed a great algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar Netflix prize. Predicting the length of stay for patients in the hospital is also a regression problem. A good rule of thumb is that any how much? or how many? problem should suggest regression, such as:\n",
        "\n",
        "- How many hours will this surgery take?\n",
        "\n",
        "- How much rainfall will this town have in the next six hours?\n",
        "\n",
        "NOTE: whole number prediction problems are regression ones, not classification\n",
        "\n",
        "## Types of Supervised ML\n",
        "\n",
        "### 1. Regression + 2. Classification - we know\n",
        "\n",
        "### 2.1 Tagging\n",
        "\n",
        "Some classification problems fit neatly into the binary or multiclass classification setups. For example, we could train a normal binary classifier to distinguish cats from dogs. Given the current state of computer vision, we can do this easily, with off-the-shelf tools. Nonetheless, no matter how accurate our model gets, we might find ourselves in trouble when the classifier encounters an image of the Town Musicians of Bremen, a popular German fairy tale featuring four animals in Fig. 1.3.3.\n",
        "\n",
        "![](http://www.d2l.ai/_images/stackedanimals.png)\n",
        "\n",
        "As you can see, there is a cat in Fig. 1.3.3, and a rooster, a dog, and a donkey, with some trees in the background. Depending on what we want to do with our model ultimately, treating this as a binary classification problem might not make a lot of sense. Instead, we might want to give the model the option of saying the image depicts a cat, a dog, a donkey, and a rooster.\n",
        "\n",
        "The problem of learning to predict classes that are not mutually exclusive is called multi-label classification. Auto-tagging problems are typically best described as multi-label classification problems. Think of the tags people might apply to posts on a technical blog, e.g., “machine learning”, “technology”, “gadgets”, “programming languages”, “Linux”, “cloud computing”, “AWS”. A typical article might have 5–10 tags applied because these concepts are correlated. Posts about “cloud computing” are likely to mention “AWS” and posts about “machine learning” could also deal with “programming languages”.\n",
        "\n",
        "We also have to deal with this kind of problem when dealing with the biomedical literature, where correctly tagging articles is important because it allows researchers to do exhaustive reviews of the literature. At the National Library of Medicine, a number of professional annotators go over each article that gets indexed in PubMed to associate it with the relevant terms from MeSH, a collection of roughly 28000 tags. This is a time-consuming process and the annotators typically have a one-year lag between archiving and tagging. Machine learning can be used here to provide provisional tags until each article can have a proper manual review. Indeed, for several years, the BioASQ organization has hosted competitions to do precisely this.\n",
        "\n",
        "### 3. Search\n",
        "\n",
        "Sometimes we do not just want to assign each example to a bucket or to a real value. In the field of information retrieval, we want to impose a ranking on a set of items. Take web search for an example. The goal is less to determine whether a particular page is relevant for a query, but rather, which one of the plethora of search results is most relevant for a particular user. We really care about the ordering of the relevant search results and our learning algorithm needs to produce ordered subsets of elements from a larger set. In other words, if we are asked to produce the first 5 letters from the alphabet, there is a difference between returning “A B C D E” and “C A B E D”. Even if the result set is the same, the ordering within the set matters.\n",
        "\n",
        "\n",
        "One possible solution to this problem is to first assign to every element in the set a corresponding relevance score and then to retrieve the top-rated elements. PageRank, the original secret sauce behind the Google search engine was an early example of such a scoring system but it was peculiar in that it did not depend on the actual query. Here they relied on a simple relevance filter to identify the set of relevant items and then on PageRank to order those results that contained the query term. Nowadays, search engines use machine learning and behavioral models to obtain query-dependent relevance scores. There are entire academic conferences devoted to this subject.\n",
        "\n",
        "### 4. Recommender Systems\n",
        "\n",
        "In the simplest formulations, these systems are trained to estimate some score, such as an estimated rating or the probability of purchase, given a user and an item.\n",
        "\n",
        "Given such a model, for any given user, we could retrieve the set of objects with the largest scores, which could then be recommended to the user. Production systems are considerably more advanced and take detailed user activity and item characteristics into account when computing such scores\n",
        "\n",
        "Despite their tremendous economic value, recommendation systems naively built on top of predictive models suffer some serious conceptual flaws. To start, we only observe censored feedback: users preferentially rate movies that they feel strongly about. For example, on a five-point scale, you might notice that items receive many five and one star ratings but that there are conspicuously few three-star ratings. Moreover, current purchase habits are often a result of the recommendation algorithm currently in place, but learning algorithms do not always take this detail into account. Thus it is possible for feedback loops to form where a recommender system preferentially pushes an item that is then taken to be better (due to greater purchases) and in turn is recommended even more frequently. Many of these problems about how to deal with censoring, incentives, and feedback loops, are important open research questions.\n",
        "\n",
        "### Sequence learning\n",
        "\n",
        "So far, we have looked at problems where we have some fixed number of inputs and produce a fixed number of outputs. For example, we considered predicting house prices from a fixed set of features: square footage, number of bedrooms, number of bathrooms, walking time to downtown. We also discussed mapping from an image (of fixed dimension) to the predicted probabilities that it belongs to each of a fixed number of classes, or taking a user ID and a product ID, and predicting a star rating. In these cases, once we feed our fixed-length input into the model to generate an output, the model immediately forgets what it just saw.\n",
        "\n",
        "This might be fine if our inputs truly all have the same dimensions and if successive inputs truly have nothing to do with each other. But how would we deal with video snippets? In this case, each snippet might consist of a different number of frames. And our guess of what is going on in each frame might be much stronger if we take into account the previous or succeeding frames. Same goes for language. One popular deep learning problem is machine translation: the task of ingesting sentences in some source language and predicting their translation in another language.\n",
        "\n",
        "These problems also occur in medicine. We might want a model to monitor patients in the intensive care unit and to fire off alerts if their risk of death in the next 24 hours exceeds some threshold. We definitely would not want this model to throw away everything it knows about the patient history each hour and just make its predictions based on the most recent measurements.\n",
        "\n",
        "These problems are among the most exciting applications of machine learning and they are instances of sequence learning. They require a model to either ingest sequences of inputs or to emit sequences of outputs (or both). Specifically, sequence to sequence learning considers problems where input and output are both variable-length sequences, such as machine translation and transcribing text from the spoken speech. While it is impossible to consider all types of sequence transformations, the following special cases are worth mentioning.\n",
        "\n",
        "__Tagging and Parsing__. This involves annotating a text sequence with attributes. In other words, the number of inputs and outputs is essentially the same. For instance, we might want to know where the verbs and subjects are. Alternatively, we might want to know which words are the named entities.\n",
        "\n",
        "__Automatic Speech Recognition__. With speech recognition, the input sequence is an audio recording of a speaker (shown in Fig. 1.3.5), and the output is the textual transcript of what the speaker said. The challenge is that there are many more audio frames (sound is typically sampled at 8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text, since thousands of samples may correspond to a single spoken word. These are sequence to sequence learning problems where the output is much shorter than the input.\n",
        "\n",
        "__Text to Speech__. This is the inverse of automatic speech recognition. In other words, the input is text and the output is an audio file. In this case, the output is much longer than the input. While it is easy for humans to recognize a bad audio file, this is not quite so trivial for computers.\n",
        "\n",
        "__Machine Translation__. Unlike the case of speech recognition, where corresponding inputs and outputs occur in the same order (after alignment), in machine translation, order inversion can be vital. In other words, while we are still converting one sequence into another, neither the number of inputs and outputs nor the order of corresponding data examples are assumed to be the same. Consider the following illustrative example of the peculiar tendency of Germans to place the verbs at the end of sentences.\n",
        "\n",
        "```\n",
        "German:           Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?\n",
        "English:          Did you already check out this excellent tutorial?\n",
        "Wrong alignment:  Did you yourself already this excellent tutorial looked-at?\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEieofVh0SKV"
      },
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "In a completely opposite way, it could be frustrating to work for a boss who has no idea what they want you to do. However, if you plan to be a data scientist, you had better get used to it. The boss might just hand you a giant dump of data and tell you to do some data science with it! This sounds vague because it is. We call this class of problems unsupervised learning, and the type and number of questions we could ask is limited only by our creativity. We will address unsupervised learning techniques in later chapters. To whet your appetite for now, we describe a few of the following questions you might ask.\n",
        "\n",
        "- Can we find a small number of prototypes that accurately summarize the data? Given a set of photos, can we group them into landscape photos, pictures of dogs, babies, cats, and mountain peaks? Likewise, given a collection of users’ browsing activities, can we group them into users with similar behavior? This problem is typically known as clustering.\n",
        "\n",
        "- Can we find a small number of parameters that accurately capture the relevant properties of the data? The trajectories of a ball are quite well described by velocity, diameter, and mass of the ball. Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as subspace estimation. If the dependence is linear, it is called principal component analysis.\n",
        "\n",
        "- Is there a representation of (arbitrarily structured) objects in Euclidean space such that symbolic properties can be well matched? This can be used to describe entities and their relations, such as “Rome”  −  “Italy”  +  “France”  =  “Paris”.\n",
        "\n",
        "- Is there a description of the root causes of much of the data that we observe? For instance, if we have demographic data about house prices, pollution, crime, location, education, and salaries, can we discover how they are related simply based on empirical data? The fields concerned with causality and probabilistic graphical models address this problem.\n",
        "\n",
        "- Another important and exciting recent development in unsupervised learning is the advent of generative adversarial networks. These give us a procedural way to synthesize data, even complicated structured data like images and audio. The underlying statistical mechanisms are tests to check whether real and fake data are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1GazwOMNQg"
      },
      "source": [
        "## Basic Data Manipulation\n",
        "\n",
        "To start, we can use arange to create a row vector x containing the first 12 integers starting with 0, though they are created as floats by default. Each of the values in a tensor is called an element of the tensor. For instance, there are 12 elements in the tensor x. Unless otherwise specified, a new tensor will be stored in main memory and designated for CPU-based computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMhTF98VQ5Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90edd294-922b-4ada-ba6c-7e621f31a230"
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "print (tf.__version__)\n",
        "print (torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBzRRI4QMh0P",
        "outputId": "29760c3e-09f0-47cf-bc74-8511b0b525d5"
      },
      "source": [
        "x = torch.arange(start=0, end=12, step=1)\n",
        "\n",
        "print (x)\n",
        "\n",
        "print (x.shape)\n",
        "\n",
        "print (x.numel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "torch.Size([12])\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB60MjLVMm7r",
        "outputId": "6d6d0ac3-3360-48e1-911e-11d9dc7868ea"
      },
      "source": [
        "X = x.reshape(3, 4)\n",
        "print (X)\n",
        "\n",
        "## automatically infer the first dimension\n",
        "X = x.reshape(-1, 4)\n",
        "print (X) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJoUaOTzN6VC"
      },
      "source": [
        "Typically, we will want our matrices initialized either with zeros, ones, some other constants, or numbers randomly sampled from a specific distribution. We can create a tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4) as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBz0Q1kCM1tA",
        "outputId": "c400dda2-2a18-43f4-9d68-fc58ed4879a3"
      },
      "source": [
        "print (torch.zeros((2,3,4)))\n",
        "\n",
        "## Similarly, we can create tensors with each element set to 1 as follows:\n",
        "\n",
        "print (torch.ones((2,3,4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCRy-5_BN_2w",
        "outputId": "6731582a-3862-491a-a655-fed6f7875de1"
      },
      "source": [
        "print (torch.randn((2,3,4)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.8856,  0.5143,  0.0783, -1.3168],\n",
            "         [ 0.1134,  1.3689,  0.2637,  0.9902],\n",
            "         [-0.0229, -0.1740, -1.0100,  0.3572]],\n",
            "\n",
            "        [[ 1.1134,  1.1219, -0.1079,  2.6444],\n",
            "         [-0.1651, -1.3910, -0.4598,  0.2072],\n",
            "         [-0.8078,  0.3612, -1.1477,  1.3847]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7r_ePh7PBXF"
      },
      "source": [
        "We can also specify the exact values for each element in the desired tensor by supplying a Python list (or list of lists) containing the numerical values. Here, the outermost list corresponds to axis 0, and the inner list to axis 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHfwNTrOuZN",
        "outputId": "cddf035c-6781-44dd-d2e6-b3e3f73fd033"
      },
      "source": [
        "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqgH550PPD2Z",
        "outputId": "7d41a5c3-e8ed-4267-ab20-445b3ee581a0"
      },
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\n",
        "y = torch.tensor([2, 2, 2, 2])\n",
        "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgfod02MTRCA",
        "outputId": "4570dbe5-fdb4-4f56-fb5d-523e58a6dd0b"
      },
      "source": [
        "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "\n",
        "print (X)\n",
        "\n",
        "print (Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([[2., 1., 4., 3.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [4., 3., 2., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFvOeDmlTtAM",
        "outputId": "d5d80572-a20b-4d62-d1bb-a6e70c0c7847"
      },
      "source": [
        "torch.cat(tensors=[X, Y], dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.],\n",
              "        [ 2.,  1.,  4.,  3.],\n",
              "        [ 1.,  2.,  3.,  4.],\n",
              "        [ 4.,  3.,  2.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7svhcO4T1MX",
        "outputId": "46493101-6658-4fb1-810b-8010c0e89846"
      },
      "source": [
        "torch.cat((X, Y), dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9SZccD-eR3u"
      },
      "source": [
        "### Broadcasting Mechanism\n",
        "\n",
        "In the above section, we saw how to perform elementwise operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays.\n",
        "\n",
        "In most cases, we broadcast along an axis where an array initially only has length 1, such as in the following example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJQSaGPtUH_y",
        "outputId": "6ba95fb3-d32a-4e1d-c4b5-efa9f4f6f9b7"
      },
      "source": [
        "a = torch.arange(3).reshape((3,1))\n",
        "print (a)\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "print (b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]])\n",
            "tensor([[0, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V-UY17zVelhu",
        "outputId": "d552fd52-2935-4817-bb32-a31db412ccac"
      },
      "source": [
        "a+b\n",
        "\n",
        "\"\"\"\n",
        "The broadcasting happens as:\n",
        "a -> [[0, 0]\n",
        "     [1, 1]\n",
        "     [2, 2]]\n",
        "b -> [[0, 1]\n",
        "     [0, 1]\n",
        "     [0, 1]]   \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe broadcasting happens as:\\na -> [[0, 0]\\n     [1, 1]\\n     [2, 2]]\\nb -> [[0, 1]\\n     [0, 1]\\n     [0, 1]]   \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7_7I9Rfc8A"
      },
      "source": [
        "### Indexing and slicing\n",
        "\n",
        "Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMfORByvew0m",
        "outputId": "7d479d8c-63d2-44f9-d3b7-d43261e861d7"
      },
      "source": [
        "print (X)\n",
        "\n",
        "print (X[-1])\n",
        "\n",
        "print (X[1:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "tensor([ 8.,  9., 10., 11.])\n",
            "tensor([[ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96XTzZFif8Nq",
        "outputId": "648e98c8-df19-4b6e-f9fc-e28dc39a538f"
      },
      "source": [
        "X[1, 2] = 9\n",
        "print (X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  9.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M4bvm_0gHc1",
        "outputId": "b0279a7b-0401-4cb6-bd49-5c777ee0046f"
      },
      "source": [
        "X[0:2, :] = 12\n",
        "\n",
        "print (X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[12., 12., 12., 12.],\n",
            "        [12., 12., 12., 12.],\n",
            "        [ 8.,  9., 10., 11.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXF-aQbm8hKz"
      },
      "source": [
        "### Saving memeory\n",
        "\n",
        "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Python’s id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v0YsUS5gO_1",
        "outputId": "d9e84447-1e69-430d-c65d-91fd4e04cc47"
      },
      "source": [
        "before = id(Y)\n",
        "print (before)\n",
        "Y=Y+X\n",
        "print (id(Y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140482362522832\n",
            "140482362606608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WNjxT5T8_Cs"
      },
      "source": [
        "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters.\n",
        "\n",
        "Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., Y[:] = <expression>. To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJiDCWO8t6a",
        "outputId": "518d999e-e406-4ec2-ed0e-5fdd3cf35cb5"
      },
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print (Z)\n",
        "print('id(Z):', id(Z))\n",
        "Z[:] = X + Y\n",
        "print('id(Z):', id(Z))\n",
        "Z = X+Y\n",
        "print('id(Z):', id(Z))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "id(Z): 140482362646192\n",
            "id(Z): 140482362646192\n",
            "id(Z): 140482362646352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2vm-2tI_jpC"
      },
      "source": [
        "If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X += Y to reduce the memory overhead of the operation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGjqrm7F_R2s",
        "outputId": "c7d0c16b-455a-4f6a-9aa3-d4110634a9ab"
      },
      "source": [
        "before = id(X)\n",
        "X = X + Y\n",
        "id(X) == before"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjFb86mE_lJM",
        "outputId": "86a603af-09a5-42ee-92fc-91cbbb2206be"
      },
      "source": [
        "before = id(X)\n",
        "X +=  Y\n",
        "id(X) == before"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTKxYwGkRg1G"
      },
      "source": [
        "## Basic Linear Algebra\n",
        "\n",
        "### Dot Product\n",
        "\n",
        "one of the most fundamental operations is the dot product. Given two vectors  x,y their dot product\n",
        "\n",
        "$\\mathbf{x}^\\top \\mathbf{y}$ or $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$\n",
        "is a sum over the products of the elements at the same position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pw3MafZ_sMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2e7da4-9691-43ea-997e-34c5e2a8fda5"
      },
      "source": [
        "print (x)\n",
        "y = torch.ones(4, dtype=torch.float32)\n",
        "\n",
        "print (x.dot(y))\n",
        "\n",
        "### Note that we can express the dot product of two vectors equivalently by performing an elementwise multiplication and then a sum:\n",
        "\n",
        "print (torch.sum(x*y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 4., 8.])\n",
            "tensor(15.)\n",
            "tensor(15.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxcS0rFrSZlU"
      },
      "source": [
        "Dot products are useful in a wide range of contexts. For example, given some set of values, and a set of wts the dot product bw these vectors give us a weighted sum. When the wts are non-negative and sum upto 1 the dot prod gives us the weighted avg\n",
        "\n",
        "After normalizing two vectors to have the unit length, the dot products express the cosine of the angle between them. We will formally introduce this notion of length later in this section.\n",
        "\n",
        "### Matrix-Vector Products\n",
        "\n",
        "Now that we know how to calculate dot products, we can begin to understand matrix-vector products. Recall the matrix  $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and the vector $\\mathbf{x} \\in \\mathbb{R}^n$ \n",
        "\n",
        "$\\begin{split}\\mathbf{A}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_m \\\\\n",
        "\\end{bmatrix},\\end{split}$\n",
        "\n",
        "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the ith row of the matrix A. The matrix-vector product Ax is simply a column vector of length  m , whose  ith  element is the dot product  $\\mathbf{a}^\\top_i \\mathbf{x}$\n",
        "\n",
        "$\\begin{split}\\mathbf{A}\\mathbf{x}\n",
        "= \\begin{bmatrix}\n",
        "\\mathbf{a}^\\top_{1} \\\\\n",
        "\\mathbf{a}^\\top_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^\\top_m \\\\\n",
        "\\end{bmatrix}\\mathbf{x}\n",
        "= \\begin{bmatrix}\n",
        " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
        " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
        "\\vdots\\\\\n",
        " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
        "\\end{bmatrix}.\\end{split}$\n",
        "\n",
        "We can think of multiplication by a matrix  A as a transformation that projects vectors (here x) from  $\\mathbb{R}^n$ to $\\mathbb{R}^m$. These transformations turn out to be remarkably useful. For example, we can represent rotations as multiplications by a square matrix. As we will see in subsequent chapters, we can also use matrix-vector products to describe the most intensive calculations required when computing each layer in a neural network given the values of the previous layer.\n",
        "\n",
        "Expressing matrix-vector products in code with tensors, we use the same dot function as for dot products. When we call np.dot(A, x) with a matrix A and a vector x, the matrix-vector product is performed. Note that the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFxOAxynR6YK",
        "outputId": "80f90a44-324a-4081-dfa3-6ddae902596f"
      },
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "\n",
        "\n",
        "print (A.shape, x.shape)\n",
        "\n",
        "print (A)\n",
        "\n",
        "print (x)\n",
        "\n",
        "print (torch.mv(input=A, vec=x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 4]) torch.Size([4])\n",
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "tensor([1., 2., 4., 8.])\n",
            "tensor([ 34.,  94., 154., 214., 274.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJvNloGpZee8"
      },
      "source": [
        "### Matrix Multiplication\n",
        "\n",
        "![](https://i.imgur.com/8lGoukC.png)\n",
        "\n",
        "#### Walkthrough\n",
        "\n",
        "![](https://i.imgur.com/YPiBqU8.jpeg)\n",
        "\n",
        "![](https://i.imgur.com/YPiBqU8.jpeg)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnAkZ0RbY8lp",
        "outputId": "33d417d7-b9ad-42c4-e992-1ef2f716ad54"
      },
      "source": [
        "print (A)\n",
        "\n",
        "B = torch.ones((4,3))\n",
        "\n",
        "print (A.shape, B.shape)\n",
        "mul = torch.mm(A, B)\n",
        "print (mul)\n",
        "print (mul.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "torch.Size([5, 4]) torch.Size([4, 3])\n",
            "tensor([[ 6.,  6.,  6.],\n",
            "        [22., 22., 22.],\n",
            "        [38., 38., 38.],\n",
            "        [54., 54., 54.],\n",
            "        [70., 70., 70.]])\n",
            "torch.Size([5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdaGhEkcvSA8",
        "outputId": "eb108ec9-9dff-4074-c01e-66d2d9b0d39d"
      },
      "source": [
        "### a is each row vector; extract the one at idx 1\n",
        "a_2 = A[1, :]\n",
        "\n",
        "print (a_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4., 5., 6., 7.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQWBLkUFv0hE",
        "outputId": "bfd65eba-1ccb-4560-d014-458cef8b1ee8"
      },
      "source": [
        "### b is each col vector; extract the last one: b_m\n",
        "b_m = B[:, -1]\n",
        "\n",
        "print (b_m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx3Zrc2vwKiI",
        "outputId": "ebe0b7d6-741b-44b3-fc7f-5882b1245958"
      },
      "source": [
        "### a2_T . b_m should give the result at pos 2, m\n",
        "\n",
        "print (a_2.T.dot(b_m))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(22.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc99Do5HwQJc",
        "outputId": "e117baa7-5b94-4e2d-ad86-2e9e107332a7"
      },
      "source": [
        "print (mul)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.,  6.,  6.],\n",
            "        [22., 22., 22.],\n",
            "        [38., 38., 38.],\n",
            "        [54., 54., 54.],\n",
            "        [70., 70., 70.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny6VcmB44k7O"
      },
      "source": [
        "### Norms\n",
        "\n",
        "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components.\n",
        "\n",
        "You might notice that norms sound a lot like measures of distance. And if you remember Euclidean distances (think Pythagoras’ theorem) from grade school, then the concepts of non-negativity and the triangle inequality might ring a bell. In fact, the Euclidean distance is a norm:\n",
        "\n",
        "specifically it is the  L2  norm. Suppose that the elements in the  n -dimensional vector  x  are  x1,…,xn . The  L2  norm of  x  is the square root of the sum of the squares of the vector elements:\n",
        "\n",
        "$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$\n",
        "\n",
        "where the subscript  2  is often omitted in  L2  norms, i.e., $\\|\\mathbf{x}\\|$ is  equivalent to $\\|\\mathbf{x}\\|_2$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Jk0JO0wsI4",
        "outputId": "47e5a73e-a3e1-4702-e4cf-f7b4700c84a1"
      },
      "source": [
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHhmigt05O4w"
      },
      "source": [
        "In deep learning, we work more often with the squared  L2  norm. You will also frequently encounter the  L1  norm, which is expressed as the sum of the absolute values of the vector elements:\n",
        "\n",
        "$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$\n",
        "\n",
        "As compared with the  L2  norm, it is less influenced by outliers. To calculate the  L1  norm, we compose the absolute value function with a sum over the elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qr-pYGb5HNK",
        "outputId": "99eb696d-0550-4b09-85f7-995902335775"
      },
      "source": [
        "torch.abs(u).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oABEOPdz8WBH"
      },
      "source": [
        "Both the  L2  norm and the  L1  norm are special cases of the more general  Lp  norm:\n",
        "\n",
        "$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.|$\n",
        "\n",
        "Analogous to  L2  norms of vectors, the Frobenius norm of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
        "\n",
        "$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$\n",
        "\n",
        "The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an  L2  norm of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSRHFAEv5XME",
        "outputId": "826244ef-b9bf-4891-ae68-c1c639c2caf7"
      },
      "source": [
        "print (A)\n",
        "torch.norm(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(49.6991)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1huW-719815F"
      },
      "source": [
        "While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; minimize the distance between predictions and the ground-truth observations. Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, the objectives, perhaps the most important components of deep learning algorithms (besides the data), are expressed as norms.\n",
        "\n",
        "### Exercises\n",
        "\n",
        "6. Run A / A.sum(axis=1) and see what happens. Can you analyze the reason?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBt3e6mv8sgR",
        "outputId": "39af3db1-5e29-4710-c76c-9facb7590be4"
      },
      "source": [
        "print (A)\n",
        "\n",
        "print (A.shape)\n",
        "\n",
        "print (A.sum(axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n",
            "torch.Size([5, 4])\n",
            "tensor([ 6., 22., 38., 54., 70.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba2jBr9m-ucA",
        "outputId": "dfba42f1-0321-462f-9063-677780cdeec3"
      },
      "source": [
        "A.size(), A.sum(axis=1).size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), torch.Size([5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdu9s725-Of7"
      },
      "source": [
        "## A/A.sum(axis=1) -> error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dBuofki_Y94"
      },
      "source": [
        "This will be fine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnHz5L5a-j0U",
        "outputId": "5c7e5298-4e61-4e7f-fb2c-c7f8fd3c67b3"
      },
      "source": [
        "B = torch.arange(25, dtype = torch.float32).reshape(5, 5)\n",
        "print (B)\n",
        "print (B.sum(axis=1))\n",
        "B / B.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
            "        [ 5.,  6.,  7.,  8.,  9.],\n",
            "        [10., 11., 12., 13., 14.],\n",
            "        [15., 16., 17., 18., 19.],\n",
            "        [20., 21., 22., 23., 24.]])\n",
            "tensor([ 10.,  35.,  60.,  85., 110.])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0286, 0.0333, 0.0353, 0.0364],\n",
              "        [0.5000, 0.1714, 0.1167, 0.0941, 0.0818],\n",
              "        [1.0000, 0.3143, 0.2000, 0.1529, 0.1273],\n",
              "        [1.5000, 0.4571, 0.2833, 0.2118, 0.1727],\n",
              "        [2.0000, 0.6000, 0.3667, 0.2706, 0.2182]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxmAFhBP_hNQ"
      },
      "source": [
        "8. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along axis 0, 1, and 2?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7Zf8xV_Txg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599e3297-7d3e-42cc-f577-403aa5eb5509"
      },
      "source": [
        "a = torch.randint(low=0, high=9, size=(2,3,4))\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[8, 0, 5, 8],\n",
              "         [2, 2, 8, 5],\n",
              "         [3, 0, 2, 8]],\n",
              "\n",
              "        [[6, 0, 7, 4],\n",
              "         [5, 1, 7, 7],\n",
              "         [0, 5, 1, 1]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKP0oPnkT2_V",
        "outputId": "1d02cd8a-879f-4a61-bafe-454ec04eb0b2"
      },
      "source": [
        "print (a.shape)\n",
        "\n",
        "print (a.sum(axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 4])\n",
            "tensor([[14,  0, 12, 12],\n",
            "        [ 7,  3, 15, 12],\n",
            "        [ 3,  5,  3,  9]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ268RB9T87C",
        "outputId": "18aa212f-10f8-4d19-bac4-8a0d43902e67"
      },
      "source": [
        "### shape of a is (2,3,4) i.e 2 separate 3x4 matrics\n",
        "a[0], a[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[8, 0, 5, 8],\n",
              "         [2, 2, 8, 5],\n",
              "         [3, 0, 2, 8]]), tensor([[6, 0, 7, 4],\n",
              "         [5, 1, 7, 7],\n",
              "         [0, 5, 1, 1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8G2OYhDUCvZ",
        "outputId": "bafd12e2-d01c-4d7b-d8be-46c4064ba39d"
      },
      "source": [
        "## a.sum(axis=0) will sum up the 2 3x4 matrices resulting in a 3x4 matrix\n",
        "a.sum(axis=0).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szp5dynXUPEE",
        "outputId": "19ec1138-c401-44ee-c6b6-02f256939850"
      },
      "source": [
        "a[0]+a[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14,  0, 12, 12],\n",
              "        [ 7,  3, 15, 12],\n",
              "        [ 3,  5,  3,  9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxADjUv1UzNT",
        "outputId": "6efabb1d-0a9a-465f-acd0-9fafec267c1d"
      },
      "source": [
        "a.sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14,  0, 12, 12],\n",
              "        [ 7,  3, 15, 12],\n",
              "        [ 3,  5,  3,  9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4YSh7UjZraj",
        "outputId": "92558d1b-6f6a-474f-c64f-4bbb33007fdb"
      },
      "source": [
        "a.sum(axis=1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T_SSPJ5aCwA",
        "outputId": "d67b2efd-4a91-47cd-a43f-86179cb1c97d"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[8, 0, 5, 8],\n",
              "         [2, 2, 8, 5],\n",
              "         [3, 0, 2, 8]],\n",
              "\n",
              "        [[6, 0, 7, 4],\n",
              "         [5, 1, 7, 7],\n",
              "         [0, 5, 1, 1]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8McnX_nZvvC",
        "outputId": "1d8064bd-ee55-4f89-9626-e5206768f615"
      },
      "source": [
        "### 2x4 shape: 2 rows for 2 matrices; 4 elems each for the sum of each matrix along the columns\n",
        "a.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[13,  2, 15, 21],\n",
              "        [11,  6, 15, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY41-rTGZ5RY",
        "outputId": "f7fc0919-a81b-490b-8e8e-ca9511c204ac"
      },
      "source": [
        "a.sum(axis=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[21, 17, 13],\n",
              "        [17, 20,  7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBwBD24LaqVr",
        "outputId": "23b87c6a-9bbe-46e0-ad22-4e32c21f42e5"
      },
      "source": [
        "### 2x3 shape: 2 rows for 2 matrices; 3 elems each for the sum of each matrix along the rows\n",
        "\n",
        "a.sum(axis=2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAigyPVUbkuW"
      },
      "source": [
        "9. Feed a tensor with 3 or more axes to the linalg.norm function and observe its output. What does this function compute for tensors of arbitrary shape?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSEuphS5axz6",
        "outputId": "394df38b-aede-4460-f629-afc073791ada"
      },
      "source": [
        "Y= torch.arange(24,dtype = torch.float32).reshape(2, 3, 4)\n",
        "print (Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.,  1.,  2.,  3.],\n",
            "         [ 4.,  5.,  6.,  7.],\n",
            "         [ 8.,  9., 10., 11.]],\n",
            "\n",
            "        [[12., 13., 14., 15.],\n",
            "         [16., 17., 18., 19.],\n",
            "         [20., 21., 22., 23.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0skacSdobx21",
        "outputId": "d4a40684-099b-41da-b355-6a2c9a552ec8"
      },
      "source": [
        "torch.linalg.norm(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(65.7571)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XCz-F4mdqWa"
      },
      "source": [
        "Analogous to  L2  norms of vectors, the Frobenius norm of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
        "\n",
        "$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw45HzFLb8gF",
        "outputId": "0525a681-ea5c-41c8-951a-7937401f6622"
      },
      "source": [
        "sum = 0\n",
        "for elem in Y[:]:\n",
        "    print (elem.shape)\n",
        "    for elem1 in elem:\n",
        "        for elem2 in elem1:\n",
        "            sum+= elem2.item()**2\n",
        "\n",
        "print(sum**0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([3, 4])\n",
            "65.75712889109438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDFEGSKue8Xb"
      },
      "source": [
        "This has computed the Frobenus norm for the tensor of shape 2x3x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABOtW9fghj3e"
      },
      "source": [
        "## Calculus\n",
        "\n",
        "### Derivatives and Differentiation\n",
        "\n",
        "We begin by addressing the calculation of derivatives, a crucial step in nearly all deep learning optimization algorithms. In deep learning, we typically choose loss functions that are differentiable with respect to our model’s parameters. Put simply, this means that for each parameter, we can determine how rapidly the loss would increase or decrease, were we to increase or decrease that parameter by an infinitesimally small amount.\n",
        "\n",
        "Suppose that we have a function  $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ whose input and output are both scalars. The derivative of  f  is defined as\n",
        "\n",
        "$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$\n",
        "\n",
        "if this limit exists. If  f'(a) exists,  f  is said to be differentiable at a . If  f  is differentiable at every number of an interval, then this function is differentiable on this interval. We can interpret the derivative  f′(x)  in (2.4.1) as the instantaneous rate of change of  f(x)  with respect to  x . The so-called instantaneous rate of change is based on the variation  h  in  x , which approaches  0 .\n",
        "\n",
        "To illustrate derivatives, let us experiment with an example. Define  \n",
        "$u = f(x) = 3x^2-4x$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wVMppOkchB3"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MtrHWBAiOzE",
        "outputId": "c99f1246-a870-437b-8673-6e40aa1affd5"
      },
      "source": [
        "def numerical_lim(f, x, h):\n",
        "    return (f(x+h) - f(x))/h\n",
        "\n",
        "h = 0.1\n",
        "for i in range(10):\n",
        "    print (f'h = {h}, numerical limit = {numerical_lim(f, 1, h)}')\n",
        "    h *= 0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h = 0.1, numerical limit = 2.3000000000000043\n",
            "h = 0.010000000000000002, numerical limit = 2.029999999999976\n",
            "h = 0.0010000000000000002, numerical limit = 2.0029999999993104\n",
            "h = 0.00010000000000000003, numerical limit = 2.000299999997956\n",
            "h = 1.0000000000000004e-05, numerical limit = 2.0000300000155837\n",
            "h = 1.0000000000000004e-06, numerical limit = 2.0000030001021676\n",
            "h = 1.0000000000000005e-07, numerical limit = 2.000000298707504\n",
            "h = 1.0000000000000005e-08, numerical limit = 1.999999987845057\n",
            "h = 1.0000000000000005e-09, numerical limit = 2.000000165480741\n",
            "h = 1.0000000000000006e-10, numerical limit = 2.000000165480741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct4Rs_DquGvz"
      },
      "source": [
        "Let us familiarize ourselves with a few equivalent notations for derivatives. Given  y=f(x) , where  x  and  y  are the independent variable and the dependent variable of the function  f , respectively. The following expressions are equivalent:\n",
        "\n",
        "$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$\n",
        "\n",
        "### Partial Derivatives\n",
        "\n",
        "So far we have dealt with the differentiation of functions of just one variable. In deep learning, functions often depend on many variables. Thus, we need to extend the ideas of differentiation to these multivariate functions.\n",
        "\n",
        "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with  n  variables. The partial derivative of  y  with respect to its  ith  parameter  xi  is:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$\n",
        "\n",
        "To calculate  $\\frac{\\partial y}{\\partial x_i}$ we can simply treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ s constants and calculate the derivative of  y  with respect to  xi . For notation of partial derivatives, the following are equivalent:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$\n",
        "\n",
        "### Gradients\n",
        "\n",
        "Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an  n -dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function  $f(\\mathbf{x})$  with respect to  $\\mathbf{x}$  is a vector of  n  partial derivatives:\n",
        "\n",
        "$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$\n",
        "\n",
        "Note that this also returns an n-dimensional vector\n",
        "\n",
        "where $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ is often replaced by $\\nabla f(\\mathbf{x})$  when there is no ambiguity.\n",
        "\n",
        "![](https://i.imgur.com/2qsZFkH.png)\n",
        "\n",
        "### Chain Rule\n",
        "\n",
        "However, such gradients can be hard to find. This is because multivariate functions in deep learning are often composite, so we may not apply any of the aforementioned rules to differentiate these functions. Fortunately, the chain rule enables us to differentiate composite functions.\n",
        "\n",
        "Let us first consider functions of a single variable. Suppose that functions y = f(u) and u = g(x) are both differentiable, then the chain rule states that\n",
        "\n",
        "$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$\n",
        "\n",
        "Now let us turn our attention to a more general scenario where functions have an arbitrary number of variables. Suppose that the differentiable function  y  has variables u1, u2... um,  where each differentiable function  ui  has variables  x1,x2,…,xn . Note that  y  is a function of  x1,x2,…,xn . Then the chain rule gives\n",
        "\n",
        "$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHjGw2Ug-OSv"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "As we have explained in Section 2.4, differentiation is a crucial step in nearly all deep learning optimization algorithms. While the calculations for taking these derivatives are straightforward, requiring only some basic calculus, for complex models, working out the updates by hand can be a pain (and often error-prone).\n",
        "\n",
        "Deep learning frameworks expedite this work by automatically calculating derivatives, i.e., automatic differentiation. In practice, based on our designed model the system builds a computational graph, tracking which data combined through which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Here, backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter.\n",
        "\n",
        "As a toy example, say that we are interested in differentiating the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to the column vector  x . To start, let us create the variable x and assign it an initial value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4a3ybnkXqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4eb97e7-323c-4bfe-d154-a00d57c080f3"
      },
      "source": [
        "x = torch.arange(4.0)\n",
        "print (x.shape)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQgekokX-pzD"
      },
      "source": [
        "The shape of x is 4x1. 2 x^T x will be a scalar. Also we know that a gradient of a scalar-valued function with respect to a vector  x  is itself vector-valued and has the same shape as  x .\n",
        "\n",
        "Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an  n -dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function  $f(\\mathbf{x})$  with respect to  $\\mathbf{x}$  is a vector of  n  partial derivatives:\n",
        "\n",
        "$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$\n",
        "\n",
        "Before we even calculate the gradient of  y  with respect to  x , we will need a place to store it. It is important that we do not allocate new memory every time we take a derivative with respect to a parameter because we will often update the same parameters thousands or millions of times and could quickly run out of memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkHEwOwW-fdJ"
      },
      "source": [
        "x.requires_grad_(True)  # Same as `x = torch.arange(4.0, requires_grad=True)` ### This means we want to calculate grad wrt x\n",
        "x.grad  # The default value is None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Ud_wmwKmGV"
      },
      "source": [
        "Now let us calculate  y . Note: y here is a scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQIxoU4xKi7r",
        "outputId": "91a68bee-772d-4e07-8762-44b744e28845"
      },
      "source": [
        "y = 2*torch.dot(x, x) ## 2 (0.0 + 1.1 + 2.2 + 3.3)\n",
        "print (x)\n",
        "print (y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor(28., grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3tPMwRMK8PI"
      },
      "source": [
        "Since x is a vector of length 4, an inner product of x and x is performed, yielding the scalar output that we assign to y. Next, we can automatically calculate the gradient of y with respect to each component of x by calling the function for backpropagation and printing the gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJkKpe9MKsye",
        "outputId": "65fa27da-a1ae-41f9-ba43-bdfec0799db2"
      },
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnGnn55eMvc4"
      },
      "source": [
        "The gradient of the function  $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$  with respect to $\\mathbf{x}$ will be $4\\mathbf{x}$.  Let us quickly verify that our desired gradient was calculated correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJNZX2f7LN1h",
        "outputId": "040d7b5b-0ca7-4003-9eef-66f2a3dc2092"
      },
      "source": [
        "4*x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0U86YuONPqY"
      },
      "source": [
        "Now let us calculate another function of x.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n-8wJgXNCgi",
        "outputId": "97313aad-f691-455a-ecdc-747d0c6f4da6"
      },
      "source": [
        "# PyTorch accumulates the gradient in default, we need to clear the previous values\n",
        "x.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DPF9EVwNU8e",
        "outputId": "11ecec4a-b7cc-4729-bc1e-4894ec7b63d8"
      },
      "source": [
        "y = x.sum()\n",
        "print (y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoZlHaCTNfot",
        "outputId": "c3bfc786-c667-49bf-df5f-21863fc8c671"
      },
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9IpuNv8Npjf"
      },
      "source": [
        "y = x1 + x2 + x3 + x4 where x1, x2, x3, x4 are each elem of vector x\n",
        "\n",
        "dy/dx = [dy/dx1, dy/dx2, dy/dx3, dy/dx4]: Note each of these are the partial derivatives\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGm3ORJWN-xE"
      },
      "source": [
        "### Backward for Non-Scalar Variables\n",
        "\n",
        "Technically, when y is not a scalar, the most natural interpretation of the differentiation of a vector y with respect to a vector x is a matrix. For higher-order and higher-dimensional y and x, the differentiation result could be a high-order tensor.\n",
        "\n",
        "However, while these more exotic objects do show up in advanced machine learning (including in deep learning), more often when we are calling backward on a vector, we are trying to calculate the derivatives of the loss functions for each constituent of a batch of training examples. Here, our intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives computed individually for each example in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onxWh7qnNpEj"
      },
      "source": [
        "x.grad.zero_()\n",
        "y = x * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-uW7OEtNhmX",
        "outputId": "f1fc11b5-e73e-491a-b258-1aeea2fff0a5"
      },
      "source": [
        "print (x)\n",
        "print (y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz71ntCvObV4",
        "outputId": "c9c44848-79d8-4121-c65a-2b0ed4bcad44"
      },
      "source": [
        "print (y.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(14., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5PjwOdLOrxm"
      },
      "source": [
        "y.sum().backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLVTDnvfOv2r",
        "outputId": "0543a078-faf2-4fb7-e0a1-c7379ade23d3"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N8DtSySQ3v3"
      },
      "source": [
        "Here again y is basically y = x1^2 + x2^2 + x3^2 + x4^2\n",
        "\n",
        "so dy/dx = [dy/dx1, dy/dx2, dy/dx3, dy/dx4]: Note each of these are the partial derivatives = [2x1, 2x2, 2x3, 2x4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG2azvLCRf6h"
      },
      "source": [
        "### Detaching Computation\n",
        "\n",
        "Sometimes, we wish to move some calculations outside of the recorded computational graph. For example, say that y was calculated as a function of x, and that subsequently z was calculated as a function of both y and x. Now, imagine that we wanted to calculate the gradient of z with respect to x, but wanted for some reason to treat y as a constant, and only take into account the role that x played after y was calculated.\n",
        "\n",
        "say y = x^2 and z = y*x = x^2 * x\n",
        "\n",
        "Here, we can detach y to return a new variable u that has the same value as y but discards any information about how y was computed in the computational graph. In other words, the gradient will not flow backwards through u to x. Thus, the following backpropagation function computes the partial derivative of z = u * x with respect to x while treating u as a constant, instead of the partial derivative of z = x * x * x with respect to x.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl7jraMqPAII",
        "outputId": "d9744072-7a57-4c25-8d67-12587246ed5f"
      },
      "source": [
        "x.grad.zero_()\n",
        "print (x)\n",
        "y = x * x\n",
        "print (y)\n",
        "\n",
        "u = y.detach()\n",
        "print (u)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
            "tensor([0., 1., 4., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDpyTgbYSL3-",
        "outputId": "9ad4f11a-8e48-4bef-b07b-7dc870db3036"
      },
      "source": [
        "z = u * x\n",
        "z.sum().backward()\n",
        "print (x.grad) ### should return u"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 4., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXPaxITIbYht",
        "outputId": "1d2bf988-30a3-495a-c730-117a1431831f"
      },
      "source": [
        "### without detaching\n",
        "x.grad.zero_()\n",
        "print (x)\n",
        "y = x * x\n",
        "print (y)\n",
        "z = y * x\n",
        "z.sum().backward()\n",
        "print (x.grad) ### 3 x^2 = [3.0^2, 3.1^2, 3.2^2, 3.3^2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True)\n",
            "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
            "tensor([ 0.,  3., 12., 27.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYCKmQOb706"
      },
      "source": [
        "### Computing the Gradient of Python Control Flow\n",
        "\n",
        "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. In the following snippet, note that the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input a.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pcEsDpfblLg"
      },
      "source": [
        "def f(a):\n",
        "    ## init b = 2a\n",
        "    b = a*2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        print ('here', b)\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBKVG4dod7CW",
        "outputId": "b9e79723-3624-47e6-bf26-2543d9414955"
      },
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "print (a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8755, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr4u454XeBK9",
        "outputId": "5bea7275-2998-47f0-ff56-209890e4b461"
      },
      "source": [
        "print (a.norm())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8755, grad_fn=<CopyBackwards>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt1T4F81eFIi",
        "outputId": "decaf169-6057-42ef-ceb4-1a5390c3c418"
      },
      "source": [
        "d = f(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here tensor(1793.0520, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXCtaI4relYB"
      },
      "source": [
        "d = f(a) = 2*a * 2^k = k(some const) * a\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjZNlvk2eTPY",
        "outputId": "ee874883-5084-4f02-aca4-bfed899e4161"
      },
      "source": [
        "d = f(a)\n",
        "d.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here tensor(1793.0520, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjGsDU2Ne18-",
        "outputId": "6c1f38d2-d4a4-4aba-9cc5-f68a85faf4a7"
      },
      "source": [
        "print (a.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2048.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZjNa501e4Jt",
        "outputId": "67a54a2d-a425-40cc-e060-a7ee4f960000"
      },
      "source": [
        "d/a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2048., grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xv_qTMpfE3s"
      },
      "source": [
        "We can now analyze the f function defined above. Note that it is piecewise linear in its input a. In other words, for any a there exists some constant scalar k such that f(a) = k * a, where the value of k depends on the input a\n",
        "\n",
        "f(a) = k*a => df/da = k  = f(a)/a = d/a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsFPT2tojPS1"
      },
      "source": [
        "## Probablility\n",
        "\n",
        "### Basic Probability Theory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH5rOQQ2gQmX"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.distributions import multinomial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGmXUiVmjfzc"
      },
      "source": [
        "Next, we will want to be able to cast the die. In statistics we call this process of drawing examples from probability distributions sampling. The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution. We will give a more formal definition of distribution later, but at a high level, think of it as just an assignment of probabilities to events.\n",
        "\n",
        "To draw a single sample, we simply pass in a vector of probabilities. The output is another vector of the same length: its value at index  i  is the number of times the sampling outcome corresponds to  i ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUioZE4PjYtv",
        "outputId": "b106f05f-5a35-403e-be40-4c80ecfdeac5"
      },
      "source": [
        "fair_probs = torch.ones([6]) / 6\n",
        "print (fair_probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73hBC2siAAOV",
        "outputId": "df33bfb3-1d98-4a52-ba52-affe38bcc9d9"
      },
      "source": [
        "### sample from a multinomial total_count times with prob of each event given by fair_probs\n",
        "multinomial.Multinomial(total_count=1, probs=fair_probs).sample()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pAdvh_EAh_M"
      },
      "source": [
        "If you run the sampler a bunch of times, you will find that you get out random values each time. As with estimating the fairness of a die, we often want to generate many samples from the same distribution. It would be unbearably slow to do this with a Python for loop, so the function we are using supports drawing multiple samples at once, returning an array of independent samples in any shape we might desire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI46a64TAS7y",
        "outputId": "f301d3a8-7cfe-4fcf-a44a-8b0625648275"
      },
      "source": [
        "multinomial.Multinomial(10, fair_probs).sample()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 1., 3., 2., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDMpY1iVAmZT",
        "outputId": "7cd57ba2-ef76-451f-bfb2-01febfb29729"
      },
      "source": [
        "# Store the results as 32-bit floats for division\n",
        "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
        "counts / 1000  # Relative frequency as the estimate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1860, 0.1550, 0.1870, 0.1630, 0.1560, 0.1530])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhOtn-IqAu9u"
      },
      "source": [
        "Because we generated the data from a fair die, we know that each outcome has true probability  16 , roughly  0.167 , so the above output estimates look good.\n",
        "\n",
        "We can also visualize how these probabilities converge over time towards the true probability. Let us conduct 500 groups of experiments where each group draws 10 samples.\n",
        "\n",
        "![](https://d2l.ai/_images/output_probability_245b7d_54_0.svg)\n",
        "\n",
        "Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments, the  6  solid curves converge towards the true probability.\n",
        "\n",
        "### Random variables\n",
        "\n",
        "![](https://i.imgur.com/ikJ3lk8.png)\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/lVO07r1.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGc3Pf9zKL4r"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8WhQqAQagM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu3dwP9zQbRf"
      },
      "source": [
        "## Linear Neural Networks\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "### Some assumptions\n",
        "\n",
        "1. First, we assume that the relationship between the independent variables $\\mathbf{x}$ and the dependent variable  y  is linear, i.e., that  y  can be expressed as a weighted sum of the elements in  $\\mathbf{x}$ , given some noise on the observations. Note here we represent $\\mathbf{x}$ as a vetor\n",
        "\n",
        "2. Second, we assume that any noise is well-behaved (following a Gaussian distribution).\n",
        "\n",
        "To motivate the approach, let us start with a running example. Suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To actually develop a model for predicting house prices, we would need to get our hands on a dataset consisting of sales for which we know the sale price, area, and age for each home. In the terminology of machine learning, the dataset is called a training dataset or training set, and each row (here the data corresponding to one sale) is called an example (or data point, data instance, sample). The thing we are trying to predict (price) is called a label (or target). The independent variables (age and area) upon which the predictions are based are called features (or covariates).\n",
        "\n",
        "Typically, we will use  n  to denote the number of examples in our dataset. We index the data examples by  i , denoting each input as\n",
        "\n",
        "\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$ and the corresponding label as $y^{(i)}$\n",
        "\n",
        "The linearity assumption just says that the target (price) can be expressed as a weighted sum of the features (area and age):\n",
        "\n",
        "$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b.$\n",
        "\n",
        "The weights determine the influence of each feature on our prediction and the bias just says what value the predicted price should take when all of the features take value 0. Even if we will never see any homes with zero area, or that are precisely zero years old, we still need the bias or else we will limit the expressivity of our model. Strictly speaking, the above equation is an affine transformation of input features, which is characterized by a linear transformation of features via weighted sum, combined with a translation via the added bias.\n",
        "\n",
        "Given a dataset, our goal is to choose the weights  $\\mathbf{w}$  and the bias  b  such that on average, the predictions made according to our model best fit the true prices observed in the data. Models whose output prediction is determined by the affine transformation of input features are linear models, where the affine transformation is specified by the chosen weights and bias.\n",
        "\n",
        "![](https://i.imgur.com/Oi0F47h.png)\n",
        "\n",
        "#### Some calculation walkthroughs\n",
        "\n",
        "![](https://i.imgur.com/EfgcBH2.jpg)\n",
        "\n",
        "![](https://i.imgur.com/sph1CMM.jpg)\n",
        "\n",
        "### Loss functions\n",
        "\n",
        "![](https://i.imgur.com/b4Nei7P.png)\n",
        "\n",
        "\n",
        "Note here $\\mathbf{x}^{(i)}$ is the ith feature vector "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgbJh_3HLjHr"
      },
      "source": [
        "### Analytic Solution\n",
        "\n",
        "Linear regression happens to be an unusually simple optimization problem. Unlike most other models that we will encounter in this book, linear regression can be solved analytically by applying a simple formula. To start, we can subsume the bias b into the parameter $\\mathbf{w}$ by appending a column to the design matrix consisting of all ones. Then our prediction problem is to minimize $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$\n",
        "\n",
        "#### Verification\n",
        "\n",
        "Let's verify that $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$ does indeed give the previous equation we had\n",
        "\n",
        "Matrix norms reference: https://learn.lboro.ac.uk/archive/olmp/olmp_resources/pages/workbooks_1_50_jan2008/Workbook30/30_4_mtrx_norms.pdf\n",
        "\n",
        "![](https://i.imgur.com/0A19uqO.jpeg)\n",
        "\n",
        "Taking the derivative of the loss with respect to  $\\mathbf{w}$  and setting it equal to zero yields the analytic (closed-form) solution:\n",
        "\n",
        "$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$\n",
        "\n",
        "https://i.imgur.com/g0HwF5V.jpeg\n",
        "\n",
        "![](https://i.imgur.com/g0HwF5V.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbpU_0D8TQ_M"
      },
      "source": [
        "\n",
        "### Minibatch Stochastic Gradient Descent\n",
        "\n",
        "Even in cases where we cannot solve the models analytically, it turns out that we can still train models effectively in practice. Moreover, for many tasks, those difficult-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.\n",
        "\n",
        "The key technique for optimizing nearly any deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called gradient descent.\n",
        "\n",
        "The most naive application of gradient descent consists of taking the derivative of the loss function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update. Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called minibatch stochastic gradient descent.\n",
        "\n",
        "![](https://i.imgur.com/UP5geII.png)\n",
        "\n",
        "#### Calculations watkthrough:\n",
        "\n",
        "![](https://i.imgur.com/tTjHJv9.jpg)\n",
        "\n",
        "![](https://i.imgur.com/XS86Z1v.jpeg)\n",
        "\n",
        "This shows the derivations of the weight vector and bias. pay careful attention to the dimensionalities shown\n",
        "\n",
        "After training for some predetermined number of iterations (or until some other stopping criteria are met), we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$. Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss because, although the algorithm converges slowly towards the minimizers it cannot achieve it exactly in a finite number of steps.\n",
        "\n",
        "Linear regression happens to be a learning problem where there is only one minimum over the entire domain. However, for more complicated models, like deep networks, the loss surfaces contain many minima. Fortunately, __for reasons that are not yet fully understood__, deep learning practitioners seldom struggle to find parameters that minimize the loss on training sets. The more formidable task is to find parameters that will achieve low loss on data that we have not seen before, a challenge called generalization. We return to these topics throughout the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPmWcRkNAsHt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}