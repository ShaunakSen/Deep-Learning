{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMYJhfFPV1V4",
        "colab_type": "text"
      },
      "source": [
        "## How to Use The Pre-Trained VGG Model to Classify Objects in Photographs\n",
        "\n",
        "[link](https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/)\n",
        "\n",
        "Convolutional neural networks are now capable of outperforming humans on some computer vision tasks, such as classifying images.\n",
        "\n",
        "That is, given a photograph of an object, answer the question as to which of 1,000 specific objects the photograph shows.\n",
        "\n",
        "A competition-winning model for this task is the VGG model by researchers at Oxford. What is important about this model, besides its capability of classifying objects in photographs, is that the model weights are freely available and can be loaded and used in your own models and applications.\n",
        "\n",
        "### ImageNet\n",
        "\n",
        "ImageNet is a research project to develop a large database of images with annotations, e.g. images and their descriptions.\n",
        "\n",
        "The images and their annotations have been the basis for an image classification challenge called the ImageNet Large Scale Visual Recognition Challenge or ILSVRC since 2010. The result is that research organizations battle it out on pre-defined datasets to see who has the best model for classifying the objects in images.\n",
        "\n",
        "For the classification task, images must be classified into one of 1,000 different categories.\n",
        "\n",
        "For the last few years very deep convolutional neural network models have been used to win these challenges and results on the tasks have exceeded human performance.\n",
        "\n",
        "### The Oxford VGG Models\n",
        "\n",
        "Researchers from the Oxford Visual Geometry Group, or VGG for short, participate in the ILSVRC challenge.\n",
        "\n",
        "In 2014, convolutional neural network models (CNN) developed by the VGG won the image classification tasks.\n",
        "\n",
        "VGG released two different CNN models, specifically a 16-layer model and a 19-layer model.\n",
        "\n",
        "The VGG models are not longer state-of-the-art by only a few percentage points. Nevertheless, they are very powerful models and useful both as image classifiers and as the basis for new models that use image inputs.\n",
        "\n",
        "### Load the VGG Model in Keras\n",
        "\n",
        "The VGG model can be loaded and used in the Keras deep learning library.\n",
        "\n",
        "Keras provides an Applications interface for loading and using pre-trained models.\n",
        "\n",
        "Using this interface, you can create a VGG model using the pre-trained weights provided by the Oxford group and use it as a starting point in your own model, or use it as a model directly for classifying images.\n",
        "\n",
        "In this tutorial, we will focus on the use case of classifying new images using the VGG model.\n",
        "\n",
        "Keras provides both the 16-layer and 19-layer version via the VGG16 and VGG19 classes. Let’s focus on the VGG16 model.\n",
        "\n",
        "The model can be created as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh9QjvX7Yl84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "model = VGG16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vid1krbHZAPs",
        "colab_type": "text"
      },
      "source": [
        "That’s it.\n",
        "\n",
        "The first time you run this example, Keras will download the weight files from the Internet and store them in the ~/.keras/models directory.\n",
        "\n",
        "Note that the weights are about 528 megabytes, so the download may take a few minutes depending on the speed of your Internet connection.\n",
        "\n",
        "The weights are only downloaded once. The next time you run the example, the weights are loaded locally and the model should be ready to use in seconds.\n",
        "\n",
        "We can use the standard Keras tools for inspecting the model structure.\n",
        "\n",
        "For example, you can print a summary of the network layers as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEpUeJMgZLtL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "outputId": "54167c90-b548-438c-9c50-5909634ba259"
      },
      "source": [
        "print (model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2A7AXr6ZAGE",
        "colab_type": "text"
      },
      "source": [
        "You can see that the model is huge.\n",
        "\n",
        "You can also see that, by default, the model expects images as input with the size 224 x 224 pixels with 3 channels (e.g. color).\n",
        "\n",
        "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Plot-of-Layers-in-the-VGG-Model.png)\n",
        "\n",
        "The VGG() class takes a few arguments that may only interest you if you are looking to use the model in your own project, e.g. for transfer learning.\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "- include_top (True): Whether or not to include the output layers for the model. You don’t need these if you are fitting the model on your own problem.\n",
        "- weights (‘imagenet‘): What weights to load. You can specify None to not load pre-trained weights if you are interested in training the model yourself from scratch.\n",
        "- input_tensor (None): A new input layer if you intend to fit the model on new data of a different size.\n",
        "- input_shape (None): The size of images that the model is expected to take if you change the input layer.\n",
        "- pooling (None): The type of pooling to use when you are training a new set of output layers.\n",
        "- classes (1000): The number of classes (e.g. size of output vector) for the model.\n",
        "\n",
        "\n",
        "Next, let’s look at using the loaded VGG model to classify ad hoc photographs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRPSs-Q7Y_43",
        "colab_type": "text"
      },
      "source": [
        "### Develop a Simple Photo Classifier\n",
        "\n",
        "Next, we can load the image as pixel data and prepare it to be presented to the network.\n",
        "\n",
        "Keras provides some tools to help with this step.\n",
        "\n",
        "First, we can use the load_img() function to load the image and resize it to the required size of 224×224 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erII1vG3ashH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf3741d5-e909-4b70-f7f7-05920ebff238"
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "# load img from file\n",
        "image = load_img(path='./4994221690_d070e8a355_z.jpg', target_size=(224, 224))\n",
        "\n",
        "# Next, we can convert the pixels to a NumPy array so that we can work with it in Keras.\n",
        "# We can use the img_to_array() function for this.\n",
        "\n",
        "image = img_to_array(img=image)\n",
        "\n",
        "print (image.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mja5h3WEbief",
        "colab_type": "text"
      },
      "source": [
        "The network expects one or more images as input; that means the input array will need to be 4-dimensional: `[samples, rows, columns, and channels]`.\n",
        "\n",
        "We only have one sample (one image). We can reshape the array by calling reshape() and adding the extra dimension.\n",
        "\n",
        "\n",
        "Next, the image pixels need to be prepared in the same way as the ImageNet training data was prepared. Specifically, from the paper:\n",
        "\n",
        "> The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\n",
        "\n",
        "Keras provides a function called preprocess_input() to prepare new input for the network.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9S-eDe1arTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36394e81-ec46-43d5-e204-c42f0fa8bc51"
      },
      "source": [
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "print (image.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrFXZKX1crT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "698b8a84-6ea6-49dd-c899-ad1bb20855c0"
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)\n",
        "\n",
        "print (image.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKlqjwl3dkLJ",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to make a prediction for our loaded and prepared image.\n",
        "\n",
        "We can call the predict() function on the model in order to get a prediction of the probability of the image belonging to each of the 1000 known object types.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh0y5wJQd0q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a796782a-13f5-4f83-8cfc-aaa48c1d2ae8"
      },
      "source": [
        "# predict the probability across all output classes\n",
        "yhat = model.predict(image)\n",
        "\n",
        "print (yhat.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkV3KaJAeGuL",
        "colab_type": "text"
      },
      "source": [
        "Keras provides a function to interpret the probabilities called decode_predictions().\n",
        "\n",
        "It can return a list of classes and their probabilities in case you would like to present the top 3 objects that may be in the photo.\n",
        "\n",
        "We will just report the first most likely object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIRm3noTeHqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1a1d2d0e-37aa-4510-da62-e62c6df7de68"
      },
      "source": [
        "from keras.applications.vgg16 import decode_predictions\n",
        "\n",
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat)\n",
        "\n",
        "print (len(label[0]))\n",
        "\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "\n",
        "label = label[0][0]\n",
        "\n",
        "print (label)\n",
        "\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "('n03063599', 'coffee_mug', 0.7336321)\n",
            "coffee_mug (73.36%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0fO308zAjpQ",
        "colab_type": "text"
      },
      "source": [
        "## How to Develop a Deep Learning Photo Caption Generator from Scratch\n",
        "\n",
        "[link](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)\n",
        "\n",
        "### Download and extract the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAf2TVTe0UCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zipurl = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip'\n",
        "    # Download the file from the URL\n",
        "zipresp = urlopen(zipurl)\n",
        "    # Create a new file on the hard drive\n",
        "tempzip = open(\"/tmp/Flickr8k_Dataset.zip\", \"wb\")\n",
        "    # Write the contents of the downloaded file into the new file\n",
        "tempzip.write(zipresp.read())\n",
        "    # Close the newly-created file\n",
        "tempzip.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9M6L9MO1GZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-open the newly-created file with ZipFile()\n",
        "zf = ZipFile(\"/tmp/Flickr8k_Dataset.zip\")\n",
        "    # Extract its contents into <extraction_path>\n",
        "    # note that extractall will automatically create the path\n",
        "zf.extractall(path = './Flickr8k_Dataset')\n",
        "    # close the ZipFile instance\n",
        "zf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiZlE1qV2bQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-open the newly-created file with ZipFile()\n",
        "zf = ZipFile(\"./Flickr8k_text.zip\")\n",
        "    # Extract its contents into <extraction_path>\n",
        "    # note that extractall will automatically create the path\n",
        "zf.extractall(path = './Flickr8k_text')\n",
        "    # close the ZipFile instance\n",
        "zf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5uxrkRyBN0F",
        "colab_type": "text"
      },
      "source": [
        "The dataset is present in the following locations:\n",
        "\n",
        "1. Flickr8k_Dataset\n",
        "2. Flickr8k_text\n",
        "\n",
        "The dataset has a pre-defined training dataset (6,000 images), development dataset (1,000 images), and test dataset (1,000 images).\n",
        "\n",
        "One measure that can be used to evaluate the skill of the model are BLEU scores.\n",
        "\n",
        "- BLEU-1: 0.401 to 0.578.\n",
        "- BLEU-2: 0.176 to 0.390.\n",
        "- BLEU-3: 0.099 to 0.260.\n",
        "- BLEU-4: 0.059 to 0.170.\n",
        "\n",
        "We describe the BLEU metric more later when we work on evaluating our model.\n",
        "\n",
        "Next, let’s look at how to load the images.\n",
        "\n",
        "### Prepare Photo Data\n",
        "\n",
        "We will use a pre-trained model to interpret the content of the photos.\n",
        "\n",
        "There are many models to choose from. In this case, we will use the Oxford Visual Geometry Group, or VGG, model that won the ImageNet competition in 2014. Learn more about the model here:\n",
        "\n",
        "[](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
        "\n",
        "Keras provides this pre-trained model directly. Note, the first time you use this model, Keras will download the model weights from the Internet, which are about 500 Megabytes. This may take a few minutes depending on your internet connection.\n",
        "\n",
        "\n",
        "We could use this model as part of a broader image caption model. The problem is, it is a large model and running each photo through the network every time we want to test a new language model configuration (downstream) is redundant.\n",
        "\n",
        "Instead, we can pre-compute the “photo features” using the pre-trained model and save them to file. We can then load these features later and feed them into our model as the interpretation of a given photo in the dataset. It is no different to running the photo through the full VGG model; it is just we will have done it once in advance.\n",
        "\n",
        "This is an optimization that will make training our models faster and consume less memory.\n",
        "\n",
        "We can load the VGG model in Keras using the VGG class. We will remove the last layer from the loaded model, as this is the model used to predict a classification for a photo. We are not interested in classifying images, but we are interested in the internal representation of the photo right before a classification is made. These are the “features” that the model has extracted from the photo.\n",
        "\n",
        "Keras also provides tools for reshaping the loaded photo into the preferred size for the model (e.g. 3 channel 224 x 224 pixel image).\n",
        "\n",
        "Below is a function named extract_features() that, given a directory name, will load each photo, prepare it for VGG, and collect the predicted features from the VGG model. The image features are a 1-dimensional 4,096 element vector.\n",
        "\n",
        "The function returns a dictionary of image identifier to image features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTptHUsU2f7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-08Ac3pODQK",
        "colab_type": "text"
      },
      "source": [
        "We can call this function to prepare the photo data for testing our models, then save the resulting dictionary to a file named ‘features.pkl‘."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuAJ1orcOVrW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20ac196c-1b31-4b0f-a2ec-5850d251ae68"
      },
      "source": [
        "model=VGG16()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0723 09:46:29.956114 139695895652224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0723 09:46:30.007027 139695895652224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0723 09:46:30.020456 139695895652224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0723 09:46:30.068988 139695895652224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 38s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0723 09:47:10.254584 139695895652224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0723 09:47:10.256208 139695895652224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FPwZKL4KJFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c1e471a-2bae-4bda-80b1-097513138499"
      },
      "source": [
        "print (model.layers[-1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.layers.core.Dense object at 0x7f0d203f2c50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUnTc8C5Km9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a48bf04a-3cc3-4224-b2ed-aad8039578c5"
      },
      "source": [
        "print (\"No of images:\", len(listdir(path='./Flickr8k_Dataset/Flicker8k_Dataset/')))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of images: 8091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vligLZzVOD8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(directory):\n",
        "  \"\"\"\n",
        "  extract features from each photo in the directory\n",
        "  \"\"\"\n",
        "  \n",
        "  # load the model\n",
        "  model = VGG16()\n",
        "  \n",
        "  # restructure the model\n",
        "  model.layers.pop()\n",
        "  model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "  \n",
        "  # summarize\n",
        "  print (model.summary())\n",
        "  \n",
        "  \n",
        "  # extract features from each photo\n",
        "  features = dict()\n",
        "  \n",
        "  # Return a list containing the names of the files in the directory.\n",
        "  for name in listdir(path=directory):\n",
        "    \n",
        "    # load an image from file\n",
        "    filename = directory + '/' + name\n",
        "    image = load_img(path=filename, target_size=(224,224))\n",
        "    \n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(img=image)\n",
        "    \n",
        "    # reshape data for the model\n",
        "    # The network expects one or more images as input; \n",
        "    # that means the input array will need to be 4-dimensional: \n",
        "    # [samples, rows, columns, and channels]\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    \n",
        "    # prepare the image for the VGG model\n",
        "    image = preprocess_input(image)\n",
        "    \n",
        "    # get features\n",
        "    feature = model.predict(x=image, verbose=0)\n",
        "    \n",
        "    # get image id\n",
        "    image_id = name.split('.')[0]\n",
        "    \n",
        "    # store feature in the dict\n",
        "    features[image_id] = feature\n",
        "    print('>%s' % name)\n",
        "    \n",
        "    \n",
        "  return features\n",
        "  \n",
        "directory = './Flickr8k_Dataset/Flicker8k_Dataset/'\n",
        "\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBzjhqfNobD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to file\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPe9vLNdP8bx",
        "colab_type": "text"
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7WNvEKDPZRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}