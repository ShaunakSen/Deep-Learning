{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reverse AD.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Reverse_AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hPWLouhy9UAJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reverse Mode Auto Diff\n",
        "\n",
        "```\n",
        "# Program A\n",
        "x = ?\n",
        "y = ?\n",
        "a = x * y\n",
        "b = sin(x)\n",
        "z = a + b\n",
        "```\n",
        "\n",
        "![](https://rufflewind.com/img/reverse-mode-automatic-differentiation-graph.png)\n",
        "\n",
        "```\n",
        "gz = ?\n",
        "gb = gz\n",
        "ga = gz\n",
        "gy = x * ga\n",
        "gx = y * ga + cos(x) * gb\n",
        "```\n",
        "\n",
        "Going back to the equations (R1), we see that if we substitute s=z, we would obtain the gradient in the last two equations. In the program, this is equivalent to setting gz = 1 since gz is just ∂s/∂z. We no longer need to run the program twice! This is reverse-mode automatic differentiation.\n",
        "\n",
        "### A simple implementation in Python\n",
        "\n",
        "\n",
        "One way is to parse the original program and then generate an adjoint program that calculates the derivatives. This is usually quite complicated to implement, and its difficulty varies significantly depending on the complexity of the host language. Nonetheless, this may be worthwhile if efficient is critical, as there are more opportunities to perform optimizations in this static approach.\n",
        "\n",
        "A simpler way is to do this dynamically: construct a full graph that represents our original expression as as the program runs. The goal is to get something akin to the dependency graph we drew earlier:\n",
        "\n",
        "The “roots” of the graph are the independent variables x and y, which could also be thought of as nullary operations. Constructing these nodes is a simple matter of creating an object on the heap:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eO0daNBu9SpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f0a192f-a5ee-4dfd-d1f7-c249eca63ead"
      },
      "cell_type": "code",
      "source": [
        "class Var:\n",
        "  def __init__(self, value):\n",
        "    self.value = value\n",
        "    self.children = []\n",
        "    \n",
        "    self.grad_value = None\n",
        "  \n",
        "  def grad(self):\n",
        "    if self.grad_value is None:\n",
        "      self.grad_value = sum(weight * var.grad() for weight, var in self.children)\n",
        "    return self.grad_value\n",
        "    \n",
        "    \n",
        "  \n",
        "  def __mul__(self, other):\n",
        "    z = Var(self.value * other.value)\n",
        "    self.children.append((other.value, z))\n",
        "    other.children.append((self.value, z))\n",
        "    return z\n",
        "  \n",
        "  def __add__(self, other):\n",
        "    z = Var(self.value + other.value)\n",
        "    self.children.append((1.0, z))\n",
        "    other.children.append((1.0, z))\n",
        "    return z\n",
        "  \n",
        "  def __truediv__(self,other):\n",
        "    z = Var(self.value/other.value)\n",
        "    self.children.append((1.0/other.value, z))\n",
        "    other.children.append((-1.0*self.value*other.value**-2, z))\n",
        "    return z\n",
        "  \n",
        "x = Var(0.5)\n",
        "y = Var(4.2)\n",
        "  \n",
        "a = x/y\n",
        "\n",
        "a.grad_value = 1.0\n",
        "\n",
        "print(\"∂a/∂x = {}\".format(y.grad()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "∂a/∂x = -0.028344671201814057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IOfNJ56Mea9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fd5bd4b-2ee8-46fb-8ebb-3ab0774ef445"
      },
      "cell_type": "code",
      "source": [
        "0.5*4.2**-2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.028344671201814057"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}