{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch 01 - Deep Learning With Python Chollet .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEjTWuQRAWBsoHSiXMtcHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Ch_01_Deep_Learning_With_Python_Chollet_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2W28EpY-ni",
        "colab_type": "text"
      },
      "source": [
        "> Notes based on the book [Deep Learning with Python by François Chollet](https://www.manning.com/books/deep-learning-with-python)\n",
        "\n",
        "## Ch 01 What is Deep Learning?\n",
        "\n",
        "A machine-learning model transforms its input data into meaningful outputs, a process that is “learned” from exposure to known examples of inputs and outputs. Therefore, the central problem in machine learning and deep learning is to meaningfully\n",
        "transform data: in other words, to learn useful representations of the input data at\n",
        "hand—representations that get us closer to the expected output. Before we go any\n",
        "further: what’s a representation? At its core, it’s a different way to look at data—to represent or encode data. For instance, a color image can be encoded in the RGB format\n",
        "(red-green-blue) or in the HSV format (hue-saturation-value): these are two different\n",
        "representations of the same data. Some tasks that may be difficult with one representation can become easy with another. For example, the task “select all red pixels in the\n",
        "image” is simpler in the RG format, whereas “make the image less saturated” is simpler\n",
        "in the HSV format. Machine-learning models are all about finding appropriate representations for their input data—transformations of the data that make it more amenable to the task at hand, such as a classification task.\n",
        "\n",
        "### Machine Learning\n",
        "\n",
        "Say we have a 2D plane of black and white points and we find a boundary to separate these points that works very well.\n",
        "\n",
        "In this case, we defined the coordinate change by hand. But if instead we tried systematically searching for different possible coordinate changes, and used as feedback the percentage of points being correctly classified, then we would be doing machine learning. Learning, in the context of machine learning, describes an automatic search process for better representations\n",
        "\n",
        "All machine-learning algorithms consist of automatically finding such transformations that turn data into more-useful representations for a given task. These operations can be coordinate changes, as you just saw, or linear projections (which may destroy information), translations, nonlinear operations (such as “select all points such that x > 0”), and so on. Machine-learning algorithms aren’t usually creative in finding these transformations; they’re merely searching through a predefined set of\n",
        "operations, called a hypothesis space.\n",
        " \n",
        "So that’s what machine learning is, technically: searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal. This simple idea allows for solving a remarkably broad range of intellectual tasks, from speech recognition to autonomous car driving.\n",
        "\n",
        "Now that you understand what we mean by learning, let’s take a look at what makes deep learning special. \n",
        "\n",
        "### The “deep” in deep learning\n",
        "\n",
        "Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly\n",
        "meaningful representations. The deep in deep learning isn’t a reference to any kind ofdeeper understanding achieved by the approach; rather, it stands for this idea of successive layers of representations. How many layers contribute to a model of the data iscalled the depth of the model. Other appropriate names for the field could have beenlayered representations learning and hierarchical representations learning. Modern deeplearning often involves tens or even hundreds of successive layers of representations—\n",
        "and they’re all learned automatically from exposure to training data. Meanwhile,\n",
        "other approaches to machine learning tend to focus on learning only one or two layers of representations of the data; hence, they’re sometimes called shallow learning\n",
        "\n",
        " In deep learning, these layered representations are (almost always) learned via\n",
        "models called neural networks, structured in literal layers stacked on top of each other.\n",
        "The term neural network is a reference to neurobiology, but although some of the central concepts in deep learning were developed in part by drawing inspiration from our understanding of the brain, deep-learning models are not models of the brain.\n",
        "There’s no evidence that the brain implements anything like the learning mechanisms used in modern deep-learning models. You may come across pop-science articles proclaiming that deep learning works like the brain or was modeled after the brain, but that isn’t the case. It would be confusing and counterproductive for newcomers to the field to think of deep learning as being in any way related to neurobiology; you don’t need that shroud of “just like our minds” mystique and mystery, and you may as well forget anything you may have read about hypothetical links between deep learning and biology. **For our purposes, deep learning is a mathematical framework for learning representations from data**\n",
        "\n",
        "<img src='./img/fig1.6.png'>\n",
        "\n",
        "As you can see in figure 1.6, the network transforms the digit image into representations that are increasingly different from the original image and increasingly informative about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task).\n",
        "\n",
        "So that’s what deep learning is, technically: a multistage way to learn data representations. It’s a simple idea—but, as it turns out, very simple mechanisms, sufficiently scaled, can end up looking like magic. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LvmN9eufbXp",
        "colab_type": "text"
      },
      "source": [
        "### Understanding how deep learning works, in three figures\n",
        "\n",
        "At this point, you know that machine learning is about mapping inputs (such as\n",
        "images) to targets (such as the label “cat”), which is done by observing many examples of input and targets.You also know that deep neural networks do this input-to-target mapping via a deep sequence of simple data transformations (layers) and that these data transformations are learned by exposure to examples. Now let’s look at how this learning happens, concretely.\n",
        "\n",
        " The specification of what a layer does to its input data is stored in the layer’s weights, which in essence are a bunch of numbers. In this context, learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets. But here’s the thing: a deep neural network can contain tens of millions of parameters. Finding the correct value for all of them may seem like a daunting task, especially given that modifying the value of one parameter will affect the behavior of all the others!\n",
        "\n",
        "\n",
        " To control the output of\n",
        "a neural network, you need to be able to measure how far this output is from what you expected. This is the job of the loss function of the network, also called the objective function. The loss function takes the predictions of the network and the true target (what you wanted the network to output) and computes a distance score, capturing\n",
        "how well the network has done on this specific example\n",
        "\n",
        "The fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example (see figure 1.9). This adjustment is the job of the optimizer, which implements what’s called the Backpropagation algorithm: the central algorithm in deep learning. The next chapter explains in more detail how backpropagation works\n",
        "\n",
        "<img src='./img/fig1.9.png'>\n",
        "\n",
        "\n",
        "Initially, the weights of the network are assigned random values, so the network\n",
        "merely implements a series of random transformations. Naturally, its output is far from what it should ideally be, and the loss score is accordingly very high. But with every example the network processes, the weights are adjusted a little in the correct direction, and the loss score decreases. This is the training loop, which, repeated a sufficient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function. A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network. Once again, it’s a simple mechanism that, once scaled, ends up looking like magic. \n",
        "\n",
        "### What makes deep learning different\n",
        "\n",
        "The primary reason deep learning took off so quickly is that it offered better performance on many problems. But that’s not the only reason. Deep learning also makes problem-solving much easier, because it completely automates what used to be the most crucial step in a machine-learning workflow: feature engineering.\n",
        "\n",
        "Previous machine-learning techniques—shallow learning—only involved transforming the input data into one or two successive representation spaces, usually via simple transformations such as high-dimensional non-linear projections (SVMs) or\n",
        "decision trees. But the refined representations required by complex problems generally can’t be attained by such techniques. As such, humans had to go to great lengths to make the initial input data more amenable to processing by these methods: they had to manually engineer good layers of representations for their data. This is called feature engineering. Deep learning, on the other hand, completely automates this step:\n",
        "with deep learning, you learn all features in one pass rather than having to engineer them yourself. This has greatly simplified machine-learning workflows, often replacing sophisticated multistage pipelines with a single, simple, end-to-end deep-learning model.\n",
        "\n",
        "You may ask, if the crux of the issue is to have multiple successive layers of representations, could shallow methods be applied repeatedly to emulate the effects of deep learning? In practice, there are fast-diminishing returns to successive applications of shallow-learning methods, *because the optimal first representation layer in a three layer model isn’t the optimal first layer in a one-layer or two-layer model.* What is transformative about deep learning is that it allows a model to learn all layers of representation **jointly, at the same time, rather than in succession (greedily, as it’s called). With joint\n",
        "feature learning, whenever the model adjusts one of its internal features, all other features that depend on it automatically adapt to the change, without requiring human intervention.** Everything is supervised by a single feedback signal: every change in the model serves the end goal. This is much more powerful than greedily stacking shallow models, because it allows for **complex, abstract representations to be learned by breaking them down into long series of intermediate spaces (layers); each space is only a simple transformation away from the previous one.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu6NMqFui_SH",
        "colab_type": "text"
      },
      "source": [
        "## Before we begin: the mathematical building blocks of neural networks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuQODsp1Y0ta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f56c6d30-25ec-442b-8d1c-8350d5c074f3"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print (train_images.shape, train_labels.shape, test_images.shape, test_labels.shape) # each img is 28x28 greyscale"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ku3zHOk9zP",
        "colab_type": "text"
      },
      "source": [
        "The workflow will be as follows: First, we’ll feed the neural network the training data, train_images and train_labels. The network will then learn to associate images and labels. Finally, we’ll ask the network to produce predictions for test_images, and we’ll verify whether these predictions match the labels from test_labels.\n",
        "\n",
        " Let’s build the network—again, remember that you aren’t expected to understand\n",
        "everything about this example yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6uNcxsnkTeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "5e04b603-22f7-439a-fb29-d916b05570f7"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(units=512, activation='relu', input_shape=(28*28, )))\n",
        "network.add(layers.Dense(units=10, activation='softmax'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN92TXE_l7vA",
        "colab_type": "text"
      },
      "source": [
        "The core building block of neural networks is the layer, a data-processing module that you can think of as a filter for data. Some data goes in, and it comes out in a more useful form. Specifically, layers extract representations out of the data fed into them—hopefully, representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation. A deep-learning model is like a sieve for data processing, made of a succession of increasingly refined data filters—the layers.\n",
        "\n",
        "Here, our network consists of a sequence of two Dense layers, which are densely\n",
        "connected (also called fully connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes\n",
        "\n",
        "To make the network ready for training, we need to pick three more things, as part of the compilation step:\n",
        "\n",
        "1. A loss function—How the network will be able to measure its performance on\n",
        "the training data, and thus how it will be able to steer itself in the right direction.\n",
        "\n",
        "2. An optimizer—The mechanism through which the network will update itself\n",
        "based on the data it sees and its loss function\n",
        "\n",
        "3. Metrics to monitor during training and testing—Here, we’ll only care about accuracy (the fraction of the images that were correctly classified).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUyu95CQl4Ks",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "09332131-60a3-449d-f276-436505ad780c"
      },
      "source": [
        "# the compilation step\n",
        "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlFM6hJvnqz3",
        "colab_type": "text"
      },
      "source": [
        "Before training, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the `[0, 1]` interval. Previously, our training images, for instance, were stored in an array of shape `(60000, 28, 28)` of type uint8 with values in the `[0, 255]` interval. We transform it into a float32 array of shape `(60000, 28 * 28)` with values between 0 and 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vosj5EOhnqAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype('float32')/255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOX3x1W5pJky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "3d56e750-109f-4771-967f-347d78ee96b3"
      },
      "source": [
        "# We also need to categorically encode the labels\n",
        "\n",
        "print (train_labels[:5])\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(y=train_labels)\n",
        "test_labels = to_categorical(y=test_labels)\n",
        "\n",
        "print (train_labels[:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5 0 4 1 9]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmpc2bMZrdWQ",
        "colab_type": "text"
      },
      "source": [
        "We’re now ready to train the network, which in Keras is done via a call to the network’s fit method—we fit the model to its training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV9dS_t9re5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "383a89c4-f63f-4eb3-dd34-d3e96d070e46"
      },
      "source": [
        "network.fit(x=train_images, y=train_labels, batch_size=128, epochs=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 11s 181us/step - loss: 0.2559 - acc: 0.9248\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.1038 - acc: 0.9686\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0690 - acc: 0.9790\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0499 - acc: 0.9849\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0371 - acc: 0.9891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61d26aafd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptFr1_B0rw1O",
        "colab_type": "text"
      },
      "source": [
        "Two quantities are displayed during training: the loss of the network over the training data, and the accuracy of the network over the training data.\n",
        "\n",
        " We quickly reach an accuracy of 0.989 (98.9%) on the training data. Now let’s\n",
        "check that the model performs well on the test set, too:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUJdMh-PrpHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d3aeebe-335e-4563-f33f-9464800f0ce5"
      },
      "source": [
        "test_loss, test_acc = network.evaluate(x=test_images, y=test_labels)\n",
        "\n",
        "print (test_loss, test_acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 37us/step\n",
            "0.07787587381124031 0.9771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vowBP0HHsM-T",
        "colab_type": "text"
      },
      "source": [
        "The test-set accuracy turns out to be 97.8%—that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of overfitting: the fact that machine-learning models tend to perform worse on new data than on their training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co_iuXe7sIBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}