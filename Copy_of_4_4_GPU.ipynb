{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 4_4_GPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Copy_of_4_4_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "4972f311d33e889babafe6f6e44edc5f",
          "grade": false,
          "grade_id": "cell-8115527bd0e08e63",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "UkUJ8c1B7Z8n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Using GPU acceleration with PyTorch"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "8ef6029eb23fe884594de09e1cd97769",
          "grade": false,
          "grade_id": "cell-2e8abb75fa5d4222",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "y2I1m-wN7Z8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "0192b3bc-2ccb-4d42-e971-aa542a0501c4"
      },
      "cell_type": "code",
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "try: \n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install torchbearer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchbearer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/62/79c45d98e22e87b44c9b354d1b050526de80ac8a4da777126b7c86c2bb3e/torchbearer-0.3.0.tar.gz (84kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from torchbearer) (0.2.2.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->torchbearer) (0.46)\n",
            "Building wheels for collected packages: torchbearer\n",
            "  Building wheel for torchbearer (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/cb/69/466aef9cee879fb8f645bd602e34d45e754fb3dee2cb1a877a\n",
            "Successfully built torchbearer\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "5ad3d98e7a666f0550c48db3d40c9fb6",
          "grade": false,
          "grade_id": "cell-56a116e085aef83c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "NkF3Xxqj7Z8v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Manual use of `.cuda()`\n",
        "\n",
        "Now the magic of PyTorch comes in. So far, we've only been using the CPU to do computation. When we want to scale to a bigger problem, that won't be feasible for very long.\n",
        "|\n",
        "PyTorch makes it really easy to use the GPU for accelerating computation. Consider the following code that computes the element-wise product of two large matrices:"
      ]
    },
    {
      "metadata": {
        "id": "csXvXeQF7Z8w",
        "colab_type": "code",
        "outputId": "0df80237-f393-45eb-9b12-f470e7485a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.randn(1000, 1000)\n",
        "t2 = torch.randn(1000, 1000)\n",
        "t3 = t1*t2\n",
        "print(t3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0801,  0.0131, -0.7778,  ..., -0.1991,  1.1845, -0.1576],\n",
            "        [-0.1461, -0.0925, -1.0823,  ..., -0.4159, -0.2597,  0.3540],\n",
            "        [-0.5988,  1.0804,  0.2496,  ...,  2.6953,  0.1726, -0.5365],\n",
            "        ...,\n",
            "        [-0.9053,  1.0892, -1.6346,  ..., -0.3574,  0.2655, -0.2569],\n",
            "        [ 0.0534,  0.9225,  0.2201,  ...,  1.9246,  0.1290,  0.8775],\n",
            "        [-1.0004, -1.6944,  0.2133,  ..., -0.0137,  0.5315, -0.2759]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "6af792ab02ecca981f5c12685789f471",
          "grade": false,
          "grade_id": "cell-6849616c01cf9f25",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "5n9FgbuQ7Z80",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By sending all the tensors that we are using to the GPU, all the operations on them will also run on the GPU without having to change anything else. If you're running a non-cuda enabled version of PyTorch the following will throw an error; if you have cuda available the following will create the input matrices, copy them to the GPU and perform the multiplication on the GPU itself:"
      ]
    },
    {
      "metadata": {
        "id": "c7_6zKcV7Z83",
        "colab_type": "code",
        "outputId": "77000108-b1b5-4d56-b19a-be1e8bc7c51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "t1 = torch.randn(1000, 1000).cuda()\n",
        "t2 = torch.randn(1000, 1000).cuda()\n",
        "t3 = t1*t2\n",
        "print(t3)\n",
        "\n",
        "print (t3.device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.6100, -0.0796, -0.0658,  ..., -0.1225, -0.6994,  0.0386],\n",
            "        [-0.1560,  0.0362, -0.5935,  ...,  1.0441,  0.4122, -0.1792],\n",
            "        [ 1.7499,  0.1702, -2.2297,  ...,  0.1659, -1.3387, -1.4362],\n",
            "        ...,\n",
            "        [-0.7714, -0.3160,  0.2048,  ...,  0.0502,  0.1226, -0.0235],\n",
            "        [ 0.3930,  0.7626,  0.2685,  ...,  0.2665, -0.2146,  2.4714],\n",
            "        [ 0.0910, -1.1168,  0.1299,  ...,  0.1488, -0.2555, -0.3139]],\n",
            "       device='cuda:0')\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c771877e9beb32f8a49585373534dad1",
          "grade": false,
          "grade_id": "cell-2bca345a3c01999c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "j8qJwSDQ7Z86",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you're running this workbook in colab, now enable GPU acceleration (`Runtime->Runtime Type` and add a `GPU` in the hardware accelerator pull-down). You'll then need to re-run all cells to this point.\n",
        "\n",
        "If you were able to run the above with hardware acceleration, the print-out of the result tensor would show that it was an instance of `cuda.FloatTensor` type on the the `(GPU 0)` GPU device. If your wanted to copy the tensor back to the CPU, you would use the `.cpu()` method."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "c064698f174abe4509b23c34a7912f44",
          "grade": false,
          "grade_id": "cell-e308141abb8d0e0c",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "htF2GSO_7Z88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Writing platform agnostic code\n",
        "\n",
        "Most of the time you'd like to write code that is device agnostic; that is it will run on a GPU if one is available, and otherwise it would fall back to the CPU. The recommended way to do this is as follows:"
      ]
    },
    {
      "metadata": {
        "id": "PHmdpUgr7Z89",
        "colab_type": "code",
        "outputId": "4e374feb-8f52-43cc-e6a8-b28fa2db0dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "t1 = torch.randn(1000, 1000).to(device)\n",
        "t2 = torch.randn(1000, 1000).to(device)\n",
        "t3 = t1*t2\n",
        "print(t3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6135,  0.3351,  0.7862,  ...,  3.3680, -0.0036,  1.2411],\n",
            "        [ 0.0132,  0.6727, -0.2120,  ...,  0.1114, -0.2625,  1.4165],\n",
            "        [ 0.2163,  0.0668, -1.3345,  ..., -0.0291, -0.0358,  0.8573],\n",
            "        ...,\n",
            "        [ 0.0475,  0.3305,  0.3017,  ...,  0.7037, -0.0046, -0.3717],\n",
            "        [ 0.7401, -0.7087,  0.0668,  ..., -0.2299,  0.0632, -0.1292],\n",
            "        [-0.1116,  1.2368,  0.0645,  ..., -2.9785, -0.5237, -0.1410]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "b31bddc27c2bdeb593ee2498dfbd7e10",
          "grade": false,
          "grade_id": "cell-24f03f8a35648313",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "ReC8Buqh7Z8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Accelerating neural net training\n",
        "\n",
        "If you wanted to accelerate the training of a neural net using raw PyTorch, you would have to copy both the model and the training data to the GPU. Unless you were using a really small dataset like MNIST, you would typically _stream_ the batches of training data to the GPU as you used them in the training loop:\n",
        "\n",
        "```python\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BaselineModel(784, 784, 10).to(device)\n",
        "\n",
        "loss_function = ...\n",
        "optimiser = ...\n",
        "\n",
        "for epoch in range(10):\n",
        "    for data in trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "```\n",
        "\n",
        "Using Torchbearer, this becomes much simpler - you just tell the `Trial` to run on the GPU and that's it!:\n",
        "\n",
        "```python\n",
        "model = BetterCNN()\n",
        "\n",
        "loss_function = ...\n",
        "optimiser = ...\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
        "trial.with_generators(trainloader)\n",
        "trial.run(epochs=10)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "064768c85834aa40d82396f9f3cccfac",
          "grade": false,
          "grade_id": "cell-cf0444554770e817",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "-plgK8Lb7Z9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multiple GPUs\n",
        "\n",
        "Using multiple GPUs is beyond the scope of the lab, but if you have multiple cuda devices, they can be referred to by index: `cuda:0`, `cuda:1`, `cuda:2`, etc. You have to be careful not to mix operations on different devices, and would need how to carefully orchestrate moving of data between the devices (which can really slow down your code to the point at which using the CPU would actually be faster)."
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "d698b1536be57d852780abaf08fcad68",
          "grade": false,
          "grade_id": "cell-f0f058c0af885275",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "J1tTwHhI7Z9C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "__Answer the following questions (enter the answer in the box below each one):__\n",
        "\n",
        "__1.__ What features of GPUs allow them to perform computations faster than a typically CPU?"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "b1dc585b11c0e499f09e409017b6cb06",
          "grade": true,
          "grade_id": "cell-76fcc457388a8223",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "Runm87x57Z9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Computations that can be easily done in parallal can be achieved much faster using a GPU\n",
        "\n",
        "The number of tasks that a larger task can be broken into depends on the number of cores contained on a particular piece of hardware. Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands.\n",
        "\n",
        "![](https://www.datascience.com/hs-fs/hubfs/gpu2.png?width=600&name=gpu2.png)\n",
        "\n",
        "Neural Networks are [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) - so it can be broken up into smaller independent tasks that can be computed independently by the GPU cores.\n",
        "\n",
        "Neural Network model training is composed of simple matrix math calculations, the speed of which can be greatly enhanced if the computations can be carried out in parallel.\n",
        "\n",
        "\n",
        "- [http://deeplizard.com/learn/video/6stDhEA0wFQ](http://deeplizard.com/learn/video/6stDhEA0wFQ)\n",
        "- [https://www.datascience.com/blog/cpu-gpu-machine-learning](https://www.datascience.com/blog/cpu-gpu-machine-learning)\n",
        "- [https://en.wikipedia.org/wiki/Embarrassingly_parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "checksum": "374801c6d757e48f93fe93618435c685",
          "grade": false,
          "grade_id": "cell-68f765cc2155e517",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "dKE9Kyfb7Z9E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__2.__ What is the biggest limiting factor for training large models with current generation GPUs?"
      ]
    },
    {
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "checksum": "964d60951aa496908969f2ca35be3104",
          "grade": true,
          "grade_id": "cell-a147457afc2c4c40",
          "locked": false,
          "points": 1,
          "schema_version": 1,
          "solution": true
        },
        "id": "GvrARPRF7Z9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, given the size of your model and the size of your batches, you can actually calculate how much GPU memory you need for training without actually running it. For example, training AlexNet with batch size of 128 requires 1.1GB of global memory, and that is just 5 convolutional layers plus 2 fully-connected layers. If we look at a bigger model, say VGG-16, using a batch size of 128 will require about 14GB of global memory. The current state-of-the-art NVIDIA Titan X has a memory capacity of 12GB . VGG-16 has only 16 convolutional layers and 3 fully-connected layers, and is much smaller than the resnet model which could contain about one hundred layers.\n",
        "\n",
        "Now, if you want to train a model larger than VGG-16, you might have several options to solve the memory limit problem.\n",
        "– reduce your batch size, which might hinder both your training speed and accuracy.\n",
        "– distribute your model among multiple GPU(s), which is a complicated process in itself.\n",
        "– reduce your model size, if you find yourself unwilling to do the aforementioned options , or you have already tried these options but they’re not good."
      ]
    }
  ]
}