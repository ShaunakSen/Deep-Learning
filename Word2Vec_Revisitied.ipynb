{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec Revisitied.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR1Zk5V0Og/w24r+gs1hPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Word2Vec_Revisitied.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ywNoDN2D4Zu",
        "colab_type": "text"
      },
      "source": [
        "## The Illustrated Word2vec\n",
        "\n",
        "> Notes on the excellent article by [Jay Alammar](http://jalammar.github.io/illustrated-word2vec/)\n",
        "\n",
        "---\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/word2vec.png)\n",
        "\n",
        "### Basic Intuition\n",
        "\n",
        "On a scale of 0 to 100, how introverted/extraverted are you (where 0 is the most introverted, and 100 is the most extraverted)? Have you ever taken a personality test like MBTI – or even better, the Big Five Personality Traits test? If you haven’t, these are tests that ask you a list of questions, then score you on a number of axes, introversion/extraversion being one of them.\n",
        "\n",
        "Imagine I’ve scored 38/100 as my introversion/extraversion score. we can plot that in this way:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/introversion-extraversion-100.png)\n",
        "\n",
        "Let’s switch the range to be from -1 to 1:\n",
        "\n",
        "How well do you feel you know a person knowing only this one piece of information about them? Not much. People are complex. So let’s add another dimension – the score of one other trait from the test.\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/two-traits-vector.png)\n",
        "\n",
        "We can represent the two dimensions as a point on the graph, or better yet, as a vector from the origin to that point. We have incredible tools to deal with vectors that will come in handy very shortly.\n",
        "\n",
        "\n",
        "I’ve hidden which traits we’re plotting just so you get used to not knowing what each dimension represents – but still getting a lot of value from the vector representation of a person’s personality.\n",
        "\n",
        "We can now say that this vector partially represents my personality. The usefulness of such representation comes when you want to compare two other people to me. Say I get hit by a bus and I need to be replaced by someone with a similar personality. In the following figure, which of the two people is more similar to me?\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/personality-two-persons.png)\n",
        "\n",
        "When dealing with vectors, a common way to calculate a similarity score is cosine_similarity:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/cosine-similarity.png)\n",
        "\n",
        "Person #1 is more similar to me in personality. Vectors pointing at the same direction (length plays a role as well) have a higher cosine similarity score.\n",
        "\n",
        "Yet again, two dimensions aren’t enough to capture enough information about how different people are. Decades of psychology research have led to five major traits (and plenty of sub-traits). So let’s use all five dimensions in our comparison:\n",
        "\n",
        "The problem with five dimensions is that we lose the ability to draw neat little arrows in two dimensions. This is a common challenge in machine learning where we often have to think in higher-dimensional space. The good thing is, though, that cosine_similarity still works. It works with any number of dimensions:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/embeddings-cosine-personality.png)\n",
        "\n",
        "At the end of this section, I want us to come out with two central ideas:\n",
        "\n",
        "1. We can represent people (and things) as vectors of numbers (which is great for machines!).\n",
        "2. We can easily calculate how similar vectors are to each other.\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/section-1-takeaway-vectors-cosine.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp5p6qTGJCHd",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings\n",
        "\n",
        "With this understanding, we can proceed to look at trained word-vector examples (also called word embeddings) and start looking at some of their interesting properties.\n",
        "\n",
        "This is a word embedding for the word “king” (GloVe vector trained on Wikipedia):\n",
        "\n",
        "```\n",
        "[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n",
        "```\n",
        "\n",
        "It’s a list of 50 numbers. We can’t tell much by looking at the values. But let’s visualize it a bit so we can compare it other word vectors. Let’s put all these numbers in one row:\n",
        "\n",
        "Let’s color code the cells based on their values (red if they’re close to 2, white if they’re close to 0, blue if they’re close to -2):\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/king-colored-embedding.png)\n",
        "\n",
        "We’ll proceed by ignoring the numbers and only looking at the colors to indicate the values of the cells. Let’s now contrast “King” against other words:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/king-man-woman-embedding.png)\n",
        "\n",
        "See how “Man” and “Woman” are much more similar to each other than either of them is to “king”? This tells you something. These vector representations capture quite a bit of the information/meaning/associations of these words.\n",
        "\n",
        "Here’s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/queen-woman-girl-embeddings.png)\n",
        "\n",
        "A few things to point out:\n",
        "\n",
        "1. There’s a straight red column through all of these different words. They’re similar along that dimension (and we don’t know what each dimensions codes for)\n",
        "\n",
        "2. You can see how “woman” and “girl” are similar to each other in a lot of places. The same with “man” and “boy”\n",
        "\n",
        "3. “boy” and “girl” also have places where they are similar to each other, but different from “woman” or “man”. Could these be coding for a vague conception of youth? possible.\n",
        "\n",
        "4. All but the last word are words representing people. I added an object (water) to show the differences between categories. You can, for example, see that blue column going all the way down and stopping before the embedding for “water”.\n",
        "\n",
        "5. There are clear places where “king” and “queen” are similar to each other and distinct from all the others. Could these be coding for a vague concept of royalty?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKKwfRFbOHEN",
        "colab_type": "text"
      },
      "source": [
        "#### Analogies\n",
        "\n",
        "The famous examples that show an incredible property of embeddings is the concept of analogies. We can add and subtract word embeddings and arrive at interesting results. The most famous example is the formula: “king” - “man” + “woman”:\n",
        "\n",
        "We can visualize this analogy as we did previously:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/king-analogy-viz.png)\n",
        "\n",
        "The resulting vector from \"king-man+woman\" doesn't exactly equal \"queen\", but \"queen\" is the closest word to it from the 400,000 word embeddings we have in this collection.\n",
        "\n",
        "Now that we’ve looked at trained word embeddings, let’s learn more about the training process. But before we get to word2vec, we need to look at a conceptual parent of word embeddings: the neural language model.\n",
        "\n",
        "### Language Modeling\n",
        "\n",
        "\n",
        "If one wanted to give an example of an NLP application, one of the best examples would be the next-word prediction feature of a smartphone keyboard. It’s a feature that billions of people use hundreds of times every day.\n",
        "\n",
        "Next-word prediction is a task that can be addressed by a language model. A language model can take a list of words (let’s say two words), and attempt to predict the word that follows them.\n",
        "\n",
        "In the screenshot above, we can think of the model as one that took in these two green words (thou shalt) and returned a list of suggestions (“not” being the one with the highest probability):\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/thou-shalt-_.png)\n",
        "\n",
        "We can think of the model as looking like this black box:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/language_model_blackbox.png)\n",
        "\n",
        "But in practice, the model doesn’t output only one word. It actually outputs a probability score for all the words it knows (the model’s “vocabulary”, which can range from a few thousand to over a million words). The keyboard application then has to find the words with the highest scores, and present those to the user.\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/language_model_blackbox_output_vector.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luj8hOpUtc6K",
        "colab_type": "text"
      },
      "source": [
        "After being trained, early neural language models (Bengio 2003) would calculate a prediction in three steps:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/neural-language-model-prediction.png)\n",
        "\n",
        "The first step is the most relevant for us as we discuss embeddings. One of the results of the training process was this matrix that contains an embedding for each word in our vocabulary. During prediction time, we just look up the embeddings of the input word, and use them to calculate the prediction:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/neural-language-model-embedding.png)\n",
        "\n",
        "### Language Model Training\n",
        "\n",
        "Language models have a huge advantage over most other machine learning models. That advantage is that we are able to train them on running text – which we have an abundance of. Think of all the books, articles, Wikipedia content, and other forms of text data we have lying around. Contrast this with a lot of other machine learning models which need hand-crafted features and specially-collected data.\n",
        "\n",
        "> “You shall know a word by the company it keeps” J.R. Firth\n",
        "\n",
        "Words get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that\n",
        "\n",
        "1. We get a lot of text data (say, all Wikipedia articles, for example). then\n",
        "2. We have a window (say, of three words) that we slide against all of that text.\n",
        "3. The sliding window generates training samples for our model\n",
        "\n",
        "As this window slides against the text, we (virtually) generate a dataset that we use to train a model. To look exactly at how that’s done, let’s see how the sliding window processes this phrase:\n",
        "\n",
        "> Thou shalt not make a machine in the likeness of a human mind\n",
        "\n",
        "When we start, the window is on the first three words of the sentence:\n",
        "\n",
        "We take the first two words to be features, and the third word to be a label:\n",
        "\n",
        "We then slide our window to the next position and create a second sample:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/lm-sliding-window-3.png)\n",
        "\n",
        "And pretty soon we have a larger dataset of which words tend to appear after different pairs of words:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/lm-sliding-window-4.png)\n",
        "\n",
        "#### Look both ways\n",
        "\n",
        "Knowing what you know from earlier in the post, fill in the blank:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/jay_was_hit_by_a_.png)\n",
        "\n",
        "The context I gave you here is five words before the blank word (and an earlier mention of “bus”). I’m sure most people would guess the word bus goes into the blank. But what if I gave you one more piece of information – a word after the blank, would that change your answer?\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/jay_was_hit_by_a_bus.png)\n",
        "\n",
        "This completely changes what should go in the blank. the word red is now the most likely to go into the blank. What we learn from this is the words both before and after a specific word carry informational value. It turns out that accounting for both directions (words to the left and to the right of the word we’re guessing) leads to better word embeddings. Let’s see how we can adjust the way we’re training the model to account for this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXueb2Aw_IBy",
        "colab_type": "text"
      },
      "source": [
        "### Skipgram\n",
        "\n",
        "Instead of only looking two words before the target word, we can also look at two words after it.\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/continuous-bag-of-words-example.png)\n",
        "\n",
        "If we do this, the dataset we’re virtually building and training the model against would look like this:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/continuous-bag-of-words-dataset.png)\n",
        "\n",
        "This is called a **Continuous Bag of Words** architecture and is described in one of the word2vec papers [pdf]. Another architecture that also tended to show great results does things a little differently.\n",
        "\n",
        "Instead of guessing a word based on its context (the words before and after it), this other architecture tries to guess neighboring words using the current word. We can think of the window it slides against the training text as looking like this:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/skipgram-sliding-window.png)\n",
        "\n",
        "The word in the green slot would be the input word, each pink box would be a possible output.\n",
        "\n",
        "The pink boxes are in different shades because this sliding window actually creates four separate samples in our training dataset:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/skipgram-sliding-window-samples.png)\n",
        "\n",
        "This method is called the **skipgram** architecture. We can visualize the sliding window as doing the following: First it covers the words `[though, shalt, not, make, a]` and the current word is `not`. This would add these four samples to our training dataset:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/skipgram-sliding-window-2.png)\n",
        "\n",
        "We then slide our window to the next position, now the crrent word is `make`\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/skipgram-sliding-window-4.png)\n",
        "\n",
        "A couple of positions later, we have a lot more examples:\n",
        "\n",
        "![](http://jalammar.github.io/images/word2vec/skipgram-sliding-window-5.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tQi5KsDCmqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}