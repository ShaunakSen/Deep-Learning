{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Programming - Deep Learning \n",
    "\n",
    "[Playlist link](https://www.youtube.com/watch?v=iTKbyFh-7GM&list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG&index=2)\n",
    "\n",
    "### PyTorch - Python deep learning neural network API\n",
    "\n",
    "\n",
    "**A tensor is an n-dimensional array.**\n",
    "\n",
    "With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system.\n",
    "\n",
    "![](./img/diag1.png)\n",
    "\n",
    "Let’s talk about the prospects for learning PyTorch. For beginners to deep learning and neural networks, the top reason for learning PyTorch is that it is a thin framework that stays out of the way.\n",
    "\n",
    "**PyTorch is thin and stays out of the way!**\n",
    "\n",
    "When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. The experience of programming in PyTorch is as close as it gets to the real thing.\n",
    "\n",
    "A common PyTorch characteristic that often pops up is that it’s great for research. The reason for this research suitability has do do with a technical design consideration. To optimize neural networks, we need to calculate derivatives, and to do this computationally, deep learning frameworks use what are called [computational graphs](http://colah.github.io/posts/2015-08-Backprop/).\n",
    "\n",
    "Computational graphs are used to graph the function operations that occur on tensors inside neural networks.\n",
    "\n",
    "\n",
    "These graphs are then used to compute the derivatives needed to optimize the neural network. PyTorch uses a computational graph that is called a dynamic computational graph. This means that the graph is generated on the fly as the operations are created.\n",
    "\n",
    "This is in contrast to static graphs that are fully determined before the actual operations occur.\n",
    "\n",
    "It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1,2,3]) # created on CPU by default\n",
    "\n",
    "# so any operation we do on this tensor will be carried out in the CPU\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move tensor t onto GPU: Returns a copy of this object in CUDA memory.\n",
    "\n",
    "# t = t.cuda()\n",
    "\n",
    "# t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing tensors for deep learning\n",
    "\n",
    "\n",
    "**A tensor is the primary data structure used by neural networks.**\n",
    "\n",
    "The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure.\n",
    "\n",
    "\n",
    "| Indexes reqd | Computer science | Mathematics\n",
    "--- | --- | ---\n",
    "|0 |\tnumber |\tscalar |\n",
    "|1 |\tarray |\tvector |\n",
    "|2 |\t2d-array |\tmatrix |\n",
    "\n",
    "For example, suppose we have this array:\n",
    "Now, suppose we want to access (refer to) the number 3 in this data structure. We can do it using a single index like so:\n",
    "\n",
    "\n",
    "```\n",
    "> a = [1,2,3,4]\n",
    "\n",
    "> a[2]\n",
    "3\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This logic works the same for a vector.\n",
    "\n",
    "As another example, suppose we have this 2d-array:\n",
    "Now, suppose we want to access (refer to) the number 3 in this data structure. In this case, we need two indexes to locate the specific element.\n",
    "\n",
    "```\n",
    "> dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]\n",
    "\n",
    "> dd[0][2]\n",
    "3 \n",
    "```\n",
    "\n",
    "#### Tensors are generalizations\n",
    "\n",
    "\n",
    "When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.\n",
    "\n",
    "Indexes required |\tComputer science |\tMathematics\n",
    "--- | --- | ---\n",
    "n |\tnd-array |\tnd-tensor\n",
    "\n",
    "**Tensors and nd-arrays are the same thing!**\n",
    "\n",
    "So tensors are multidimensional arrays or nd-arrays for short. The reason we say a tensor is a generalization is because we use the word tensor for all values of n like so:\n",
    "\n",
    "- A scalar is a 0 dimensional tensor\n",
    "- A vector is a 1 dimensional tensor\n",
    "- A matrix is a 2 dimensional tensor\n",
    "- A nd-array is an n dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank, Axes and Shape\n",
    "\n",
    "The rank, axes, and shape are three tensor attributes that will concern us most when starting out with tensors in deep learning. These concepts build on one another starting with rank, then axes, and building up to shape, so keep any eye out for this relationship between these three.\n",
    "\n",
    "#### Rank\n",
    "\n",
    "The rank of a tensor refers to the number of dimensions present within the tensor. Suppose we are told that we have a rank-2 tensor. This means all of the following:\n",
    "\n",
    "- We have a matrix\n",
    "- We have a 2d-array\n",
    "- We have a 2d-tensor\n",
    "\n",
    "\n",
    "**A tensor's rank tells us how many indexes are needed to refer to a specific element within the tensor.**\n",
    "\n",
    "#### Axes\n",
    "\n",
    "An axis of a tensor is a specific dimension of a tensor.\n",
    "\n",
    "\n",
    "If we say that a tensor is a rank 2 tensor, we mean that the tensor has 2 dimensions, or equivalently, the tensor has two axes.\n",
    "\n",
    "Elements are said to exist or run along an axis. This running is constrained by the length of each axis. Let's look at the length of an axis now.\n",
    "\n",
    "The length of each axis tells us how many indexes are available along each axis.\n",
    "\n",
    "Suppose we have a tensor called t, and we know that the first axis has a length of three while the second axis has a length of four.\n",
    "\n",
    "Since the first axis has a length of three, this means that we can index three positions along the first axis like so:\n",
    "\n",
    "```\n",
    "t[0]\n",
    "t[1]\n",
    "t[2]\n",
    "```\n",
    "\n",
    "All of these indexes are valid, but we can't move passed index 2.\n",
    "\n",
    "Since the second axis has a length of four, we can index four positions along the second axis. This is possible for each index of the first axis, so we have\n",
    "\n",
    "```\n",
    "t[0][0]\n",
    "t[1][0]\n",
    "t[2][0]\n",
    "\n",
    "t[0][1]\n",
    "t[1][1]\n",
    "t[2][1]\n",
    "\n",
    "t[0][2]\n",
    "t[1][2]\n",
    "t[2][2]\n",
    "\n",
    "t[0][3]\n",
    "t[1][3]\n",
    "t[2][3]\n",
    "```\n",
    "\n",
    "Let's look at some examples to make this solid. We'll consider the same tensor dd as before:\n",
    "\n",
    "```\n",
    "\n",
    "> dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]\n",
    "\n",
    "# Each element along the first axis, is an array:\n",
    "\n",
    "> dd[0]\n",
    "[1, 2, 3]\n",
    "\n",
    "> dd[1]\n",
    "[4, 5, 6]\n",
    "\n",
    "> dd[2]\n",
    "[7, 8, 9]\n",
    "\n",
    "# Each element along the second axis, is a number:\n",
    "\n",
    "> dd[0][0]\n",
    "1\n",
    "\n",
    "> dd[1][0]\n",
    "4\n",
    "\n",
    "> dd[2][0]\n",
    "7\n",
    "\n",
    "```\n",
    "\n",
    "Note that, with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays. This is what we see in this example, but this idea generalizes.\n",
    "\n",
    "The rank of a tensor tells us how many axes a tensor has, and the length of these axes leads us to the very important concept known as the shape of a tensor.\n",
    "\n",
    "#### Shape of a tensor\n",
    "\n",
    "\n",
    "The shape of a tensor gives us the length of each axis of the tensor.\n",
    "\n",
    "\n",
    "To work with this tensor's shape, we’ll create a torch.Tensor object like so:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]\n",
    "\n",
    "t = torch.Tensor(dd)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to see the tensor's shape is 3 x 3. Note that, in PyTorch, size and shape of a tensor are the same thing.\n",
    "\n",
    "The shape of 3 x 3 tells us that each axis of this rank two tensor has a length of 3 which means that we have three indexes available along each axis. Let's look now at why the shape of a tensor is so important.\n",
    "\n",
    "The shape of a tensor is important for a few reasons. The first reason is because the shape allows us to conceptually think about, or even visualize, a tensor. Higher rank tensors become more abstract, and the shape gives us something concrete to think about.\n",
    "\n",
    "The shape also encodes all of the relevant information about axes, rank, and therefore indexes.\n",
    "\n",
    "Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping.\n",
    "\n",
    "As our tensors flow through our networks, certain shapes are expected at different points inside the network, and as neural network programmers, it is our job to understand the incoming shape and have the ability to reshape as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN tensor input shape and feature maps\n",
    "\n",
    "\n",
    "The shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensor’s shape represents a specific axis, and the value at each index gives us the length of the corresponding axis.\n",
    "\n",
    "Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall.\n",
    "\n",
    "To break this down, we’ll work backwards, considering the axes from right to left. Remember, the last axis, which is where we’ll start, is where the actual numbers or data values are located.\n",
    "\n",
    "If we are running along the last axis and we stop to inspect an element there, we will be looking at a number. If we are running along any other axis, the elements are multidimensional arrays.\n",
    "\n",
    "For images, the raw data comes in the form of pixels that are represented by a number and are laid out using two dimensions, height and width.\n",
    "\n",
    "#### Image height and width\n",
    "\n",
    "\n",
    "The image height and width are represented on the last two axes. Possible values here are 28 x 28, as will be the case for our image data in the fashion-MNIST dataset we’ll be using in our CNN project, or the 224 x 224 image size that is used by VGG16 neural network, or any other image dimensions we can imagine.\n",
    "\n",
    "#### Image color channels\n",
    "\n",
    "The next axis represents the color channels. Typical values here are 3 for RGB images or 1 if we are working with grayscale images. This color channel interpretation only applies to the input tensor.\n",
    "\n",
    "As we will reveal in a moment, the interpretation of this axis changes after the tensor passes through a convolutional layer.\n",
    "\n",
    "Up to this point using the last three axes, we have represented a complete image as a tensor. We have the color channels and the height and width all laid out in tensor form using three axes.\n",
    "\n",
    "In terms of accessing data at this point, we need three indexes. We choose a color channel, a height, and a width to arrive at a specific pixel value.\n",
    "\n",
    "#### Image batches\n",
    "\n",
    "\n",
    "This brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch.\n",
    "\n",
    "This allows us to see that an entire batch of images is represented using a single rank-4 tensor.\n",
    "\n",
    "Suppose we have the following shape [3, 1, 28, 28] for a given tensor. Using the shape, we can determine that we have a batch of three images.\n",
    "\n",
    "**[Batch, Channels, Height, Width]**\n",
    "\n",
    "Each image has a single color channel, and the image height and width are 28 x 28 respectively.\n",
    "\n",
    "\n",
    "This gives us a single rank-4 tensor that will ultimately flow through our convolutional neural network.\n",
    "\n",
    "Given a tensor of images like this, we can navigate to a specific pixel in a specific color channel of a specific image in the batch using four indexes.\n",
    "\n",
    "#### Output channels and feature maps\n",
    "\n",
    "Let’s look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer.\n",
    "\n",
    "Suppose we have a tensor that contains data from a single 28 x 28 grayscale image. This gives us the following tensor shape: [1, 1, 28, 28].\n",
    "\n",
    "Now suppose this image is passed to our CNN and passes through the first convolutional layer. When this happens, the shape of our tensor and the underlying data will be changed by the convolution operation.\n",
    "\n",
    "The convolution changes the height and width dimensions as well as the number of channels. The number of output channels changes based on the number of filters being used in the convolutional layer.\n",
    "\n",
    "Suppose we have three convolutional filters, and lets just see what happens to the channel axis.\n",
    "\n",
    "Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output channels opposed to color channels.\n",
    "\n",
    "Each of the three filters convolves the original single input channel producing three output channels. The output channels are still comprised of pixels, but the pixels have been modified by the convolution operation. Depending on the size of the filter, the height and width dimensions of the output will change also, but we'll leave those details for a future post.\n",
    "\n",
    "With the output channels, we no longer have color channels, but modified channels that we call feature maps. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters.\n",
    "\n",
    "The word “feature” is used because the outputs represent particular features from the image, like edges for example, and these mappings emerge as the network learns during the training process and become more complex as we move deeper into the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing PyTorch Tensors\n",
    "\n",
    "PyTorch tensors are instances of the torch.Tensor Python class. We can create a torch.Tensor object using the class constructor like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor()\n",
    "print (type(t))\n",
    "print (t.dtype)\n",
    "print (t.device)\n",
    "print (t.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device, cpu in our case, specifies the device (CPU or GPU) where the tensor's data is allocated. This determines where tensor computations for the given tensor will be performed.\n",
    "\n",
    "PyTorch supports the use of multiple devices, and they are specified using an index like so:\n",
    "\n",
    "```\n",
    "> device = torch.device('cuda:0')\n",
    "> device\n",
    "device(type='cuda', index=0)\n",
    "```\n",
    "\n",
    "If we have a device like above, we can create a tensor on the device by passing the device to the tensor’s constructor. One thing to keep in mind about using multiple devices is that tensor operations between tensors must happen between tensors that exists on the same device.\n",
    "\n",
    "The layout, strided in our case, specifies how the tensor is stored in memory.\n",
    "\n",
    "- Tensors contain data of a uniform type (dtype).\n",
    "- Tensor computations between tensors depend on the dtype and the device\n",
    "\n",
    "\n",
    "#### Creating tensors using data\n",
    "\n",
    "These are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch:\n",
    "\n",
    "1. torch.Tensor(data)\n",
    "2. torch.tensor(data)\n",
    "3. torch.as_tensor(data)\n",
    "4. torch.from_numpy(data)\n",
    "\n",
    "Let’s look at each of these. They all accept some form of data and give us an instance of the torch.Tensor class. Sometimes when there are multiple ways to achieve the same result, things can get confusing, so let’s break this down.\n",
    "\n",
    "We’ll begin by just creating a tensor with each of the options and see what we get. We’ll start by creating some data.\n",
    "\n",
    "We can use a Python list, or sequence, but numpy.ndarrays are going to be the more common option, so we’ll go with a numpy.ndarray like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) tensor([1, 2, 3]) tensor([1, 2, 3]) tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "o1 = torch.Tensor(data)\n",
    "o2 = torch.tensor(data)\n",
    "o3 = torch.as_tensor(data)\n",
    "o4 = torch.from_numpy(data)\n",
    "\n",
    "print(o1, o2, o3, o4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the options (o1, o2, o3, o4) appear to have produced the same tensors except for the first one. The first option (o1) has dots after the number indicating that the numbers are floats, while the next three options have a type of int32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) tensor([1, 2, 3]) tensor([1000,    2,    3]) tensor([1000,    2,    3])\n"
     ]
    }
   ],
   "source": [
    "print(o1, o2, o3, o4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification of the numpy array modifies the tensors in case of `as_tensor` and `from_numpy` methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PyTorch Tensors - Best Options\n",
    "\n",
    "Uppercase/lowercase: torch.Tensor() vs torch.tensor()\n",
    "Notice how the first option torch.Tensor() has an uppercase T while the second option torch.tensor() has a lowercase t. What’s up with this difference?\n",
    "\n",
    "The first option with the uppercase T is the constructor of the torch.Tensor class, and the second option is what we call a factory function that constructs torch.Tensor objects and returns them to the caller.\n",
    "\n",
    "You can think of the torch.tensor() function as a factory that builds tensors given some parameter inputs. Factory functions are a software design pattern for creating objects. Okay. That’s the difference between the uppercase T and the lower case t, but which way is better between these two? The answer is that it’s fine to use either one. However, the factory function torch.tensor() has better documentation and more configuration options, so it gets the winning spot at the moment.\n",
    "\n",
    "Alright, before we knock the torch.Tensor() constructor off our list in terms of use, let’s go over the difference we observed in the printed tensor outputs. The difference is in the dtype of each tensor.\n",
    "\n",
    "The difference here arises in the fact that the torch.Tensor() constructor uses the default dtype when building the tensor. We can verify the default dtype using the torch.get_default_dtype() method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other calls choose a dtype based on the incoming data. This is called type inference. The dtype is inferred based on the incoming data. Note that the dtype can also be explicitly set for these calls by specifying the dtype as an argument:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1000.,    2.,    3.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1000.,    2.,    3.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With torch.Tensor(), we are unable to pass a dtype to the constructor. This is an example of the torch.Tensor() constructor lacking in configuration options. This is one of the reasons to go with the torch.tensor() factory function for creating our tensors.\n",
    "\n",
    "#### Sharing memory for performance: copy vs share\n",
    "\n",
    "torch.Tensor() and torch.tensor() copy their input data while torch.as_tensor() and torch.from_numpy() share their input data in memory with the original input object.\n",
    "\n",
    "This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the torch.Tensor and the numpy.ndarray.\n",
    "\n",
    "Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory.\n",
    "\n",
    "This establishes that torch.as_tensor() and torch.from_numpy() both share memory with their input data. However, which one should we use, and how are they different?\n",
    "\n",
    "The torch.from_numpy() function only accepts numpy.ndarrays, while the torch.as_tensor() function accepts a wide variety of Python array-like objects including other PyTorch tensors. For this reason, torch.as_tensor() is the winning choice in the memory sharing game.\n",
    "\n",
    "#### Best options for creating tensors in PyTorch\n",
    "\n",
    "Given all of these details, these two are the best options:\n",
    "\n",
    "- torch.tensor()\n",
    "- torch.as_tensor()\n",
    "\n",
    "The torch.tensor() call is the sort of go-to call, while torch.as_tensor() should be employed when tuning our code for performance.\n",
    "\n",
    "Some things to keep in mind (memory sharing works where it can):\n",
    "\n",
    "\n",
    "- Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used.\n",
    "- The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists.\n",
    "- The as_tensor() call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.\n",
    "- The as_tensor() performance improvement will be greater when there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping operations\n",
    "\n",
    "We have the following high-level categories of operations:\n",
    "\n",
    "- Reshaping operations\n",
    "- Element-wise operations\n",
    "- Reduction operations\n",
    "- Access operations\n",
    "\n",
    "#### Reshaping operations for tensors\n",
    "\n",
    "Reshaping operations are perhaps the most important type of tensor operations. This is because, like we mentioned in the post where we introduced tensors, the shape of a tensor gives us something concrete we can use to shape an intuition for our tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "Rank: 2\n",
      "Num elems: tensor(12)\n",
      "Num elems: 12\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print (t.size())\n",
    "print (t.shape)\n",
    "print (\"Rank:\", len(t.shape))\n",
    "print (\"Num elems:\", torch.tensor(t.shape).prod())\n",
    "print (\"Num elems:\", t.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of elements contained within a tensor is important for reshaping because the reshaping must account for the total number of elements present. Reshaping changes the tensor's shape but not the underlying data. Our tensor has 12 elements, so any reshaping must account for exactly 12 elements.\n",
    "\n",
    "Let’s look now at all the ways in which this tensor t can be reshaped without changing the rank:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [3.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12])\n",
    "t.reshape([2,6])\n",
    "t.reshape([4,3])\n",
    "t.reshape(12,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The underlying logic is the same for higher dimensional tenors even though we may not be able to use the intuition of rows and columns in higher dimensional spaces. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 3.],\n",
       "         [3., 3., 3.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([2,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we increase the rank to 3, and so we lose the rows and columns concept. However, the product of the shape's components (2,2,3) still has to be equal to the number of elements in the original tensor ( 12).\n",
    "\n",
    "Note that PyTorch has another function that you may see called view() that does the same thing as the reshape() function, but don't let these names through you off. No matter which deep learning framework we are using, these concepts will be the same.\n",
    "\n",
    "#### Changing shape by squeezing and unsqueezing\n",
    "\n",
    "The next way we can change the shape of our tensors is by squeezing and unsqueezing them.\n",
    "\n",
    "- Squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "- Unsqueezing a tensor adds a dimension with a length of one.\n",
    "\n",
    "These functions allow us to expand or shrink the rank (number of dimensions) of our tensor. Let’s see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
      "torch.Size([12])\n",
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "torch.Size([1, 12])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.]])\n",
      "torch.Size([12, 1])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape([1,12]))\n",
    "print(t.reshape([1,12]).shape)\n",
    "print(t.reshape([1,12]).squeeze())\n",
    "print(t.reshape([1,12]).squeeze().shape)\n",
    "\n",
    "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))\n",
    "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)\n",
    "print(t.reshape([1,12]).squeeze().unsqueeze(dim=1))\n",
    "print(t.reshape([1,12]).squeeze().unsqueeze(dim=1).shape)\n",
    "\n",
    "# t remains unchanged in all of these ops\n",
    "\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten a tensor\n",
    "\n",
    "A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements.\n",
    "\n",
    "Flattening a tensor means to remove all of the dimensions except for one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flatten() function takes in a tensor t as an argument.\n",
    "\n",
    "Since the argument t can be any tensor, we pass -1 as the second argument to the reshape() function. In PyTorch, the -1 tells the reshape() function to figure out what the value should be based on the number of elements contained within the tensor. Remember, the shape must equal the product of the shape's component values. This is how PyTorch can figure out what the value should be, given a 1 as the first argument.\n",
    "\n",
    "Since our tensor t has 12 elements, the reshape() function is able to figure out that a 12 is required for the length of the second axis.\n",
    "\n",
    "After squeezing, the first axis (axis-0) is removed, and we obtain our desired result, a 1d-array of length 12.\n",
    "\n",
    "Here's an example of this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones(4, 3)\n",
    "print (t)\n",
    "\n",
    "flatten(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a future post when we begin building a convolutional neural network, we will see the use of this flatten() function. We'll see that flatten operations are required when passing an output tensor from a convolutional layer to a linear layer.\n",
    "\n",
    "In these examples, we have flattened the entire tensor, however, it is possible to flatten only specific parts of a tensor. For example, suppose we have a tensor of shape [2,1,28,28] for a CNN. This means that we have a batch of 2 grayscale images with height and width dimensions of 28 x 28, respectively.\n",
    "\n",
    "Here, we can specifically flatten the two images. To get the following shape: [2,1,784]. We could also squeeze off the channel axes to get the following shape: [2,784].\n",
    "\n",
    "#### Concatenating tensors\n",
    "\n",
    "We combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape of the two input tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "t2 = torch.tensor([\n",
    "    [5,6],\n",
    "    [7,8]\n",
    "])\n",
    "\n",
    "# We can combine t1 and t2 row-wise (axis-0) in the following way:\n",
    "\n",
    "print (torch.cat((t1,t2), dim=0))\n",
    "\n",
    "print (torch.cat((t1,t2), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten operation for a batch of image inputs to a CNN\n",
    "\n",
    "A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input.\n",
    "\n",
    "In past posts, we learned about a tensor’s shape and then about reshaping operations. A flatten operation is a specific type of reshaping operation where by all of the axes are smooshed or squashed together.\n",
    "\n",
    "To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. Let’s look now at a hand written image of an eight from the MNIST dataset. This image has 2 distinct dimensions, height and width.\n",
    "\n",
    "![](http://deeplizard.com/images/CNN%20Flatten%20Operation%20Visualized.png)\n",
    "\n",
    "The height and width are 18 x 18 respectively. These dimensions tell us that this is a cropped image because the MNIST dataset contains 28 x 28 images. Let’s see now how these two axes of height and width are flattened out into a single axis of length 324.\n",
    "\n",
    "The image above shows our flattened output with a single axis of length 324. The white on the edges corresponds to the white at the top and bottom of the image. The flatten operation can be thought of as taking rows of pixels from the img one by one nad stacking them up horizontally. \n",
    "\n",
    "In this example, we are flattening the entire tensor image, but what if we want to only flatten specific axes within the tensor? This is typically required when working with CNNs.\n",
    "\n",
    "Let’s see how we can flatten out specific axes of a tensor in code with PyTorch.\n",
    "\n",
    "#### Flattening specific axes of a tensor\n",
    "\n",
    "\n",
    "In the post on CNN input tensor shape, we learned how tensor inputs to a convolutional neural network typically have 4 axes, one for batch size, one for color channels, and one each for height and width.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a tensor representation for a batch of images\n",
    "\n",
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these has a shape of 4 x 4, so we have three rank-2 tensors. For our purposes here, we’ll consider these to be three 4 x 4 images that well use to create a batch that can be passed to a CNN.\n",
    "\n",
    "Remember, batches are represented using a single tensor, so we’ll need to combine these three tensors into a single larger tensor that has three axes instead of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack((t1,t2,t3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we used the stack() function to concatenate our sequence of three tensors along a new axis. Since we have three tensors along a new axis, we know the length of this axis should be 3, and indeed, we can see in the shape that we have 3 tensors that have height and width of 4.\n",
    "\n",
    "The axis with a length of 3 represents the batch size while the axes of length for represent the height and width respectively.\n",
    "\n",
    "At this point, we have a rank-3 tensor that contains a batch of three 4 x 4 images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images.\n",
    "\n",
    "A CNN will expect to see an explicit color channel axis, so let’s add one by reshaping this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 3, 3, 3],\n",
      "          [3, 3, 3, 3],\n",
      "          [3, 3, 3, 3],\n",
      "          [3, 3, 3, 3]]]])\n",
      "torch.Size([3, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "t = t.reshape(3,1,4,4)\n",
    "print (t)\n",
    "\n",
    "print (t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have specified an axis of length 1 right after the batch size axis. Then, we follow with the height and width axes length 4. Also, notice how the additional axis of length 1 doesn’t change the number of elements in the tensor. This is because the product of the components values doesn't change when we multiply by one.\n",
    "\n",
    "The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]]])\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "tensor([1, 1, 1, 1])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# first image\n",
    "print (t[0])\n",
    "\n",
    "# first (and only) color channel of first image\n",
    "\n",
    "print (t[0][0])\n",
    "\n",
    "# first row of pixels in first col channel of first image\n",
    "\n",
    "print (t[0][0][0])\n",
    "\n",
    "# first pixel value in the first row of the first color channel of the first image.\n",
    "\n",
    "print (t[0][0][0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. Let’s see how to flatten the images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, so we don’t want to flatten the whole thing. We only want to flatten the image tensors within the batch tensor.\n",
    "\n",
    "Let’s flatten the whole thing first just to see what it will look like. Plus I want to do a shout out to everyone who provided alternative implementations of the flatten() function we created in the last post. Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print (t.reshape(1,-1)[0])\n",
    "\n",
    "print (t.reshape(-1))\n",
    "\n",
    "print (t.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the bottom, you’ll notice another way that comes built-in as method for tensor objects called, you guessed it, flatten(). This method produces the very same output as the other alternatives.\n",
    "\n",
    "What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the images together into a single axis. Remember the ones represent the pixels from the first image, the twos the second image, and the threes from the third.\n",
    "\n",
    "This flattened batch won’t work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess.\n",
    "\n",
    "The solution here, is to flatten each image while still maintaining the batch axis. This means we want to flatten only part of the tensor. We want to flatten the, color channel axis with the height and width axes.\n",
    "\n",
    "```\n",
    "These axes need to be flattened: (C,H,W)\n",
    "```\n",
    "\n",
    "This can be done with PyTorch’s built-in flatten() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "torch.Size([3, 16])\n"
     ]
    }
   ],
   "source": [
    "print (t.flatten(start_dim=1))\n",
    "\n",
    "print (t.flatten(start_dim=1).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the call how we specified the start_dim parameter. This tells the flatten() method which axis it should start the flatten operation. The one here is an index, so it’s the second axis which is the color channel axis. We skip over the batch axis so to speak, leaving it intact.\n",
    "\n",
    "Checking the shape, we can see that we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting and Element-wise Operations\n",
    "\n",
    "An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors.\n",
    "\n",
    "Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element.\n",
    "\n",
    "Suppose we have the following two tensors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [9,8],\n",
    "    [7,6]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Example of the first axis\n",
    "print(t1[0])\n",
    "\n",
    "\n",
    "# Example of the second axis\n",
    "print(t1[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that two elements are said to be corresponding if the two elements occupy the same position within the tensor, and the position is determined by the indexes used to locate each element. Two tensors must have the same shape in order to perform element-wise operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition is an element-wise operation\n",
    "\n",
    "t1+t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allow us to see that addition between tensors is an element-wise operation. Each pair of elements in corresponding locations are added together to produce a new tensor of the same shape.\n",
    "\n",
    "So, addition is an element-wise operation, and in fact, all the arithmetic operations, add, subtract, multiply, and divide are element-wise operations.\n",
    "\n",
    "#### Arithmetic operations are element-wise operations\n",
    "\n",
    "An operation we commonly see with tensors are arithmetic operations using scalar values. There are two ways we can do this:\n",
    "\n",
    "(1) Using these symbolic operations:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "> print(t + 2)\n",
    "tensor([[3., 4.],\n",
    "        [5., 6.]])\n",
    "\n",
    "> print(t - 2)\n",
    "tensor([[-1.,  0.],\n",
    "        [ 1.,  2.]])\n",
    "\n",
    "> print(t * 2)\n",
    "tensor([[2., 4.],\n",
    "        [6., 8.]])\n",
    "\n",
    "> print(t / 2)\n",
    "tensor([[0.5000, 1.0000],\n",
    "        [1.5000, 2.0000]])\n",
    "        \n",
    "```\n",
    "or equivalently, (2) these built-in tensor object methods:\n",
    "\n",
    "```\n",
    "> print(t1.add(2))\n",
    "tensor([[3., 4.],\n",
    "        [5., 6.]])\n",
    "\n",
    "> print(t1.sub(2))\n",
    "tensor([[-1.,  0.],\n",
    "        [ 1.,  2.]])\n",
    "\n",
    "> print(t1.mul(2))\n",
    "tensor([[2., 4.],\n",
    "        [6., 8.]])\n",
    "\n",
    "> print(t1.div(2))\n",
    "tensor([[0.5000, 1.0000],\n",
    "        [1.5000, 2.0000]])\n",
    "```\n",
    "\n",
    "Both of these options work the same. We can see that in both cases, the scalar value, 2, is applied to each element with the corresponding arithmetic operation.\n",
    "\n",
    "Something seems to be wrong here. These examples are breaking the rule we established that said element-wise operations operate on tensors of the same shape.\n",
    "\n",
    "Scalar values are Rank-0 tensors, which means they have no shape, and our tensor t1 is a rank-2 tensor of shape 2 x 2.\n",
    "\n",
    "So how does this fit in? Let’s break it down.\n",
    "\n",
    "The first solution that may come to mind is that the operation is simply using the single scalar value and operating on each element within the tensor.\n",
    "\n",
    "This logic kind of works. However, it’s a bit misleading, and it breaks down in more general situations where we’re note using a scalar.\n",
    "\n",
    "To think about these operations differently, we need to introduce the concept of tensor broadcasting or broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting tensors\n",
    "\n",
    "Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors.\n",
    "\n",
    "\n",
    "Let's think about the t1 + 2 operation. Here, the scaler valued tensor is being broadcasted to the shape of t1, and then, the element-wise operation is carried out.\n",
    "\n",
    "We can see what the broadcasted scalar value looks like using the broadcast_to() Numpy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(2, shape=t1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the scalar value is transformed into a rank-2 tensor just like t1, and just like that, the shapes match and the element-wise rule of having the same shape is back in play. This is all under the hood of course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print (t1 + 2)\n",
    "\n",
    "# is essentially this:\n",
    "\n",
    "print (t1 + torch.tensor(np.broadcast_to(2, t1.shape), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a trickier example to hit this point home. Suppose we have the following tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1],\n",
    "    [1,1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 4.]\n",
      " [2. 4.]]\n",
      "tensor([[3., 5.],\n",
      "        [3., 5.]])\n"
     ]
    }
   ],
   "source": [
    "print (np.broadcast_to(t2.numpy(), t1.shape))\n",
    "\n",
    "print (t1 + t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After broadcasting, the addition operation between these two tensors is a regular element-wise operation between tensors of the same shape.\n",
    "\n",
    "Understanding element-wise operations and the same shape requirement provide a basis for the concept of broadcasting and why it is used.\n",
    "\n",
    "When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing our data, and especially during normalization routines.\n",
    "\n",
    "#### Comparison operations are element-wise\n",
    "\n",
    "Comparison operations are also element-wise. For a given comparison operations between tensors, a new tensor of the same shape is returned with each element containing either a 0 or a 1.\n",
    "\n",
    "- 0 if the comparison between corresponding elements is False.\n",
    "- 1 if the comparison between corresponding elements is True.\n",
    "\n",
    "```\n",
    "\n",
    "> t = torch.tensor([\n",
    "    [0,5,0],\n",
    "    [6,0,7],\n",
    "    [0,8,0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "> t.eq(0)\n",
    "tensor([[1, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 1]], dtype=torch.uint8)\n",
    "\n",
    "\n",
    "> t.ge(0)\n",
    "tensor([[1, 1, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1]], dtype=torch.uint8)\n",
    "\n",
    "\n",
    "> t.gt(0)\n",
    "tensor([[0, 1, 0],\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 0]], dtype=torch.uint8)\n",
    "\n",
    "\n",
    "> t.lt(0)\n",
    "tensor([[0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 0]], dtype=torch.uint8)\n",
    "\n",
    "> t.le(7)\n",
    "tensor([[1, 1, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 0, 1]], dtype=torch.uint8)\n",
    "        \n",
    "```\n",
    "Thinking about these operations from a broadcasting perspective, we can see that the last one, t.le(7), is really this:\n",
    "\n",
    "```\n",
    "> t <= torch.tensor(\n",
    "    np.broadcast_to(7, t.shape)\n",
    "    ,dtype=torch.float32\n",
    ")\n",
    "\n",
    "tensor([[1, 1, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 0, 1]], dtype=torch.uint8)\n",
    "        \n",
    "```\n",
    "\n",
    "#### Element-wise operations using functions\n",
    "\n",
    "With element-wise operations that are functions, it’s fine to assume that the function is applied to each element of the tensor.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "```\n",
    "> t.abs() \n",
    "tensor([[0., 5., 0.],\n",
    "        [6., 0., 7.],\n",
    "        [0., 8., 0.]])\n",
    "\n",
    "\n",
    "> t.sqrt()\n",
    "tensor([[0.0000, 2.2361, 0.0000],\n",
    "        [2.4495, 0.0000, 2.6458],\n",
    "        [0.0000, 2.8284, 0.0000]])\n",
    "\n",
    "> t.neg()\n",
    "tensor([[-0., -5., -0.],\n",
    "        [-6., -0., -7.],\n",
    "        [-0., -8., -0.]])\n",
    "\n",
    "> t.neg().abs()\n",
    "tensor([[0., 5., 0.],\n",
    "        [6., 0., 7.],\n",
    "        [0., 8., 0.]])\n",
    "        \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 4],\n",
       "       [5, 5, 5],\n",
       "       [6, 6, 6]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to([[4],[5],[6]], shape=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Detailed broadcasting post](http://deeplizard.com/learn/video/6_33ulFDuCg)\n",
    "\n",
    "#### Broadcasting Example 1: Same shapes\n",
    "\n",
    "For example, it might be relatively easy to look at these two rank-2 tensors and figure out what the sum of them would be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1,2,3]])\n",
    "t1.shape\n",
    "t2 = torch.tensor([[4,5,6]])\n",
    "print (t1+t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since these two tensors have the same shape, (1, 3), no broadcasting is happening here. Remember, broadcasting comes into play when we have tensors with different shapes.\n",
    "\n",
    "#### Example 2: Same rank, different shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([3, 1])\n",
      "tensor([[5, 6, 7],\n",
      "        [6, 7, 8],\n",
      "        [7, 8, 9]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1,2,3]])\n",
    "t2 = torch.tensor([[4],\n",
    "                   [5],\n",
    "                   [6]])\n",
    "print (t1.shape, t2.shape)\n",
    "\n",
    "print (t1+t2)\n",
    "\n",
    "print ((t1+t2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two tensors with different shapes. The goal of broadcasting is to make the tensors have the same shape so we can perform element-wise operations on them.\n",
    "\n",
    "First, we have to see if the operation we’re trying to do is even possible between the given tensors. Based on the tensors’ original shapes, there may not be a way to reshape them to force them to be compatible, and if we can’t do that, then we can’t use broadcasting.\n",
    "\n",
    "#### Step 1: Determine if tensors are compatible\n",
    "\n",
    "The rule to see if broadcasting can be used is this.\n",
    "\n",
    "We compare the shapes of the two tensors, starting at their last dimensions and working backwards. Our goal is to determine whether each dimension between the two tensors’ shapes is compatible.\n",
    "\n",
    "The dimensions are compatible when either:\n",
    "\n",
    "- They’re equal to each other.\n",
    "- One of them is 1.\n",
    "\n",
    "In our example, we have shapes (3, 1) and (1, 3). So we first compare the last dimensions.\n",
    "\n",
    "Comparing the last dimensions of the two shapes, we have a 1 and a 3. Are these compatible? Well, let’s check the rule.\n",
    "\n",
    "Are they equal to each other? No, 1 doesn’t equal 3.\n",
    "\n",
    "Is one of them 1? Yes.\n",
    "\n",
    "Great, the last dimensions are compatible. Working our way to the front, for the next dimension, we have a 3 and a 1. Similar story, just switched order, right? So, are these compatible? Yes, again, because one of them is 1.\n",
    "\n",
    "Ok, that’s the first step. We’ve confirmed each dimension between the two shapes is compatible.\n",
    "\n",
    "If, however, while comparing the dimensions, we confirmed that at least one dimension wasn’t compatible, then we would cease our efforts there because the arithmetic operation would not be possible between the two.\n",
    "\n",
    "Now, since we’ve confirmed that our two tensors are compatible, we can sum them and use broadcasting to do it.\n",
    "\n",
    "#### Step 2: Determine the shape of the resulting tensor\n",
    "\n",
    "When we sum two tensors, the result of this sum will be a new tensor. Our next step is to find out the shape of this resulting tensor. We do that by, again, comparing the shapes of the original tensors.\n",
    "\n",
    "Let’s see exactly how this is done.\n",
    "\n",
    "Comparing the shape of (1, 3) to (3, 1), we first calculate the max of the last dimension.\n",
    "\n",
    "The max of 3 and 1 is 3. 3 will be the last dimension of the shape of the resulting tensor.\n",
    "\n",
    "Moving on to the next dimension, again, the max of 1 and 3 is 3. So, 3 will be the next dimension of the shape of the resulting tensor.\n",
    "\n",
    "We’ve now stepped through each dimension of the shapes of the original tensors. We can conclude that the resulting tensor will have shape (3, 3).\n",
    "\n",
    "The original tensors of shape (1, 3) and (3, 1) will now be expanded to shape (3, 3) in order to do the element-wise operation.\n",
    "\n",
    "Broadcasting can be thought of as copying the existing values within the original tensor and expanding that tensor with these copies until it reaches the required shape.\n",
    "\n",
    "The values in our (1, 3) tensor will now be broadcast to this (3, 3) tensor.\n",
    "\n",
    "Tensor 1 and Tensor 2 broadcast to shape (3,3):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [1 2 3]\n",
      " [1 2 3]]\n",
      "[[4 4 4]\n",
      " [5 5 5]\n",
      " [6 6 6]]\n"
     ]
    }
   ],
   "source": [
    "print (np.broadcast_to(t1.numpy(), (3,3)))\n",
    "print (np.broadcast_to(t2.numpy(), (3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily take the element-wise sum of these two to get this resulting (3, 3) tensor.\n",
    "\n",
    "```\n",
    "    [[1, 2, 3],\n",
    "     [1, 2, 3],\n",
    "     [1, 2, 3]]\n",
    "+\n",
    "    [[4, 4, 4],\n",
    "     [5, 5, 5],\n",
    "     [6, 6, 6]]\n",
    "-------------------- \n",
    "    [[5, 6, 7],\n",
    "     [6, 7, 8],\n",
    "     [7, 8, 9]] \n",
    "```\n",
    "\n",
    "#### Broadcasting Example 3: Different ranks\n",
    "\n",
    "What if we wanted to multiply this rank-2 tensor of shape (1, 3) with this rank-0 tensor, better known as a scalar?\n",
    "\n",
    "We first compare the last dimensions of the two shapes.\n",
    "\n",
    "When we’re in a situation where the ranks of the two tensors aren’t the same, like what we have here, then we simply substitute a one in for the missing dimensions of the lower-ranked tensor.\n",
    "\n",
    "In our example, we substitute a one for both missing dimensions in the scalar's shape, making it now have shape (1,1)\n",
    "\n",
    "Then, we ask, are the dimensions compatible? And the answer will always be yes in this type of scenario since one of them will always be a one.\n",
    "\n",
    "Alright, all the dimensions are compatible, so what will the resulting tensor look like from multiplying these two together? Again, go ahead and pause here and try yourself before getting the answer.\n",
    "\n",
    "Well, the max of 3 and 1 is 3, and the max of 1 and 1 is 1. So our resulting tensor will be of shape (1, 3).\n",
    "\n",
    "Our first tensor is already this shape, so it gets left alone. Our second tensor is now expanded to this shape by broadcasting it’s value like this.\n",
    "\n",
    "```\n",
    "Before:\n",
    "    5\n",
    "\n",
    "After:\n",
    "    [[5, 5, 5],]\n",
    "    \n",
    "```\n",
    "\n",
    "Now, we can do our element-wise multiplication to get this resulting (1, 3) tensor.\n",
    "\n",
    "```\n",
    "  [[1, 2, 3],]\n",
    "x \n",
    "    5\n",
    "-------------------- \n",
    "    [[5, 10, 15],]\n",
    "    rank: 2\n",
    "    shape: (1,3)\n",
    "```\n",
    "\n",
    "#### Broadcasting Example 4: Different ranks… again\n",
    "\n",
    "```\n",
    "\n",
    "[[[1, 2, 3],\n",
    "  [4, 5, 6]]]\n",
    "\n",
    "rank: 3\n",
    "shape: (1,2,3)\n",
    "\n",
    "[[1, 1, 1],\n",
    " [2, 2, 2],\n",
    " [3, 3, 3]]\n",
    "\n",
    "rank: 2\n",
    "shape: (3,3)\n",
    "\n",
    "```\n",
    "\n",
    "Comparing the second-to-last dimensions of the shapes, they’re not equal to each other, and neither one of them is one, so we stop there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Reduction Ops for Deep Learning\n",
    "\n",
    "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.\n",
    "\n",
    "```\n",
    "> t = torch.tensor([\n",
    "    [0,1,0],\n",
    "    [2,0,2],\n",
    "    [0,3,0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "> t.sum()\n",
    "tensor(8.)\n",
    "\n",
    "```\n",
    "\n",
    "The sum of our tensor's scalar components is calculated using the sum() tensor method. The result of this call is a scalar valued tensor.\n",
    "\n",
    "Checking the number of elements in the original tensor against the result of the sum() call, we can see that, indeed, the tensor returned by the call to sum() contains fewer elements than the original.\n",
    "\n",
    "Since the number of elements have been reduced by the operation, we can conclude that the sum() method is a reduction operation.\n",
    "\n",
    "#### Reducing tensors by axes\n",
    "\n",
    "To reduce a tensor with respect to a specific axis, we use the same methods, and we just pass a value for the dimension parameter. Let’s see this in action.\n",
    "\n",
    "Suppose we have the following tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3 x 4 rank-2 tensor. Having different lengths for the two axes will help us understand these reduce operations.\n",
    "\n",
    "Let’s consider the sum() method again. Only, this time, we will specify a dimension to reduce. We have two axes so we'll do both. Check it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6., 6.])\n",
      "tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "print (t.sum(dim=0))\n",
    "\n",
    "print (t.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go over what happened here.\n",
    "\n",
    "We’ll tackle the first axis first. When take the summation of the first axis, we are summing the elements of the first axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0] + t[1] + t[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sum across the first axis, we are taking the summation of all the elements of the first axis.\n",
    "\n",
    "The second axis in this tensor contains numbers that come in groups of four. Since we have three groups of four numbers, we get three sums.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.) tensor(8.) tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "print (t[0].sum(), t[1].sum(), t[2].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this heavy lifting out of the way. Let’s look now a very common reduction operation used in neural network programming called Argmax.\n",
    "\n",
    "#### Argmax tensor reduction operation\n",
    "\n",
    "Argmax is a mathematical function that tells us which argument, when supplied to a function as input, results in the function’s max output value.\n",
    "\n",
    "When we call the argmax() method on a tensor, the tensor is reduced to a new tensor that contains an index value indicating where the max value is inside the tensor. Let’s see this in code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,0,0,2],\n",
    "    [0,3,3,0],\n",
    "    [4,0,0,5]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tensor, we can see that the max value is the 5 in the last position of the last array.\n",
    "\n",
    "Suppose we are tensor walkers. To arrive at this element, we walk down the first axis until we reach the last array element, and then we walk down to the end of this array passing by the 4, and the two 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(11)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())\n",
    "\n",
    "print (t.argmax())\n",
    "\n",
    "print (t.flatten()[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first piece of code confirms for us that the max is indeed 5, but the call to the argmax() method tells us that the 5 is sitting at index 11. What’s happening here?\n",
    "\n",
    "We’ll have a look at the flattened output for this tensor. If we don’t specific an axis to the argmax() method, it returns the index location of the max value from the flattened tensor, which in this case is indeed 11.\n",
    "\n",
    "Let's see how we can work with specific axes now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2]))\n",
      "tensor([2, 1, 1, 2])\n",
      "(tensor([2., 3., 5.]), tensor([3, 2, 3]))\n",
      "tensor([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print (t.max(dim=0))\n",
    "\n",
    "print (t.argmax(dim=0))\n",
    "\n",
    "print (t.max(dim=1))\n",
    "\n",
    "print (t.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re working with both axes of this tensor in this code. Notice how the call to the max() method returns two tensors. The first tensor contains the max values and the second tensor contains the index locations for the max values. This is what argmax gives us.\n",
    "\n",
    "For the first axis, the max values are, 4, 3, 3, and 5. These values are determined by taking the element-wise maximum across each array running across the first axis.\n",
    "\n",
    "For each of these maximum values, the argmax() method tells us which element along the first axis where the value lives.\n",
    "\n",
    "- The 4 lives at index two of the first axis.\n",
    "- The first 3 lives at index one of the first axis.\n",
    "- The second 3 lives at index one of the first axis.\n",
    "- The 5 lives at index two of the first axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.) tensor(3.) tensor(3.) tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "print(t[2][0],t[1][1], t[1][2], t[2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second axis, the max values are 2, 3, and 5. These values are determined by taking the maximum inside each array of the first axis. We have three groups of four, which gives us 3 maximum values.\n",
    "\n",
    "The argmax values here, tell the index inside each respective array where the max value lives.\n",
    "\n",
    "In practice, we often use the argmax() function on a network’s output prediction tensor, to determine which category has the highest prediction value.\n",
    "\n",
    "#### Accessing elements inside tensors\n",
    "\n",
    "\n",
    "The last type of common operation that we need for tensors is the ability to access data from within the tensor. Let’s look at these for PyTorch.\n",
    "\n",
    "Suppose we have the following tensor:\n",
    "\n",
    "```\n",
    "> t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "> t.mean()\n",
    "tensor(5.)\n",
    "\n",
    "> t.mean().item()\n",
    "5.0\n",
    "```\n",
    "Check out these operations on this one. When we call mean on this 3 x 3 tensor, the reduced output is a scalar valued tensor. If we want to actually get the value as a number, we use the item() tensor method. This works for scalar valued tensors.\n",
    "\n",
    "Have a look at how we do it with multiple values:\n",
    "\n",
    "```\n",
    "> t.mean(dim=0).tolist()\n",
    "[4.0, 5.0, 6.0]\n",
    "\n",
    "> t.mean(dim=0).numpy()\n",
    "array([4., 5., 6.], dtype=float32)\n",
    "```\n",
    "\n",
    "When we compute the mean across the first axis, multiple values are returned, and we can access the numeric values by transforming the output tensor into a Python list or a NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract, Transform, and Load (ETL) with PyTorch\n",
    "\n",
    "There are four general steps that we’ll be following as we move through this project:\n",
    "\n",
    "- Prepare the data\n",
    "- Build the model\n",
    "- Train the model\n",
    "- Analyze the model’s results\n",
    "\n",
    "In this post, we’ll kick things off by preparing the data. To prepare our data, we'll be following what is loosely known as an ETL process.\n",
    "\n",
    "- Extract data from a data source.\n",
    "- Transform data into a desirable format.\n",
    "- Load data into a suitable structure.\n",
    "\n",
    "\n",
    "The ETL process can be thought of as a fractal process because it can be applied on various scales. The process can be applied on a small scale, like a single program, or on a large scale, all the way up to the enterprise level where there are huge systems handling each of the individual parts.\n",
    "\n",
    "We begin by importing all of the necessary PyTorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table describes the of each of these packages:\n",
    "\n",
    "Package |\tDescription\n",
    "--- | ---\n",
    "torch |\tThe top-level PyTorch package and tensor library.\n",
    "torch.nn |\tA subpackage that contains modules and extensible classes for building neural networks.\n",
    "torch.optim |\tA subpackage that contains standard optimization operations like SGD and Adam.\n",
    "torch.nn.functional |\tA functional interface that contains typical operations used for building neural networks like loss functions and convolutions.\n",
    "torchvision |\tA package that provides access to popular datasets, model architectures, and image transformations for computer vision.\n",
    "torchvision.transforms |\tAn interface that contains common transforms for image processing.\n",
    "\n",
    "Other imports\n",
    "The next imports are standard packages used for data science in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from plotcm import plot_confusion_matrix\n",
    "\n",
    "import pdb\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pdb is the Python debugger and the commented import is a local file that we’ll introduce in future posts for plotting the confusion matrix, and the last line sets the print options for PyTorch print statements.\n",
    "\n",
    "We are ready now to prepare our data.\n",
    "\n",
    "#### Preparing our data using PyTorch\n",
    "\n",
    "Our ultimate goal when preparing our data is to do the following (ETL):\n",
    "\n",
    "- Extract – Get the Fashion-MNIST image data from the source.\n",
    "- Transform – Put our data into tensor form.\n",
    "- Load – Put our data into an object to make it easily accessible\n",
    "\n",
    "For these purposes, PyTorch provides us with two classes:\n",
    "\n",
    "- torch.utils.data.Dataset:\tAn abstract class for representing a dataset.\n",
    "- torch.utils.data.DataLoader:\tWraps a dataset and provides access to the underlying data.\n",
    "\n",
    "An abstract class is a Python class that has methods we must implement, so we can create a custom dataset by creating a subclass that extends the functionality of the Dataset class.\n",
    "\n",
    "To create a custom dataset using PyTorch, we extend the Dataset class by creating a subclass that implements these required methods. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object.\n",
    "\n",
    "We will be using the fashion-MNIST dataset that comes built-in with the torchvision package, so we won’t have to do this for our project. Just know that the Fashion-MNIST built-in dataset class is doing this behind the scenes.\n",
    "\n",
    "Specifically, there are two methods that are required to be implemented. The __len__ method which returns the length of the dataset, and the __getitem__ method that gets an element from the dataset at a specific index location within the dataset.\n",
    "\n",
    "\n",
    "\n",
    "#### Challenge in video:\n",
    "\n",
    "Use pytorch torchvision source code to figure out:\n",
    "\n",
    "1. Which class does MNIST extend - torch.utils.data.Dataset\n",
    "2. Where the MNIST class fetches its data from (find domain names) - http://yann.lecun.com/\n",
    "3. Try to see the the significance of the name in the domain name as it pertains to CNNs - http://yann.lecun.com/ : He is the founding father of CNN\n",
    "\n",
    "\n",
    "To get an instance of the FashionMNIST dataset using torchvision, we just create one like so:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True, # extract\n",
    "    transform = transforms.Compose([ # transform\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter|\tDescription\n",
    "--- | ---\n",
    "root|\tThe location on disk where the data is located.\n",
    "train|\tIf the dataset is the training set\n",
    "download|\tIf the data should be downloaded.\n",
    "transform|\tA composition of transformations that should be performed on the dataset elements.\n",
    "\n",
    "Since we want our images to be transformed into tensors, we use the built-in transforms.ToTensor() transformation, and since this dataset is going to be used for training, we’ll name the instance train_set.\n",
    "\n",
    "When we run this code for the first time, the Fashion-MNIST dataset will be downloaded locally. Subsequent calls check for the data before downloading it. Thus, we don't have to worry about double downloads or repeated network calls.\n",
    "\n",
    "To create a DataLoader wrapper for our training set, we do it like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD step\n",
    "# setting up the DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just pass train_set as an argument. Now, we can leverage the loader for tasks that would otherwise be pretty complicated to implement by hand:\n",
    "\n",
    "- batch_size (1000 in our case)\n",
    "- shuffle (True in our case)\n",
    "- num_workers (Default is 0 which means the main process will be used)\n",
    "\n",
    "From an ETL perspective, we have achieved the extract, and the transform using torchvision when we created the dataset:\n",
    "\n",
    "- Extract – The raw data was extracted from the web.\n",
    "- Transform – The raw image data was transformed into a tensor.\n",
    "- Load – The train_set wrapped by (loaded into) the data loader giving us access to the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Datasets and DataLoaders\n",
    "\n",
    "Let’s begin by looking at some operations we can perform to better understand our data.\n",
    "\n",
    "To see how many images are in our training set, we can check the length of the dataset using the Python len() function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "<class 'torchvision.datasets.mnist.FashionMNIST'>\n"
     ]
    }
   ],
   "source": [
    "print (len(train_set))\n",
    "\n",
    "print (type(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 60000 number makes sense based on what we learned in the post on the Fashion-MNIST dataset. Suppose we want to see the labels for each image. This can be done like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "print (train_set.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image is a 9 and the next two are zeros. Remember from posts past, these values encode the actual class name or label. The 9 for example is an ankle boot while the 0 is a t-shirt.\n",
    "\n",
    "If we want to see how many of each label exists in the dataset, we can use the PyTorch bincount() function like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])\n"
     ]
    }
   ],
   "source": [
    "print (train_set.targets.bincount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class imbalance: Balanced and unbalanced datasets\n",
    "\n",
    "This shows us that the Fashion-MNIST dataset is uniform with respect to the number of samples from each class. This means we have 6000 samples for each class. As a result, this dataset is said to be balanced. If the classes had a varying number of samples, we would call the set an unbalanced dataset.\n",
    "\n",
    "Class imbalance is a common problem, but in our case, we have just seen that the Fashion-MNIST dataset is indeed balanced, so we need not worry about that for our project.\n",
    "\n",
    "**Oversampling** ahs proven to be a very good method to tackle this problem of class imbalance. This involves copying the data of the minority class multiple times till the imbalance is negated\n",
    "\n",
    "[more info](https://arxiv.org/abs/1710.05381)\n",
    "\n",
    "To access an individual element from the training set, we first pass the train_set object to Python’s iter() built-in function, which returns an object representing a stream of data.\n",
    "\n",
    "With the stream of data, we can use Python built-in next() function to get the next data element in the stream of data. From this we are expecting to get a single sample, so we’ll name the result accordingly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_set))\n",
    "\n",
    "print (len(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After passing the sample to the len() function, we can see that the sample contains two items, and this is because the dataset contains image-label pairs. Each sample we retrieve from the training set contains the image data as a tensor and the corresponding label as a tensor.\n",
    "\n",
    "Since the sample is a sequence type, we can use sequence unpacking to assigned the image and the label. We will now check the type of the image and the label and see they are both torch.Tensor objects:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print (sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types: <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "shapes: torch.Size([1, 28, 28]) torch.Size([])\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "image, label = sample\n",
    "\n",
    "label = torch.tensor(label)\n",
    "\n",
    "print('types:', type(image), type(label))\n",
    "\n",
    "print('shapes:', image.shape, label.shape)\n",
    "\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also call the squeeze() function on the image to see how we can remove the dimension of size 1. This is review of course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeezed: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "print (\"squeezed:\", image.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, based on our previous discussion of the Fashion-MNIST dataset, we do expect to see the 28 x 28 shape for our image. The reason we see a 1 on the first dimension of the tensor is because the number of channels needs to be represented. Opposed to RGB images that have 3 color channels, grayscale images have a single color channel. This is why we have a 1 x 28 x 28 tensor. We have 1 color channel that has a size of 28 x 28.\n",
    "\n",
    "Let’s plot the image now, and we’ll see why we squeezed the tensor in the first place. We first squeeze the tensor and then pass it to the imshow() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0\t:'T-shirt/top',\n",
    "    1\t:'Trouser',\n",
    "    2\t:'Pullover',\n",
    "    3\t:'Dress',\n",
    "    4\t:'Coat',\n",
    "    5\t:'Sandal',\n",
    "    6\t:'Shirt',\n",
    "    7\t:'Sneaker',\n",
    "    8\t:'Bag',\n",
    "    9\t:'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvxAof4PYwQAhKEd7F8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "\n",
    "print (\"label:\", label_dict[label.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch DataLoader: Working with batches of data\n",
    "\n",
    "We’ll start by creating a new data loader with a smaller batch size of 10 so it’s easy to demonstrate what’s going on:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loader = torch.utils.data.DataLoader(train_set, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a batch from the loader in the same way that we saw with the training set. We use the iter() and next() functions.\n",
    "\n",
    "There is one thing to notice when working with the data loader. If shuffle=True, then the batch will be different each time a call to next occurs. With shuffle=True, the first samples in the training set will be returned on the first call to next. The shuffle functionality is turned off by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(display_loader))\n",
    "print (len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print (batch[0].shape) # (batch size, number of color channels, image height, image width)\n",
    "\n",
    "print (batch[1].shape)\n",
    "\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot a batch of images, we can use the torchvision.utils.make_grid() function to create a grid that can be plotted like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot T-shirt/top T-shirt/top Dress T-shirt/top Pullover Sneaker Pullover Sandal Sandal "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2wAAAB7CAYAAAAIVwPvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "\n",
    "for label in labels:\n",
    "    print (label_dict[label.item()], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
