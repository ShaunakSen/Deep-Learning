{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Programming - Deep Learning \n",
    "\n",
    "[Playlist link](https://www.youtube.com/watch?v=iTKbyFh-7GM&list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG&index=2)\n",
    "\n",
    "### PyTorch - Python deep learning neural network API\n",
    "\n",
    "\n",
    "**A tensor is an n-dimensional array.**\n",
    "\n",
    "With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system.\n",
    "\n",
    "![](./img/diag1.png)\n",
    "\n",
    "Let’s talk about the prospects for learning PyTorch. For beginners to deep learning and neural networks, the top reason for learning PyTorch is that it is a thin framework that stays out of the way.\n",
    "\n",
    "**PyTorch is thin and stays out of the way!**\n",
    "\n",
    "When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. The experience of programming in PyTorch is as close as it gets to the real thing.\n",
    "\n",
    "A common PyTorch characteristic that often pops up is that it’s great for research. The reason for this research suitability has do do with a technical design consideration. To optimize neural networks, we need to calculate derivatives, and to do this computationally, deep learning frameworks use what are called [computational graphs](http://colah.github.io/posts/2015-08-Backprop/).\n",
    "\n",
    "Computational graphs are used to graph the function operations that occur on tensors inside neural networks.\n",
    "\n",
    "\n",
    "These graphs are then used to compute the derivatives needed to optimize the neural network. PyTorch uses a computational graph that is called a dynamic computational graph. This means that the graph is generated on the fly as the operations are created.\n",
    "\n",
    "This is in contrast to static graphs that are fully determined before the actual operations occur.\n",
    "\n",
    "It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1,2,3]) # created on CPU by default\n",
    "\n",
    "# so any operation we do on this tensor will be carried out in the CPU\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move tensor t onto GPU: Returns a copy of this object in CUDA memory.\n",
    "\n",
    "# t = t.cuda()\n",
    "\n",
    "# t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing tensors for deep learning\n",
    "\n",
    "\n",
    "**A tensor is the primary data structure used by neural networks.**\n",
    "\n",
    "The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure.\n",
    "\n",
    "\n",
    "| Indexes reqd | Computer science | Mathematics\n",
    "--- | --- | ---\n",
    "|0 |\tnumber |\tscalar |\n",
    "|1 |\tarray |\tvector |\n",
    "|2 |\t2d-array |\tmatrix |\n",
    "\n",
    "For example, suppose we have this array:\n",
    "Now, suppose we want to access (refer to) the number 3 in this data structure. We can do it using a single index like so:\n",
    "\n",
    "\n",
    "```\n",
    "> a = [1,2,3,4]\n",
    "\n",
    "> a[2]\n",
    "3\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This logic works the same for a vector.\n",
    "\n",
    "As another example, suppose we have this 2d-array:\n",
    "Now, suppose we want to access (refer to) the number 3 in this data structure. In this case, we need two indexes to locate the specific element.\n",
    "\n",
    "```\n",
    "> dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]\n",
    "\n",
    "> dd[0][2]\n",
    "3 \n",
    "```\n",
    "\n",
    "#### Tensors are generalizations\n",
    "\n",
    "\n",
    "When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.\n",
    "\n",
    "Indexes required |\tComputer science |\tMathematics\n",
    "--- | --- | ---\n",
    "n |\tnd-array |\tnd-tensor\n",
    "\n",
    "**Tensors and nd-arrays are the same thing!**\n",
    "\n",
    "So tensors are multidimensional arrays or nd-arrays for short. The reason we say a tensor is a generalization is because we use the word tensor for all values of n like so:\n",
    "\n",
    "- A scalar is a 0 dimensional tensor\n",
    "- A vector is a 1 dimensional tensor\n",
    "- A matrix is a 2 dimensional tensor\n",
    "- A nd-array is an n dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank, Axes and Shape\n",
    "\n",
    "The rank, axes, and shape are three tensor attributes that will concern us most when starting out with tensors in deep learning. These concepts build on one another starting with rank, then axes, and building up to shape, so keep any eye out for this relationship between these three.\n",
    "\n",
    "#### Rank\n",
    "\n",
    "The rank of a tensor refers to the number of dimensions present within the tensor. Suppose we are told that we have a rank-2 tensor. This means all of the following:\n",
    "\n",
    "- We have a matrix\n",
    "- We have a 2d-array\n",
    "- We have a 2d-tensor\n",
    "\n",
    "\n",
    "**A tensor's rank tells us how many indexes are needed to refer to a specific element within the tensor.**\n",
    "\n",
    "#### Axes\n",
    "\n",
    "An axis of a tensor is a specific dimension of a tensor.\n",
    "\n",
    "\n",
    "If we say that a tensor is a rank 2 tensor, we mean that the tensor has 2 dimensions, or equivalently, the tensor has two axes.\n",
    "\n",
    "Elements are said to exist or run along an axis. This running is constrained by the length of each axis. Let's look at the length of an axis now.\n",
    "\n",
    "The length of each axis tells us how many indexes are available along each axis.\n",
    "\n",
    "Suppose we have a tensor called t, and we know that the first axis has a length of three while the second axis has a length of four.\n",
    "\n",
    "Since the first axis has a length of three, this means that we can index three positions along the first axis like so:\n",
    "\n",
    "```\n",
    "t[0]\n",
    "t[1]\n",
    "t[2]\n",
    "```\n",
    "\n",
    "All of these indexes are valid, but we can't move passed index 2.\n",
    "\n",
    "Since the second axis has a length of four, we can index four positions along the second axis. This is possible for each index of the first axis, so we have\n",
    "\n",
    "```\n",
    "t[0][0]\n",
    "t[1][0]\n",
    "t[2][0]\n",
    "\n",
    "t[0][1]\n",
    "t[1][1]\n",
    "t[2][1]\n",
    "\n",
    "t[0][2]\n",
    "t[1][2]\n",
    "t[2][2]\n",
    "\n",
    "t[0][3]\n",
    "t[1][3]\n",
    "t[2][3]\n",
    "```\n",
    "\n",
    "Let's look at some examples to make this solid. We'll consider the same tensor dd as before:\n",
    "\n",
    "```\n",
    "\n",
    "> dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]\n",
    "\n",
    "# Each element along the first axis, is an array:\n",
    "\n",
    "> dd[0]\n",
    "[1, 2, 3]\n",
    "\n",
    "> dd[1]\n",
    "[4, 5, 6]\n",
    "\n",
    "> dd[2]\n",
    "[7, 8, 9]\n",
    "\n",
    "# Each element along the second axis, is a number:\n",
    "\n",
    "> dd[0][0]\n",
    "1\n",
    "\n",
    "> dd[1][0]\n",
    "4\n",
    "\n",
    "> dd[2][0]\n",
    "7\n",
    "\n",
    "```\n",
    "\n",
    "Note that, with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays. This is what we see in this example, but this idea generalizes.\n",
    "\n",
    "The rank of a tensor tells us how many axes a tensor has, and the length of these axes leads us to the very important concept known as the shape of a tensor.\n",
    "\n",
    "#### Shape of a tensor\n",
    "\n",
    "\n",
    "The shape of a tensor gives us the length of each axis of the tensor.\n",
    "\n",
    "\n",
    "To work with this tensor's shape, we’ll create a torch.Tensor object like so:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]\n",
    "\n",
    "t = torch.Tensor(dd)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to see the tensor's shape is 3 x 3. Note that, in PyTorch, size and shape of a tensor are the same thing.\n",
    "\n",
    "The shape of 3 x 3 tells us that each axis of this rank two tensor has a length of 3 which means that we have three indexes available along each axis. Let's look now at why the shape of a tensor is so important.\n",
    "\n",
    "The shape of a tensor is important for a few reasons. The first reason is because the shape allows us to conceptually think about, or even visualize, a tensor. Higher rank tensors become more abstract, and the shape gives us something concrete to think about.\n",
    "\n",
    "The shape also encodes all of the relevant information about axes, rank, and therefore indexes.\n",
    "\n",
    "Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping.\n",
    "\n",
    "As our tensors flow through our networks, certain shapes are expected at different points inside the network, and as neural network programmers, it is our job to understand the incoming shape and have the ability to reshape as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
